// Based on a script by Kathie Decora : katydecorah.com/code/lunr-and-jekyll/

// Create the lunr index for the search
var index = elasticlunr(function () {
    this.addField('title')
    this.addField('author')
    this.addField('layout')
    this.addField('content')
    this.setRef('id')
});

// Add to this index the proper metadata from the Jekyll content
index.addDoc({
    id:      0,
    title:   "Chapter 9 - Generalization bounds via stability",
    author:  null,
    layout:  "page",
    content: "In the last two lectures, we have investigated generalization through the lens of\n\n\n  margin-based regularization\n  model parsimony\n\n\nEach had their shortfalls, especially in the context of deep learning:\n\n\n  Regularization very much depends on model architecture and/or training procedure. Implicit bias of GD shows up mysteriously in several places.\n  Model parsimony is hard to measure; interacts with the data distribution in hard-to-quantify ways; and anyway in the last chapter we saw that uniform convergence seems to be an insufficient tool for getting non-trivial bounds, especially in the over-parameterized setting.\n\n\nLet us now briefly a third lens to understand generalization:\n\n\n  Algorithmic stability.\n\n\nOur central hypothesis in this chapter is:\n\nStable learning implies generalization.\n\n\nThe high level idea is that if the model produced by a training algorithm is not sensitive to any single training data point, then the model generalizes. This idea has been around for some time (and in fact, methods such as bagging1 explicitly were developed in order to make classifiers stable to individual points.)\n\n(Notice that the definition of stability involves specifying the training procedure; once again, the role of the training algorithm is more clearly illuminated here.)\n\nThe plus side is that we won’t need to appeal to uniform convergence, so over-parameterization is not a concern. The minus side is that convexity of the loss seems to be the key tool here, which makes application to deep learning difficult.\n\nOn the other hand, our tour of optimization (Chapter 4) has given us tools to deal with (some) non-convex losses. All these will be prove to be fruitful.\n\nSetup\n\nStability as a tool for generalization dates back to an influential paper by Bousquet and Elisseef2, who introduced the notion of uniform stability.\n\nFurther refined in the context of empirical risk minimization by Shalev-Schwartz et al.3.\n\nAlgorithmic stability of (S)GD for empirical risk minimization\n\nBeyond convexity\n\nStability under smoothness + PL-condition. Key paper: here4.\n\nConnections to differential privacy\n\nFascinating parallels between\n\n\n  generalization via uniform stability\n  differential privacy (DP)\n\n\nwhich has a different origin story, dating back to several papers by Dwork and co-authors 5 6 7.\n\nIn DP the goal is to protect “data leakage” – nothing about the user’s identity (as far as possible) should be revealed from the model parameters. But the method to achieve is the same as the ones we discussed above: make sure the model does not depend on any one data point too much.\n\nUpshot: DP implies uniform stability (and DP-style algorithm design gives a way to control generalization.)\n\nComplete.\n\n\n\n\n  \n    \n\n      L. Breiman, Bagging Predictors, 1994. &#x21a9;&#xfe0e;\n    \n    \n\n      O. Bousquet and A. Elisseef, Stability and Generalization, 2002. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Shalev-Schwartz, O. Shamir, K. Sridharan, N. Srebro, Learnability, Stability and Uniform Convergence, 2010. &#x21a9;&#xfe0e;\n    \n    \n\n      Z. Charles and D. Papailiopoulos, Stability and Generalization of Learning Algorithms that Converge to Global Optima, 2018. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Dwork, Differential Privacy, 2006. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Dwork, F. McSherry, K. Nissim, A. Smith, Calibrating Noise to Sensitivity in Private Data Analysis, 2006. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Dwork and A. Roth, The Algorithmic Foundations of Differential Privacy, 2014. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      1,
    title:   "Chapter 8 - PAC learning primer and error bounds",
    author:  null,
    layout:  "page",
    content: "OK, so implicit regularization (via the choice of training algorithm) may be part of the reason for generalization, but almost surely not the entire picture.\n\nIt may be useful to revisit classical ML here. Very broadly, the conventional thinking has been:\n\nParsimony enables generalization.\n\n\nThe challenge is to precisely define what “parsimony” means here, since there are a myriad different ways of doing this. The bulk of work in generalization theory explores more and more refined complexity measures of ML models, but as we will see below, most existing classical approaches lead to vacuous bounds for deep networks. Getting non-vacuous generalization guarantees is a major challenge.\n\nWarmup: Finite hypothesis classes\n\nBelow, $R(f)$ is the population risk, $\\hat{R}(f)$ is the empirical risk, and we would like $R(f) - \\hat{R}(f)$ to be (a) small, and (b) decreasing as a function of sample size.\nComplete.\n\n\n\nTheorem\n  Consider the setting of binary classification, and let $\\f$ be a hypothesis class with finitely many elements. If $n \\geq O\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{|\\f|}{\\delta}\\right)$ then with probability at least $1-\\delta$, we have that for every $f \\in \\f$, $|R(f) - \\hat{R}(f)| \\leq \\varepsilon$.\nProof\n  Hoeffding + union bound.\n\nRemark\n  Observe that this holds for all $f \\in \\f$, not just the optimal predictor. Such a bound is called a uniform convergence result.\n\nRemark\n  This works irrespective of the choice of $\\f$ or the data distribution, which could be a weakness of the technique. Even in incompatible cases (for example, the data is highly highly nonlinear, but we only use linear classifiers), one can use the above result that the generalization error is small. The lesson is that we need to control both train and generalization error.\n\nRemark\n  Rearranging terms, we get that the generalization error $\\lesssim \\frac{1}{\\sqrt{n}}$, which is very typical. Classical theory tells us that this is a natural stopping point for optimization – this is the “noise floor” so we don’t have to optimize the train error below this level. (Note: unfortunately, deep learning doesn’t obey this, and optimizing to zero is indeed beneficial; this is the “double descent” phenomenon.)\n\nRemark\n  The scaling of the sample complexity looks like $n \\approx \\log |\\f|$, which basically is the number of “bits” needed to encode any particular hypothesis.\n\nComplexity measures\n\nLong list of ways to extend the above reasoning. The major drawback is that the above bound is meaningful only for hypothesis classes $\\f$ with finite cardinality.  Alternatives:\n\n\n  Covering number\n  VC-dimension\n  Pseudo-dimension\n\n\nAgnostic (PAC) learning\n\nData-dependent bounds\n\nDefinition of Rademacher complexity, upper bounds for NN.\n\nPAC-Bayes bounds\n\nPossibly first approach to produce “non-vacuous” bounds, at least for small networks. Key results: basic approach1, application to DL2.\n\nError bounds for (deep) neural networks\n\nKey results: here3, here4, here5.\n\nPossible roadblocks?\n\nAll of the above bounds lead to generalization bounds of the form:\n\n\\[\\text{Test error} \\leq \\text{train error}~+~O \\left(\\frac{\\text{complexity measure}}{\\sqrt{n}}\\right),\\]\n\nand progress has been focused on defining better and better complexity measures. However, two issues with this:\n\n\n  \n    Usually, the complexity measure in the numerator is far too large anyway, leading to “vacuous” bounds. For example, in 5 it reduces to $\\text{depth} \\times \\text{width}$, which is too large in the overparameterized setting.\n  \n  \n    This also (seemingly) means that error bounds should decrease with dataset size, for a fixed model class. Not the case :( .\n  \n\n\nA recent result provides evidence6 to show that any result that uses uniform convergence may suffer from this kind of looseness. We likely need alternate techniques, which we will do next.\n\n\n\n\n  \n    \n\n      D. Mcallester, Some PAC-Bayesian Theorems, 1999. &#x21a9;&#xfe0e;\n    \n    \n\n      G. K. Dziugaite, D. Roy, Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      P. Bartlett, For Valid Generalization the Size of the Weights is More Important than the Size of the Network, 1997. &#x21a9;&#xfe0e;\n    \n    \n\n      P. Bartlett, V. Maiorov, R. Meir, Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks, 1998. &#x21a9;&#xfe0e;\n    \n    \n\n      P. Bartlett, N. Harvey, C. Liaw, A. Mehrabian,  Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks, 2019. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      V. Nagarajan, Z. Kolter, Uniform convergence may be unable to explain generalization in deep learning, 2019. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      2,
    title:   "Chapter 7 - Implicit regularization",
    author:  null,
    layout:  "page",
    content: "We spent the last few chapters understanding the optimization aspect of deep learning. Namely, given a dataset and a family of network architectures, how well do standard training procedures (such as gradient descent or SGD) work in terms of fitting the data? As we witnessed, an answer to this question is far from complete, but a picture (involving tools from optimization theory and kernel-based learning) is beginning to emerge.\n\nHowever, training is not the end goal. Recall in the Introduction that we want to discover prediction functions that perform “well” on “most” possible data points. In other words, we not only want our model to fit a training dataset well, but also would like our model to exhibit low generalization error when evaluated on hitherto unseen inputs.\n\nOver the next few lectures, we address the central question:\n\nWhen (and why) do networks trained using (S)GD generalize?  \n\n\nAs we work through different aspects of this question, we will find that\n\n\n  classical statistical learning theory fails to give useful (non-vacuous) generalization error bounds.\n  in fact, lots of other weirdness happens.\n  architecture, optimization, and generalization in deep learning interact in a non-trivial manner, and should be studied simultaneously.\n\n\nLet us work backwards. In this chapter, we will see how the choice of training algorithm (which, in the case of deep learning, is almost always GD or SGD or some variant thereof) affects generalization.\n\nClassical models, margins, and regularization\n\nStatistical learning theory offers many, many lenses through which we can study generalization. One approach (classical, and fairly intuitive) is to think in terms of margins.\n\nLet us be concrete. Say that we are doing binary classification using linear models.  We have a dataset $X = \\lbrace (x_i,y_i)_{i=1}^n \\subset \\R^d \\times \\pm 1 \\rbrace$. For simplicity let us ignore the bias and assume that everything is centered around the origin. The problem is to find a linear classifier that generalizes well. The following picture (in $d=2$) comes to mind.\n\n\n\nAssume that the data is linearly separable. Then, we can write down the desired classifer as the solution to the following optimization problem (and in classical ML, we call such a model a perceptron.)\n\n\\[\\text{Find}~w,~\\text{such that}~\\text{sign}\\lang w,x_i \\rang = y_i~~\\forall~i \\in [n].\\]\n\nor, equivalently,\n\n\\[\\text{Find}~w,~\\text{such that}~ y_i \\lang w,x_i \\rang \\geq 0~~\\forall~i \\in [n].\\]\n\nHowever, as is the case in the picture above, there could be several linear models that exactly interpolate the labels, which means that the above optimization problem may not have unique solutions. Which solution, then, should we pick?\n\nOne way to resolve this issue is by the Large Margin Hypothesis, which asserts that:\n\nLarger margins --&gt; better generalization.\n\n\nSuppose we associate each linear classifier with the corresponding separating hyperplane (marked in red above). Since we want “most” points in the data space to be correctly classified, a reasonable choice is to find the hyperplane that is furthest away from either class. An easy calculation (shown, for example, here) shows that for any hyperplane parameterized by $w$, the margin equals $\\frac{2}{\\lVert w \\rVert}$. Therefore, maximizing the margin is the same as solving the the optimization problem:\n\n\\[\\begin{aligned}\n&amp;\\min_w \\frac{1}{2} \\lVert w \\rVert^2  \\\\\n\\text{s.t.~}&amp; y_i \\lang w, x_i \\rang \\geq 1,~\\forall i \\in [n] .\n\\end{aligned}\\]\n\nThe solution to this problem is called the support vector machine (SVM). This is fine, as long as the data is linearly separable. If not, the constraint set is empty and the optimization problem is ill-posed. A remedy is to relax the (hard) constraints using Lagrange. We get the soft-margin SVM as a solution to the (relaxed) problem:\n\n\\[\\min_w \\frac{1}{2} \\lVert w \\rVert^2 + C \\sum_{i=1}^n l(y_i \\lang w, x_i \\rang)\\]\n\nwhere $C &gt; 0$ is some weighting parameter, and $l : \\R \\rightarrow \\R$ is some (scalar) loss function that penalizes large negative inputs and leaves inputs bigger than 1 untouched. The $\\exp$ loss ($l(z) = e^{-z}$), or the Hinge loss ($l(z) = \\max(0,1-z)$) are examples.\n\nNotice that the relaxed problem resembles the standard setup in statistical inference. There is a training loss function $L(w) = \\sum_{i=1}^n l(y_i \\lang w, x_i \\rang)$ and a regularizer $R(w) = \\lVert w \\rVert^2$, and we seek a model that tries to minimize both. The $\\ell_2$-regularizer used above (in deep learning jargon) is sometimes called “weight decay” but others (such as the $\\ell_1$-regularizer) are also popular.\n\nAn alternative way to arrive at this standard setup is by invoking Occam’s Razor: among the many explanations to the data, we want the one that is of “minimum complexity”. This also motivates a formulation for linear regression which is the same except we use the least-squares loss. Setting $X \\in \\R^{n \\times d}$ as the data matrix, we get:\n\n\\[\\min_w \\frac{1}{2} \\lVert w \\rVert^2 + \\mu \\lVert y - Xw \\rVert^2\\]\n\nwhich is sometimes called Tikhonov regularization. Different choices of $\\mu$ lead to different solutions: a large $\\mu$ reduces the data fit error, while a small $\\mu$ encourages lower-norm solutions.\n\n[There is a long history here which we won’t recount. See here1 for a nice introduction to the area, and here2 which establishes formal generalization bounds in terms of the margin.\n\nHowever.\n\nIn deep learning, anecdotally we seem to be doing just fine even without any regularizers. While regularization may help in some cases, several careful experiments3 show that regularization is neither necessary nor sufficient for good generalization. Why is that the case?\n\nTo resolve this, a large body of recent work (which we introduce below) studies what I will call the Algorithmic Bias Hypothesis. This hypothesis asserts that the dynamics of the training procedure (specifically, gradient descent) implicitly introduces a regularization effect. That is, the algorithm itself is biased towards the choice of solution towards minimum complexity solutions, and therefore the explicit imposition of a regularizer is unnecessary.\n\nRemark\n  The results we will prove below hold for both GD as well as SGD, so there is nothing inherently special about the stochasticity of the training procedure. The literature is split on this issue, and I am not aware of compelling theoretical benefits of SGD beyond a per-iteration computational speedup. Indeed, even empirical evidence seems to suggest that stochastic training does not provide any particular benefits from the generalization perspective4.\n\nImplicit bias of GD/SGD for linear models\n\nLet’s stick with linear models for a bit longer, since the math will be instructive. We start with the setting of linear regression. We are given a dataset $X = \\lbrace (x_i,y_i)_{i=1}^n \\subset \\R^d \\times \\R \\rbrace$. We assume that the model is over-parameterized, so the number of parameters is greater than the number of examples $(n &lt; d)$. We would like to fit a linear model $u = \\langle w, x \\rangle$ that exactly interpolates the training data. If we focus on the squared error as the loss function of choice, we seek a (global) minimizer of the loss function:\n\n\\[L(w) = \\frac{1}{2}\\lVert y - Xw \\rVert^2 .\\]\n\nBut note that due to over-parameterization, the data matrix is short and wide, and there are many, many candidate models that achieve zero-loss (indeed, an infinity of them, assuming that the data is in general position.) This is where classical statistics would invoke Occam’s Razor, or margin theory, or such and start introducing regularizers.\n\nLet us blissfully ignore such suggestions, and instead, starting from $w_0 = 0$, run gradient descent on the un-regularized loss $L(w)$. (To be precise, we will run gradient flow, i.e., with infinitesimally small step size – but similar conclusions can be derived for finite but small step sizes, or for stochastic minibatches, with extra algebra.) Therefore, at any time $t$, we have the gradient flow (GF) ordinary differential equation:\n\n\\[\\frac{dw(t)}{dt} = - \\nabla L(w(t)) .\\]\n\nWe have the following:\n\nLemma\n  Gradient flow over the squared error loss converges to the minimum $\\ell_2$-norm interpolator:\n  \\(\\begin{aligned}\n  w^* = \\arg \\min_w \\lVert w \\rVert^2,~\\text{s.t.}~y = Xw.\n  \\end{aligned}\\)\n  In other words, GF provides an implicit bias towards an $\\ell_2$-regularized solution.\n\nProof\n  This fact is probably folklore, but a proof can be found in Zhang et al.3. We only need one key fact: because we are using the squared error loss, at any time $t$ the gradient looks like:\n\n\\[-\\nabla L(w(t)) = X^T (y - Xw(t)) = \\sum_{i=1}^n x_i e_i(t)\\]\n\nwhich is always a vector in the span of the data points. This means that if $w(0) = 0$, then the weights at any time $t$ continues to remain in this span. (Geometrically, the span of the data forms an $n$-dimensional subspace in the $d$-dimensional weight space, so the invariance being preserved here is that the path of gradient descent always lies within this subspace.)\n\nNow, assume that GF converges to a global minimizer, $\\bar{w}$. We need to be a bit careful and actually prove that global convergence happens. But we already know that GF/GD minimizes convex loss functions, so let us not repeat the calculations from Chapter 4. Therefore, $\\bar{w} = X^T \\alpha$ for some set of coefficients $\\alpha$. But we also know that since $\\bar{w}$ is a global minimizer, $X\\bar{w} = y$, which means that\n\n\\[XX^T \\alpha = y, \\quad \\text{or} \\quad \\alpha = (XX^T)^{-1} y.\\]\n\nwhere the inverse exists since the data is in general position. Therefore,\n\n\\(\\bar{w} =  X^T (XX^T)^{-1} y\\)\n  which is the Moore-Penrose inverse, or pseudo-inverse, of $X$ applied to $y$. From basic linear algebra we know that the pseudo-inverse gives the minimum-norm solution to the linear system $y = Xw$.\n\nRemark\n  The above proof is algebraic, but there is a simple geometric intuition. Gradient flowlines are orthogonal to the subspace of all possible solutions. Therefore, if we start gradient flow from the origin, then the shortest path to the set of feasible solutions is the straight line segment orthogonal to this subspace, which means that the final answer is the interpolator with minimum $L_2$ norm.\n\nGreat! For the squared error loss, the path of gradient descent always leads to the lowest-norm solution, regardless of the training dataset.\n\nDoes this intuition carry over to more complex settings? The answer is yes. Let us now describe a similar result (proved here5 and here6) for the setting of linear classification. Given a linearly separable training dataset, we discover an interpolating classifier using gradient descent over the $\\exp$ loss. (This result also holds for the logistic loss.) Somewhat surprisingly, we show that the limit of gradient descent (approximately) produces the min-norm interpolator, which is nothing but the (hard margin) SVM!\n\nThe exact theorem statement needs some motivation. For that, let’s first examine the $\\exp$ loss, which is somewhat of a strange beast. As defined above, this looks like:\n\n\\[L(w) = \\sum_{i=1}^n \\exp(- y_i \\lang w, x_i \\rang) .\\]\n\nA bit of reflection will suggest that the infimum of $L$ is indeed 0, but ‘zero-loss’ is attained only when the norm of $w$ diverges to infinity. (Exercise: Prove this.) Therefore, we will need to define ‘convergence’ a bit more carefully. The right way to think about it is to look at convergence in direction; for linear classification, the norm of $w$ does not really matter — so we can examine the properties of $\\lim w(t)/\\lVert w(t) \\rVert$ as $t \\rightarrow \\infty$. We obtain the following:\n\nTheorem\n  Let $w^*$ be the hard-margin SVM solution, or the minimum $\\ell_2$-norm interpolator:\n\n\\[\\begin{aligned}\n  w^* = \\arg \\min_w \\lVert w \\rVert^2,~\\text{s.t.}~y_i \\lang w, x_i \\rang \\geq 1~\\forall i \\in [n].\n  \\end{aligned}\\]\n\nThen, gradient flow over the $\\exp$ loss converges to:\n\n\\[w(t) \\rightarrow w^* \\log t + O(\\log \\log t).\\]\n\nMoreover,\n\n\\[\\frac{w(t)}{\\lVert w(t) \\rVert} \\rightarrow  \\frac{w^*}{\\lVert w^* \\rVert} .\\]\n\nIn other words, GF directionally converges towards the $\\ell_2$-max margin solution.\n\nProof sketch\n  (Complete.)\n\nRemark\n  Convergence happens, but at rate $\\frac{1}{\\log (t)}$, which is way slower than all the rates we showed in Chapter 4. (Prove this?)\n\nTherefore, we have seen that in linear models, gradient flow induces implicit $\\ell_2$-bias for:\n\n\n  the squared error loss, and\n  the $\\exp$ loss (as well as the logistic loss5.)\n\n\nWhat about other choices of loss? Ji et al.7 showed a similar directional convergence result for general exponentially tailed loss functions, while clarifying that the limit direction may differ across different losses.\n\nMore pertinently, what about other choices of architecture (beyond linear models)? We examine this next.\n\nImplicit bias in multilinear models\n\nThe picture becomes much more murky (and, frankly, really fascinating) when we move beyond linear models. As we will show below the architecture plays a fundamental role in the gradient dynamics, which further goes to show that both representation and optimization method play a crucial role in inducing algorithmic bias.\n\nA linear model of the form $u = \\lang w, x \\rang$ can be viewed as a single neuron with linear activation. Let us persist with linear activations for some more time, but graduate to multiple layers of neurons. One such model is called a two-layer diagonal linear network model:\n\n\\[u = \\sum_{j=1}^d v_j u_j x_j ,\\]\n\nwhere the weights $(u_j, v_j)$ are trainable. This model, of course, is merely a re-parameterization of a single neuron and can only express linear functions. However, the weights interact multiplicatively, and therefore the output is a nonlinear function of the weights. This re-parameterization makes a world of difference in the context of gradient dynamics, and leads to a very different form of algorithmic bias, as we will show below.\n\nDiagonal linear models\n\nFor simplicity, let’s just assume that the weights are tied across layers, i.e., $u_i = v_i$ for $i \\in [d]$. I don’t believe that this assumption is important; similar conclusions should hold even if no weight tying occurs.\n\nThen, the prediction for any data point $x \\in \\R^d$ becomes\n\n\\[y = \\sum_{j=1}^d x_j u_j^2\\]\n\nor if we stack up the training data then\n\n\\[y = X (u \\circ u)\\]\n\nwhere $\\circ$ denotes the element-wise (Hadamard) product. This is called a “two-layer diagonal linear network”, and the extension to $L$-layers is analogous. Notice that by using this re-parameterization, we have not done very much in terms of expressiveness.\n\n(Actually, the last statement is not very precise; square reparameterization only expresses linear models $y = Xw$, but with the added restriction that $w \\geq 0$. But this can be fixed easily as follows: introduce 2 sets of variables $u$ and $v$, and write $y = X(u\\circ u - v\\circ v)$.)\n\nLet’s stick with the squared error loss:\n\n\\[L(u) = \\frac{1}{2} \\lVert y - X(u \\circ u) \\rVert^2\\]\n\nand suppose we perform gradient flow over this loss. Since $L$ is a fourth-order polynomial (quartic) function of $u$, the ODE governing the gradient at any time instant is a bit more complicated:\n\n\\[\\frac{du}{dt} = - \\nabla_u L(u) = 2 u \\circ X^T (y - X (u \\circ u)) .\\]\n\nNotice now that the gradient in fact is now a cubic function of $u$. Suppose that we initialize $u = \\alpha \\mathbf{1}$, where $\\mathbf{1}$ is the all-ones vector and $\\alpha &gt; 0$ is a scalar. (This is fine since in the end we will only be interested in models with positive coefficients.)\n\nSomewhat curiously, we can prove that if $\\alpha$ is small enough then the algorithmic bias corresponding to gradient flow corresponds to $\\ell_1$-regularization (i.e., we recover the familiar basis pursuit or Lasso formulation. Formally, we get the following:\n\nTheorem\n  Let $w^*$ be the (non-negative) interpolator with minimum $\\ell_1$-norm:\n\n\\[\\begin{aligned}\n  w^* = \\arg \\min_w \\lVert w \\rVert_1,~\\text{s.t.}~y = Xw,~w \\geq 0.\n  \\end{aligned}\\]\n\nSuppose that gradient flow for diagonal linear networks converges to an estimate $\\bar{u}$. Then $\\bar{w} = \\bar{u} \\circ \\bar{u}$ converges to $w^*$ as $\\alpha \\rightarrow 0$.\n\nProof sketch\n  This proof is by Woodworth et al.8 Let us give high level intuition first. It will be helpful to understand two types of dynamics: one in the original (canonical) linear parameterization in terms of $w$, and the other in the square re-parameterization (in terms of $u$). By definition, we have $w_j = u_j^2$, and therefore\n\n\\[\\frac{dw_j}{dt} = 2u_j \\frac{du_j}{dt} .\\]\n\nBy plugging in the definition of gradient flow, we get:\n\n\\[\\begin{aligned}\n  \\frac{dw}{dt} &amp;= - 2 u(t) \\circ \\nabla_u L(u(t)) \\\\\n  &amp; =2 u(t) \\circ 2 u(t) \\circ X^T(y - X(u(t) \\circ u(t))) \\\\\n  &amp; = 4 w(t) \\circ X^T(y - Xw(t)).\n  \\end{aligned}\\]\n\nThis resembles gradient flow over the standard $\\ell_2$-loss, except that each coordinate is multiplied with $w_i(t)$. Intuitively, this induces a “rich-get-richer” phenomenon: a bigger coefficient in $w(t)$ is assigned a larger gradient update (relative to the other coefficients) and therefore “moves” towards its final destination faster.\n\nGissin et al.9 call this incremental learning: large coefficients are learned (approximately) in decreasing order of their magnitude. (See their paper/website for some terrific visualizations of the dynamics.)\n\n\n\nTo rigorously argue that the bias indeed is the $\\ell_1$-regularizer, let $e(t) = y - X w(t)$ denote the prediction error at any time instant. Then, as shown above, we have the ODE:\n\n\\[\\frac{du}{dt} = 2 u(t) \\circ X^T e(t) .\\]\n\nSolving this equation coordinate-wise, we get the closed form solution:\n\n\\[u(t) = u(0) \\circ \\exp\\left( 2 X^T \\int_0^t e(s) ds \\right) .\\]\n\nPlugging in the initialization $u(0) = \\alpha \\mathbf{1}$ and $w(t) = u(t) \\circ u(t)$, we get:\n\n\\[w(t) = \\alpha^2 \\mathbf{1} \\circ \\exp\\left( 4 X^T \\int_0^t e(s) ds \\right).\\]\n\nTherefore, assuming the dynamics converges (technically we need to prove this, but let’s sweep that under the rug for now), we get that the limit of gradient flow gives:\n\n\\[\\bar{w} = f(X^T \\lambda)\\]\n\nwhere $f$ is a coordinate-wise function such that $f(z):= \\alpha^2 \\exp(z)$ and $\\lambda := 4 \\int_0^\\infty e(s) ds$. Since $\\alpha &gt; 0$, $f$ is bijective and therefore\n\n\\[X^T \\lambda = f^{-1}(\\bar{w}) = \\log \\frac{\\bar{w}}{\\alpha^2} .\\]\n\nNow, assume that the algorithmic bias of GF can be expressed by some unique regularizer $Q(w)$. (Again, we are making a major assumption here that the bias is expressible in the form of a well-defined function of $w$, but let’s do more rug+sweeping here.) We will show that $Q(w)$ approximately equals the $\\ell_1$-norm of $w$. We get:\n\n\\[\\bar{w} = \\arg \\min_w Q(w), \\quad y = Xw.\\]\n\nKKT optimality tells us that the optimum ($\\bar{w}$) should satisfy:\n\n\\[y = X\\bar{w}, \\quad \\nabla_w Q(\\bar{w}) = X^T \\lambda.\\]\n\nfor some vector $\\lambda$. Therefore, we can reverse-engineer $Q$ by solving the differential equation:\n\n\\[\\nabla_w Q(w) = \\frac{1}{\\alpha^2} \\log w .\\]\n\nThis (again) has the closed form solution:\n\n\\[\\begin{aligned}\n  Q(w) &amp;= \\sum_{i=1}^d \\int_{0}^{w_i} \\log \\frac{w_i}{\\alpha^2} dw_i \\\\\n  &amp;= \\sum_{i=1}^d w_i \\log \\frac{w_i}{\\alpha^2} - w_i \\\\\n  &amp;= \\sum_{i=1}^d 2w_i \\log \\frac{1}{\\alpha} +  \\sum_{i=1}^d (w_i \\log w_i - w_i) .\n  \\end{aligned}\\]\n\nTherefore, as $\\alpha \\rightarrow 0$, the first term starts to (significantly) dominate the second, and we can therefore write:\n\n\\[Q(w) \\approx C_\\alpha \\sum_{i=1}^d w_i \\propto \\lVert w \\rVert_1 ,\\]\n\ndue to the fact that $w$ is non-negative by definition.\n\nRemark\n  The above proof shows (again) that the algorithmic bias of gradient descent highly depends on the initialization; the fact that $\\alpha \\rightarrow 0$ was crucial in establishing $\\ell_1$-bias. On the other hand, a different initialization with $\\alpha \\rightarrow \\infty$ leads to a completely different bias: GF automatically provides $\\ell_2$-regularization.8 Woodworth et al. categorize these two distinct behaviors of gradient descent as respectively operating in the “rich” and “kernel” regimes.\n\nRemark\n  The fact that $u(0)$ was initialized with a constant vector (with magnitude $\\alpha$) is unimportant; one can impose different forms of bias with varying choices (or “shapes”) of the initialization10.\n\nRemark\n  At its core, the net effect of square reparameterization is to alter the optimization geometry of gradient descent. This can be connected to classical optimization theory in general Banach spaces, and indeed the dynamics of diagonal linear networks can be viewed as a form of mirror descent (MD). See here11 and here12 for a more thorough treatment.\n\nRemark\n  All of the above assume that the limit of gradient flow admits an exact interpolation. In the presence of noise in the labels the situation is a bit more subtle, and a more careful analysis of the ODEs is necessary; in some cases, for consistent generalization, early stopping may be necessary. See our paper for details13.\n\nRemark\n  (Two-layer dense nets and nuclear norm regularization) (complete).\n\nImplicit bias of gradient descent in nonlinear networks\n\nAs we saw above, changing the architecture fundamentally changes the gradient dynamics (and therefore the implicit regularization induced during training.) But our discussion above focused on networks with linear activations. How does the picture change with more standard networks (such as those with ReLU activations?)\n\nNot surprisingly, life becomes harder when dealing with nonlinearities. Yet, the tools above pave a way to understand algorithmic bias for such networks. Let’s start simple and work our way up.\n\nSingle neurons\n\nConsider a network consisting of a single neuron with nonlinear activation $\\psi : \\R \\rightarrow \\R$. The model becomes:\n\n\\[u = \\psi(\\langle w,x \\rangle),\\]\n\nand we will use the squared error loss to train it:\n\n\\[L(w) = \\frac{1}{2} \\lVert y - \\psi(Xw) \\rVert^2\\]\n\nwhere $X$ is the data matrix. We train the above (single-neuron) network using gradient flow. The following proof is by Vardi and Shamir14.\n\nTheorem\n  Suppose that $\\psi : \\R \\rightarrow \\R$ is a monotonic function. If gradient flow over $L(w)$ converges to zero loss, then the limit $\\bar{w}$ automatically satisfies:\n\n\\[\\begin{aligned}\n  w^* = \\arg \\min_w \\lVert w \\rVert^2,~\\text{s.t.}~y = \\psi(Xw).\n  \\end{aligned}\\]\n\nIn other words, GF provides an implicit bias towards an $\\ell_2$-regularized solution.\nProof\n  Reduce to linear case using monotonicity. (complete)\n\nActivation functions such as leaky-ReLU, sigmoid etc do satisfy monotonicity. (For sigmoid, convergence to zero loss is a bit tricky because of non-convexity.) What about the regular ReLU? This is more difficult, and there does not appear to be a clean “norm”-based regularizer.\n\nMultiple layers\n\nTwo-layer nets. complete.\n\n\n\n\n  \n    \n\n      T. Hastie, J. Friedman, R. Tibshirani, Elements of Statistical Learning. &#x21a9;&#xfe0e;\n    \n    \n\n      R. Schapire, Y. Freund, P. Bartlett, W. Sun Lee, Boosting the margin: A new explanation for the effectiveness of voting methods, 1997. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep learning requires rethinking generalization, 2017. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      J. Geiping, M. Goldblum, P. Pope, M. Moeller, T. Goldstein, Stochastic Training is Not Necessary for Generalization, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      D. Soudry, E. Hoffer, M. Spiegel Nacson, S. Gunasekar, N. Srebro, The Implicit Bias of Gradient Descent on Separable Data, 2018. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      Z. Ji, M. Telgarsky, Directional convergence and alignment in deep learning, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      Z. Ji, M. Dudik, R. Schapire, M. Telgarsky, Gradient descent follows the regularization path for general losses, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      B. Woodworth, S. Gunasekar, J. Lee, E. Moroshko, P. Savarse, I. Golan, D. Soudry, N. Srebro, Kernel and Rich Regimes in Overparametrized Models, 2020. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      D. Gissin, S. Shalev-Shwartz, A. Daniely, The Implicit Bias of Depth: How Incremental Learning Drives Generalization, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Azulay, E. Moroshko, M. Naczon, N. Srebro, B. Woodworth, A. Globerson, D. Soudry, On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Gunasekar, J. Lee, D. Soudry, N. Srebro, Characterizing Implicit Bias in Terms of Optimization Geometry, 2018. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Gunasekar, B. Woodworth, N. Srebro, Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      J. Li, T. Nguyen, C. Hegde, R. Wong, Implicit Sparse Regularization: The Impact of Depth and Early Stopping, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      G. Vardi and O. Shamir, Implicit Regularization in ReLU Networks with the Square Loss, 2020. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      3,
    title:   "Chapter 6 - Beyond NTK",
    author:  null,
    layout:  "page",
    content: "Under construction.\n\n\n  \n    Is NTK the only explanation? Almost surely not.\n  \n  \n    The crux of NTK: weights don’t move very much from their (random) inits\n  \n  \n    Therefore, meaningful feature learning does not happen!\n  \n  \n    However, we know (both from visualizations as well as controlled experiments) that neural networks do learn features using GD.\n  \n  \n    Hessian control: a theory that explains dynamics “far away” from the initialization.\n  \n\n\nSetup\n\nThe PL* condition\n\nHessian control\n",
});
index.addDoc({
    id:      4,
    title:   "Chapter 5 - Optimizing wide networks",
    author:  null,
    layout:  "page",
    content: "In the previous chaper, we proved that (S)GD converges locally for training neural networks with smooth activations. (The case for ReLU networks is harder but still doable with additional algebraic heavy lifting.)\n\nHowever, in practice, somewhat puzzlingly we encounter very deep networks that are regularly optimized to zero training loss, i.e., they exactly learn to interpolate the given training data and labels. Since loss functions are typically non-negative, this means that (S)GD have achieved convergence to the global optimum.\n\nIn this chapter, we ask the question:\n\nWhen and why does (S)GD give zero train loss?\n\n\nAn answer to this question in full generality is not yet available. However, we will prove a somewhat surprising fact: for very wide networks that are randomly initialized, this is indeed the case. The road to proving this will give us several surprising insights, and connections to classical ML (such as kernel methods) will become clear along the way.\n\nLocal versus global minima\n\nLet us bring up this picture from Xu et al.1 again:\n\n\n\nThe picture on the left shows the loss landscape of a feedforward convolutional network. The picture on the right shows the loss landscape of the same network with residual skip connections.\n\nWhy would (S)GD navigate its way to the global minimum of this jagged-y, corrugated loss landscape? There could be one of many explanations.\n\n\n  \n    Explanation 1: We got lucky and initialized very well (close enough to the global optimum for GD to work). But if there is only one global minimum then “getting lucky” is an event that happens with exponentially small probability. So there has to be another reason.\n  \n  \n    Explanation 2: The skips (clearly) matter, so maybe most real-world networks in fact look like the ones on the right. There is a grain of truth here: in modern (very) deep networks, there exist all kinds of bells and whistles to “make things work”. Residual (skip) connections are common. So are other tweaks such as batch normalization, dropout, learning rate scheduling, etc etc. All2 of these3 tweaks4 significantly influence the nature of the optimization problem.\n  \n  \n    Explanation 3: Since modern deep networks are so heavily over-parameterized, there may be tons of minima that exactly interpolate the data. In fact, with high probability, a random initialization lies “close” to a interpolating set of weights. Therefore, gradient descent starting from this location successfully trains to zero loss.\n  \n\n\nWe will pursue Explanation 3, and derive (mathematically) fairly precise justifications of the above claims. As we will argue in the next chapter, this is by no means the full picture. But the exercise is still going to be very useful.\n\nThe Neural Tangent Kernel\n\nOur goal will be to understand what level of over-parameterization enables gradient descent to train neural network models to zero loss. So if the network contains $p$ parameters, we would like to derive scaling laws of $p$ in terms of $n$ and $d$. Since the number of parameters is greater than the number of samples, proving generalization bounds will be challenging. But let’s set aside such troubling thoughts for now.\n\nWhen we think of modern neural networks and their size, we typically associate “growing” the size of networks in terms of their depth. However, our techniques below will instead focus on the width of the network (mirroring our theoretical treatment of network representation capacity). Many results below apply to deep networks, but the key controlling factor is the width (note: actually, not even the width but other strange properties; more later.) The impact of depth will continue to remain elusive.\n\nHere’s a high level intuition of the path forward:\n\n\n  \n    We know that if the loss landscape is convex, then the dynamics of training algorithms such as GD or SGD ensures that we always converge to a global optimum.\n  \n  \n    We know that linear/kernel models exhibit convex loss landscapes.\n  \n  \n    We will prove that for wide enough networks, the loss landscape looks like that of a kernel model.\n  \n\n\nSo in essence, establishing the last statement triggers the chain of implications:\n\n\\[(3) \\implies (2) \\implies (1)\\]\n\nand we are done.\n\n\n\nSome history. The above idea is called the Neural Tangent Kernel, or the NTK, approach. The term itself first appeared in a landmark paper by Jacot et al.5, which first established the global convergence of NN training in the infinite-width limit. Roughly around the same time, several papers emerged that also provided global convergence guarantees in certain specialized families of neural network models. See here6, here7, and here8. All of them involved roughly the same chain of implications as described above, so now can be dubbed as “NTK-style” papers.\n\n\n\nGradient dynamics for linear models\n\nAs a warmup, let’s first examine how GD over the squared-error loss learns the parameters of linear models. Most of this should be classical. For a given training dataset $(x_i, y_i)$, define the linear model using weights $w$, so that the predicted labels enjoy the form:\n\n\\[u_i = \\langle x_i, w \\rangle, \\qquad u = Xw\\]\n\nwhere $X$ is a matrix with the data points stacked row-wise. This leads to the familiar loss:\n\n\\[L(w) = \\frac{1}{2} \\sum_{i=1}^n \\left( y_i - \\langle x_i, w \\rangle \\right)^2 = \\frac{1}{2} \\lVert y - Xw \\rVert^2 .\\]\n\nWe optimize this loss via GD:\n\n\\[w^+ = w - \\eta \\nabla L(w)\\]\n\nwhere the gradient of the loss looks like:\n\n\\[\\nabla L(w) = - \\sum_{i=1}^n x_i (y_i - u_i) =  - X^T (y - u)\\]\n\nand so far everything is good. Now, imagine that we execute gradient descent with infinitesimally small step size $\\eta \\rightarrow 0$, which means that we can view the evolution of the weights as the output of the following ordinary differential equation (ODE):\n\n\\[\\frac{dw}{dt} = - \\nabla L(w) = X^T (y - u).\\]\n\nThis is sometimes called Gradient Flow (GF), and standard GD is rightfully viewed as a finite-time discretization of this ODE. Moreover, due to linearity of the model, the evolution of the output labels $u$ follows the ODE:\n\n\\[\\begin{aligned}\n\\frac{du}{dt} &amp;= X \\frac{dw}{dt}, \\\\\n\\frac{du}{dt} &amp;= XX^T (y-u) \\\\\n&amp;= H (y - u).\n\\end{aligned}\\]\n\nSeveral remarks are pertinent at this point:\n\n\n  \n    Linear ODEs of this nature can be solved in closed form. If $r = y-u$ is the “residual” then the equation becomes:\n\n\\[\\frac{dr}{dt} = - H r\\]\n\n    whose solution, informally, is the (matrix) exponential:\n\n\\[r(t) = \\exp(-Ht) r(0)\\]\n\n    which immediately gives that if $H$ is full-rank with $\\lambda_{\\text{min}} (H) &gt;0$, then GD is extremely well-behaved; it provably converges at an exponential rate towards a set of weights with zero loss.\n  \n  \n    Following up from the first point: the matrix $H = X X^T$, which principally governs the dynamics of GD, is constant with respect to time, and is entirely determined by the geometry of the data. Configurations of data points which push $\\lambda_{\\text{min}} (H)$ as high as possible enable GD to converge quicker, and vice versa.\n  \n  \n    The data points themselves don’t matter very much; all that matters is the set of their pairwise dot products:\n\n\\[H_{ij} = \\langle x_i, x_j \\rangle .\\]\n\n    This immediately gives rise to an alternate way to introduce the well-known kernel trick: simply replace $H$ by any other easy-to-compute kernel function:\n\n\\[K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle\\]\n\n    where $\\phi$ is some feature map. We now suddenly have acquired the superpower of being able to model non-linear features of the data. This derivation also shows that training kernel models is no more challenging than training linear models (provided $K$ is easy to write down.)\n  \n\n\nGradient dynamics for general models\n\nLet us now derive the gradient dynamics for a general deep network $f(w)$. We mirror the above steps. For a given input $x_i$, the predicted label obeys the form:\n\n\\[u_i = f_{x_i}(w)\\]\n\nwhere the subscript of $f$ here makes the dependence on the input features explicit. The squared-error loss becomes:\n\n\\[L(w) = \\frac{1}{2} \\sum_{j=1}^n \\left( y_j - f_{x_j}(w) \\right)^2\\]\n\nand its gradient with respect to any one weight becomes:\n\n\\[\\nabla L(w) \\vert_{\\text{one coordinate}} = - \\sum_{j=1}^n \\frac{\\partial f_{x_j}(w)}{\\partial w} \\left( y_j - u_j \\right) .\\]\n\nTherefore, the dynamics of any one output label – which, in general, depends on all the weights – can be calculated by summing over the partial derivatives over all the weights:\n\n\\[\\begin{aligned}\n\\frac{du_i}{dt} &amp;= \\sum_w \\frac{\\partial u_i}{\\partial w} \\frac{dw}{dt} \\\\\n&amp; = \\sum_w \\frac{\\partial f_{x_i}(w)}{\\partial w} \\left( \\sum_{j=1}^n \\frac{\\partial f_{x_j}(w)}{\\partial w} \\left( y_j - u_j \\right) \\right) \\\\\n&amp;= \\sum_{j=1}^n \\Big\\lang \\frac{\\partial f_{x_i}(w)}{\\partial w}, \\frac{\\partial f_{x_j}(w)}{\\partial w} \\Big\\rang \\left(y_j - u_j \\right) \\\\\n&amp;= \\sum_{j=1}^n H_{ij} (y_j - u_j) ,\n\\end{aligned}\\]\n\nwhere in the last step we switched the orders of summation, and defined:\n\n\\[H_{ij} := \\Big\\lang \\frac{\\partial f_{x_i}(w)}{\\partial w}, \\frac{\\partial f_{x_j}(w)}{\\partial w} \\Big\\rang = \\sum_w \\frac{\\partial f_{x_i}(w)}{\\partial w}  \\frac{\\partial f_{x_j}(w)}{\\partial w} .\\]\n\nWe have used angular brackets to denote dot products. In the case of an infinite number of parameters, the same relation holds. But we have to replace summations by expectations over a measure (and perform additional algebra book-keeping, so let’s just take it as true).\n\nOnce again, we make several remarks:\n\n\n  \n    Observe that the dynamics is very similar in appearance to that of a linear model! in vector form, we get the evolution of all $n$ labels as\n\n\\[\\frac{du}{dt} = H_t (y - u)\\]\n\n    where $H_t$ is an $n \\times n$ matrix governing the dynamics.\n  \n  \n    But! There is a crucial twist! The governing matrix $H_t$ is no longer constant: it depends on the current set of weights $w$, and therefore is a function of time — hence the really pesky subscript $t$. This also means that the ODE is no longer linear; $H_t$ interacts with $u(t)$, and therefore the picture is far more complex.\n  \n  \n    One can check that $H_t$ is symmetric and positive semi-definite; therefore, we can view the above equation as the dynamics induced by a (time-varying) kernel mapping. Moreover, the corresponding feature map is nothing but:\n\n\\[\\phi : x \\mapsto \\frac{\\partial f_{x}(w)}{\\partial w }\\]\n\n    which can be viewed as the “tangent model” of $f$ at $w$. This is a long-winded explanation of the origin of the name “NTK” for the above analysis.\n  \n\n\nWide networks exhibit linear model dynamics\n\nThe above calculations give us a mechanism to understand how (and under what conditions) gradient dynamics of general networks resemble those of linear models. Basically, our strategy will be as follows:\n\n\n  \n    We will randomly initialize weights at $t=0$.\n  \n  \n    At $t=0$, we will prove that the corresponding NTK matrix, $H_0$, is full-rank and that its eigenvalues are bounded away from zero.\n  \n  \n    For large widths, we will show that $H_t \\approx H_0$, i.e., the NTK matrix stays approximately constant. In particular, the dynamics always remains full rank.\n  \n\n\nCombining $1+2+3$ gives the overall proof. This proof appeared in Du et al.6 and the below derivations are adapted from this fantastic book 9.\n\n\n\nConcretely, we consider two-layer networks with $m$ hidden neurons with twice-differentiable activations $\\psi$ with bounded first and second derivatives. This again means that ReLU doesn’t count, but other analogous proofs can be derived for ReLUs; see6.\n\nFor ease of derivation, let us assume that the second layer weights are fixed and equal to $\\pm 1$ chosen equally at random, and that we only train the first layer. This assumption may appear strange, but (a) all proofs will go through if we train both layers, and (b) the first layer weights are really the harder ones in terms of theoretical analysis. (Exercise: Show that if we flip things around and train only the second layer, then really we are fitting a linear model to the data.)\n\nTherefore, the model assumes the form:\n\n\\[f_x(w) = \\frac{1}{\\sqrt{m}} \\sum_{r=1}^m a_r \\psi(\\langle w_r, x \\rangle) .\\]\n\nwhere $a_r = \\pm 1$ and the scaling $\\frac{1}{\\sqrt{m}}$ is chosen to make the algebra below nice. (Somewhat curiously, this exact scaling turns out to be crucial, and we will revisit this point later.)\n\nWe initialize all weights $w_1(0), w_2(0), \\ldots, w_r(0)$ according to a standard normal distribution. In neural network Since we are only using 2-layer feedforward networks, the gradient at time $t=0$ becomes:\n\n\\[\\frac{\\partial f_{x_i}(w(0))}{\\partial w_r} = \\frac{1}{\\sqrt{m}} a_r x_i \\psi'( \\langle w_r(0), x_i \\rangle )\\]\n\nwith respect to the weights of the $r^{th}$ neuron. As  per our above derivation, at time $t=0$, we get that the NTK has entries:\n\n\\[\\begin{aligned}\n[H(0)]_{ij} &amp;= \\Big\\lang \\frac{\\partial f_{x_i}}{\\partial w_r}, \\frac{\\partial f_{x_j}}{\\partial w_r} \\Big\\rang \\\\\n&amp;= x_i^T x_j\\Big[ \\frac{1}{m} \\sum_{r=1}^m a_r^2 \\psi'( \\langle w_r(0), x_i \\rangle ) \\psi'( \\langle w_r(0), x_j \\rangle ) \\Big]\n\\end{aligned}\\]\n\nThere is quite a bit to parse here. The main point here is to note that each entry of $H(0)$ is the average of $m$ random variables whose expectation equals:\n\n\\[x_i^T x_j \\mathbb{E}_{w \\sim \\mathcal{N}(0,I)} \\psi'(x_i^T w) \\psi'(x_j^T w) := H^*_{ij}.\\]\n\nIn other words, if we had infinitely many neurons in the hidden layer then the NTK at time $t=0$ would equal its expected value, given by the matrix $H^$. (Exercise: It is not hard to check that for $m = \\Omega(n)$ and for data in general position, $H^$ is full rank; prove this.)\n\nOur first theoretical result will be a bound on the width of the network that ensures that $H(0)$ and $H^*$ are close.\n\nTheorem\n  Fix $\\varepsilon &gt;0$. Then, with high probability we have\n\n\\[\\lVert H(0) - H^* \\rVert_2 \\leq \\varepsilon\\]\n\nprovided the hidden layer has at least\n\n\\[m \\geq \\tilde{O} \\left( \\frac{n^4}{\\varepsilon^2} \\right)\\]\n\nneurons.\n\nOur second theoretical result will be a width bound that ensures that $H(t)$ remains close to $H(0)$ throughout training.\n\nTheorem\n  Suppose that $y_i = \\pm 1$ and $u_i(\\tau)$ remains bounded throughout training, i.e., for $0 \\leq \\tau &lt; t$. Fix $\\varepsilon &gt;0$. Then, with high probability we have\n\n\\[\\lVert H(0) - H^* \\rVert_2 \\leq \\varepsilon\\]\n\nprovided the hidden layer has at least\n\n\\[m \\geq \\tilde{O} \\left( \\frac{n^6 t^2}{\\varepsilon^2} \\right)\\]\n\nneurons.\n\n\n\nBefore providing proofs of the above statements, let us make several more remarks.\n\n\n  \n    The above results show that the width requirement scales polynomially with the number of samples. (In fact, it is a rather high degree polynomial.) Subsequent works have tightened this dependence; these10 papers11 were able to achieve a quadratic scaling of $m = \\tilde{O}(n^2)$ hidden neurons for GD to provably succeed. As far as I know, the current best bound is sub-quadratic  ($O(n^{\\frac{3}{2}})$), using similar arguments as above; see here12.\n  \n  \n    The above derivation is silent on the dimension and the geometry of the input data. If we assume additional structure on the data, we can improve the dependence to $O(nd)$; see our paper13. However, the big-oh here hides several data-dependent constants that could become polynomially large themselves.\n  \n  \n    For $L$-layer networks, the best available bounds are rather weak; widths need to scale as $m = \\text{poly}(n, L)$. See here7 and here14.\n  \n\n\n\n\nLazy training\n\n** (Complete) **\n\nProofs\n\n** (COMPLETE) **.\n\n\n\n\n  \n    \n\n      H. Li, Z. Xu, G. Taylor, C. Studer, T. Goldstein, Visualizing the Loss Landscape of Neural Nets, 2018. &#x21a9;&#xfe0e;\n    \n    \n\n      M. Hardt and T. Ma, Identity Matters in Deep Learning, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      H Daneshmand, A Joudaki, F Bach, Batch normalization orthogonalizes representations in deep random networks, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      Z. Li and S. Arora, An exponential learning rate schedule for deep learning, 2019. &#x21a9;&#xfe0e;\n    \n    \n\n      A. Jacot, F. Gabriel. C. Hongler, Neural Tangent Kernel: Convergence and Generalization in Neural Networks, 2018. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Du, X. Zhai, B. Poczos, A. Singh, Gradient Descent Provably Optimizes Over-parameterized Neural Networks, 2019. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2 &#x21a9;&#xfe0e;3\n    \n    \n\n      Z. Allen-Zhu, Y. Li, Z. Song, Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers, 2019. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, J. Pennington, Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent, 2019. &#x21a9;&#xfe0e;\n    \n    \n\n      R. Arora, S. Arora, J. Bruna. N. Cohen, S. Du. R. Ge, S. Gunasekar, C. Jin, J. Lee, T. Ma, B. Neyshabur, Z. Song, Theory of Deep Learning,  2021. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Oymak, M. Soltanolkotabi, Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?, 2019. &#x21a9;&#xfe0e;\n    \n    \n\n      Z. Song and X. Yang, Over-parametrization for Learning and Generalization in Two-Layer Neural Networks, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Song, A. Ramezani-Kebrya, T. Pethick, A. Eftekhari, V. Cevher, Subquadratic Overparameterization for Shallow Neural Networks, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      T. Nguyen, R. Wong, C. Hegde, Benefits of Jointly Training Autoencoders: An Improved Neural Tangent Kernel Analysis, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      D. Zou, Y. Cao, D. Zhou, Q. Gu, Gradient descent optimizes over-parameterized deep ReLU networks, 2020. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      5,
    title:   "Chapter 4 - A primer on optimization",
    author:  null,
    layout:  "page",
    content: "In the first few chapters, we covered several results related to the representation power of neural networks. We obtained estimates — sometimes tight ones — for the sizes of neural networks needed to memorize a given dataset, or to simulate a target prediction function.\n\nHowever, most of our theoretical results used somewhat funny-looking constructions of neural networks. Our theoretically best-performing networks were all either too wide or too narrow, and didn’t really look like the typical deep networks that we see in practice.\n\nBut even setting aside the issue of size, none of the (theoretically attractive) techniques that we used to memorize datasets resemble deep learning practice. When folks refer to “training models”, they almost always are talking about fitting datasets to neural networks via local, greedy, first-order algorithms such as gradient descent (GD), or stochastic variations (like SGD), or accelerations (like Adam), or some other such approach.\n\nIn the next few chapters, we address the question:\n\nDo practical approaches for model training work well?\n\n\nThis question is once again somewhat ill-posed, and we will have to be precise about what “work” and “well” mean in the above question. (From the practical perspective: an overwhelming amount of empirical evidence seems to suggest that they work just fine, as long as certain tricks/hacks are applied.)\n\nWhile there is very large variation in the way we train deep networks, we will focus on a handful of the most canonical settings, analyze them, and derive precise bounds on their behavior. The hope is that such analysis can illuminate differences/tradeoffs between different choices and provide useful thumb rules for practice.\n\nSetup\n\nMost (all?) popular approaches in deep learning do the following:\n\n\n  \n    Write down the loss (or empirical risk) in terms of the training data points and the weights. (Usually the loss is decomposable across the data points). So if the data is $(x_i,y_i)_{i=1}^n$ then the loss looks something like:\n\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n l(y_i,\\hat{y_i}), \\quad \\hat{y_i} = f_w(x_i),\\]\n\n    where $l(\\cdot,\\cdot) : \\R \\times \\R \\rightarrow \\R_{\\geq 0}$ is a non-negative measure of label fit, and $f_w$ is the function represented by the neural network with weight/bias parameters $w$. We are abusing notation here since we previously used $R$ for the risk, but let’s just use $L$ to denote loss.\n  \n  \n    Then, we seek the weights/biases $w$ that optimize the loss:\n\n\\[\\hat{w} = \\arg \\min_w L(w) .\\]\n\n    Sometimes, we throw in an extra regularization term defined on $w$ for kicks, but let’s just stay simple for now.\n  \n  \n    Importantly, invariably, the above optimization is carried out using a simple, iterative, first-order method. For example, if gradient descent (GD) is the method of choice, then discovering $\\hat{w}$ amounts to iterating the following recursion:\n\n\\[w \\leftarrow w - \\eta \\nabla_w L(w)\\]\n\n    some number of times. Or, if SGD is the method of choice, then discovering $\\hat{w}$ amounts to iterating the following recursion:\n\n\\[w \\leftarrow w - \\eta g_w\\]\n\n    where $g_w$ is some (properly defined) stochastic approximation of $\\nabla L(w)$. Or if Adam1 is the method of choice, then discovering $\\hat{w}$ amounts to iterating…a slightly more complicated recursion, but still a first-order method involving gradients and nothing more. You get the picture.\n  \n\n\nIn this chapter, let us focus on GD and SGD. The following questions come to mind:\n\n\n  Do GD and SGD converge at all.\n  If so, do they converge to the set (or a set?) of globally optimal weights.\n  How many steps do they need to converge reliably.\n  How to pick step size, or (in the case of SGD) batch size, or other parameters.\n\n\namong many others.\n\nBefore diving in to the details, let us qualitatively address the first couple of questions.\n\nIntuition tells us that convergence in a local sense is to be expected, but converegence to global minimizers seems unlikely. Indeed, it is easy to prove that for anything beyond the simplest neural networks, the loss function $L(w)$ is extremely non-convex. Therefore the “loss landscape” of $L(w)$, viewed as a function of $w$, has many peaks, valleys, and ridges, and a myopic first-order approach such as GD may be very prone to get stuck in local optima, or saddle points, or other stationary points. A fantastic paper2 by Xu et al. proposes creative ways of visualizing these high-dimensional landscapes.\n\n\n\nSomewhat fascinatingly, however, it turns out that this intuition may not be correct. GD/SGD can be used to train models all the way down to zero train error (at least, this is common for deep networks used in classification.) This fact seems to have been folklore, but was systematically demonstrated in a series of interesting experiments by Zhang et al.3.\n\nWe will revisit this fact in the next two chapters. But for now, we limit ourselves to analyzing the local convergence behavior of GD/SGD. We establish the more modest claim:\n\n\n  (S)GD converges to (near) stationary points, provided $L(w)$ is smooth.\n\n\nThe last caveat — that $L(w)$ is required to be smooth — is actually a rather significant one, and excludes several widely used architectures used in practice. For example, the ubiquitous ReLU activation function, $\\psi(z) = \\max(z,0)$, is not smooth, and therefore neural networks involving ReLUs don’t lead to smooth losses.\n\nThis should not deter us. The analysis is still very interesting and useful; it is still applicable to other widely used architectures; and extensions to ReLUs can be achieved with a bit more technical heavy lifting (which we won’t cover here). For a more formal treatment of local convergence in networks with nonsmooth activations, see, for example, the paper by Ji and Telgarsky4, or these lecture notes5.\n\nGradient descent\n\nWe start with analyzing gradient descent for smooth loss functions. (As asides, we will also discuss application of our results to convex functions, but these functions are not common in deep learning and therefore not our focus.)\n\nLet us first define smoothness. (All norms below refer to the 2-norm, and all gradients are with respect to the parameters.)\n\nDefinition\n  $L$ is said to be $\\beta$-smooth if $L$ has $\\beta$-Lipschitz gradients:\n\n\\[\\lVert \\nabla L(w) - \\nabla L(u) \\rVert \\leq \\beta \\lVert w - u \\rVert .\\]\n\nWith some algebra, one can arrive at the following lemma.\n\nLemma\n  If $L$ is twice-differentiable and $\\beta$-smooth, then the eigenvalues of its Hessian are less than $\\beta$:\n\n\\[\\nabla^2 L(w) \\preceq \\beta I.\\]\n\nEquivalently, $L(w)$ is upper-bounded by a quadratic function:\n\n\\[L(w) \\leq L(u) + \\langle \\nabla L(u), w-u \\rangle + \\frac{\\beta}{2} \\lVert w-u \\rVert^2 .\\]\n\nfor all $w,u$.\n\nBasically, the smoothness condition (or its implications according to the above Lemma) says that if $\\beta$ is not unreasonably big, then the gradients of $L(w)$ are rather well-behaved.\n\nIt is natural to see why something like this condition is needed to analyze GD. If smoothness did not hold and the gradient was not well-behaved, then first-order methods such as GD are not likely to be very informative.\n\nThere is a second natural reason why this definition is relevant to GD. Imagine, for a moment, not minimizing $L(w)$, but rather minimizing the upper bound:\n\n\\[B(w) := L(u) + \\langle \\nabla L(u), w-u \\rangle + \\frac{\\beta}{2} \\lVert w-u \\rVert^2 .\\]\n\nThis is a convex (in fact, quadratic) function of $w$. Therefore, it can be optimized very easily by setting $\\nabla B(w)$ to zero and solving for $w$:\n\n\\[\\begin{aligned}\n\\nabla B(w) &amp;= 0, \\\\\n\\nabla L(u) &amp;+ \\beta (w - u) = 0, \\qquad \\text{and thus} \\\\\nw &amp;= u - \\frac{1}{\\beta} \\nabla L(u) .\n\\end{aligned}\\]\n\nThis is the same as a single step of gradient descent starting from $u$ (with step size inversely proportional to the smoothness parameter). In other words, gradient descent is nothing but the successive optimization of a Lipschitz upper bound of in every iteration6.\n\nWe are now ready to prove our first result.\n\nTheorem\n  If $L$ is $\\beta$-smooth, then GD with fixed step size converges to stationary points.\n\nProof\n  Consider any iteration of GD (with a step size $\\eta$ which we will specify shortly):\n\n\\[w = u - \\eta \\nabla L(u) .\\]\n\nThis means that:\n\n\\[w - u = - \\eta \\nabla L(u).\\]\n\nPlug this value of $w-u$ into the quadratic upper bound to get:\n\n\\[\\begin{aligned}\n  L(w) &amp;\\leq L(u) - \\eta \\lVert \\nabla L(u) \\rVert^2 + \\frac{\\beta \\eta^2}{2} \\lVert \\nabla L(u) \\rVert^2, \\quad \\text{or} \\\\\n  L(w) &amp;\\leq L(u) - \\eta \\left( 1 - \\frac{\\beta \\eta}{2} \\right) \\lVert \\nabla L(u) \\rVert^2 .\n  \\end{aligned}\\]\n\nThis inequality already gives a proof of convergence. Suppose that the step size is small enough such that $\\eta &lt; \\frac{2}{\\beta}$. We are in one of two situations:\n\n\n  \n    Either $\\nabla L(u) = 0$, in which case we are done — since $u$ is a stationary point.\n  \n  \n    Or $\\nabla L(u) \\neq 0$, in which case $\\lVert \\nabla L(u) \\rVert &gt; 0$ and the second term in the right hand side is strictly positive. Therefore GD makes progress (and decreases $L$ in the next step).\n  \n\n\nSince $L$ is lower bounded by 0 (since we have assumed a non-negative loss), we get convergence.\n\nThis argument does not quite tell us how many iterations are needed by GD. To estimate this, let us just set $\\eta = \\frac{1}{\\beta}$. This choice is not precisely necessary to get similar bounds, but the algebra becomes simpler. Let’s just rename $w_i := u$ and $w_{i+1} := w$. Then, the last inequality becomes:\n\n\\[\\frac{1}{2\\beta} \\lVert \\nabla L(w_i) \\rVert^2 \\leq L(w_i) - L(w_{i+1}).\\]\n\nTelescoping from $w_0, w_1, \\ldots, w_t$, we get:\n\n\\[\\begin{aligned}\n  \\frac{1}{2\\beta} \\sum_{i = 0}^{t-1} \\lVert \\nabla L(w_i) \\rVert^2 &amp;\\leq L(w_0) - L(w_t) \\\\\n  &amp;\\leq L(w_0) - L_{\\text{opt}}\n  \\end{aligned}\\]\n\n(since $L_\\text{opt}$ is the smallest achievable loss.) Therefore, if we pick $i = \\arg \\min_{i &lt; t} \\nabla \\lVert L(w_i) \\rVert^2$ as the estimate with lowest gradient norm and set $\\hat{w} = w_{i}$, then we get:\n\n\\[\\frac{t}{2\\beta} \\lVert \\nabla L(\\hat{w}) \\rVert^2 \\leq L_0 - L_{\\text{opt}},\\]\n\nwhich implies that if $L_0$ is bounded (i.e.: we start somewhere reasonable) then GD reaches a point $\\hat{w}$ within at most $t$ iterations whose gradient norm is\n\n\\[\\lesssim \\sqrt{\\frac{\\beta}{t}}\\]\n\nat most. To put it a different way, to find an $\\varepsilon$-stationary point, GD needs:\n\n\\[O\\left( \\frac{\\beta}{\\varepsilon^2} \\right)\\]\n\niterations.\n\nNotice that the proof is simple: we didn’t use much information beyond the definition of Lipschitz smoothness. But it already reveals a lot.\n\nFirst, step sizes in standard GD can be set to a constant. Later, we will analyze SGD (where step sizes have to be variable.)\n\nSecond, step sizes should be chosen inversely proportional to $\\beta$. This makes intuitive sense: if $\\beta$ is large then gradients are wiggling around, and therefore it is prudent to take small steps. On the other hand, it is not easy to estimate Lipschitz smoothness constants (particularly for neural networks), so in practice $\\eta$ is just tuned by hand.\n\nThird, we only get convergence in the “neighborhood” sense (in that there is some point along the trajectory which is close to the stationary point). It is harder to prove “last-iterate” convergence results. In fact, one can even show that GD can go near a stationary point, spend a very long time near this point, but then bounce away later7.\n\nFourth, we get $\\frac{1}{\\sqrt{t}}$ error after $t$ iterations. The terminology to describe this error rate is not very consistent in the optimization literature, but one might call this a “sub-linear” rate of convergence.\n\n\n\nLet us now take a lengthy detour into classical optimization. What if $L$ were smooth and convex? As mentioned above, convex losses (as a function of the weights) are not common in deep learning; but it is instructive to understand how much convexity can buy us.\n\nLife is much simpler now, since we can expect GD to find a global minimizer if $L$ is convex (i.e., not just a point where $\\nabla L \\approx 0$, but actually a point where $L \\approx L_{\\text{opt}}$).\n\nHow do we show this? While smoothness shows that $L$ is upper bounded by a quadratic function, convexity implies that $L$ is also lower bounded by tangents at every point. The picture looks like this:\n\n\n\nand mathematically, we have:\n\n\\[L(w) \\geq L(u) + \\langle \\nabla L(u), w-u \\rangle .\\]\n\nThis lets us control not just $\\nabla L$ but $L$ itself. Formally, we obtain:\n\nTheorem\n  If $L$ is $\\beta$-smooth and convex, then GD with fixed step size converges to a minimizer.\n\nProof\n\nLet $w^*$ be some minimizer, which achieves loss $L_{\\text{opt}}$.\n\n(Q. What if there is more than one minimizer? Good question!)\n\nSet $\\eta = \\frac{1}{\\beta}$, as before. We will bound the error in weight space as follows:\n\n\\[\\begin{aligned}\n  \\lVert w_{i+1} - w^* \\rVert^2 &amp;= \\lVert w_i - \\frac{1}{\\beta} \\nabla L(w_i) - w^* \\rVert^2 \\\\\n  &amp;= \\lVert w_i - w^* \\rVert^2 - \\frac{2}{\\beta} \\langle \\nabla L(w_i), w_i - w^* \\rangle + \\frac{1}{\\beta^2} \\lVert \\nabla L(w_i) \\rVert^2 . \\\\\n  \\end{aligned}\\]\n\nFrom the smoothness proof above, we already showed that\n\n\\[\\lVert \\nabla L(w_i) \\rVert^2 \\leq 2 \\beta \\left(L(w_i) - L(w_{i+1})\\right).\\]\n\nMoreover, plugging in $u = w_i$ and $w = w^*$ in the convexity lower bound, we get:\n\n\\[\\langle \\nabla L(w_i), w^* - w_i \\rangle \\leq L_{\\text{opt}} - L(w_i) .\\]\n\nTherefore, we can bound both the rightmost terms in the weight error:\n\n\\[\\begin{aligned}\n  \\lVert w_{i+1} - w^* \\rVert^2 &amp;\\leq \\lVert w_i - w^* \\rVert^2 + \\frac{2}{\\beta} \\left(L(w_i) - L(w_{i+1}) + L_{\\text{opt}} - L(w_i) \\right) \\\\\n  &amp;= \\lVert w_i - w^* \\rVert^2 - \\frac{2}{\\beta} (L_{i+1} - L_{\\text{opt}}).\n  \\end{aligned}\\]\n\nTherefore, we can invoke a similar argument as in the smoothness proof. One of two situations:\n\n\n  \n    Either $L_{i+1} = L_{\\text{opt}}$, which means we have achieved a point with optimal loss.\n  \n  \n    Or, $L_{i+1} &gt; L_{\\text{opt}}$, which means GD decreases weight error.\n  \n\n\nTherefore, we get convergence. In order to estimate the number of iterations, rearrange terms:\n\n\\[L_{i+1} - L_{\\text{opt}} \\leq \\frac{\\beta}{2} \\left( \\lVert w_i - w^* \\rVert^2 - \\lVert w_{i+1} - w^* \\rVert^2 \\right)\\]\n\nand telescope $i$ from 0 to $t-1$ to get:\n\n\\[\\sum_{i=0}^{t-1} L_i - t L_{\\text{opt}} \\leq \\frac{\\beta}{2} \\left( \\lVert w_0 - w^* \\rVert^2 - \\lVert w_t - w^* \\rVert^2 \\right)\\]\n\nwhich gives:\n\n\\[\\begin{aligned}\n  L_t &amp;\\leq \\frac{1}{t} \\sum_{i=0}^{t-1} L_i \\\\\n  &amp;\\leq L_{\\text{opt}} + \\frac{\\beta}{2t} \\lVert w_0 - w^* \\rVert^2 .\n  \\end{aligned}\\]\n\nTherefore, the optimality gap decreases as $1/t$ and to find an $\\varepsilon$-approximate point (assuming we start somewhere reasonable), GD needs $O(\\frac{\\beta}{\\varepsilon})$ iterations.\n\nSimilar conclusions as above. Constant step size suffices for GD. Step size should be inversely proportional to smoothness constant. Convexity gives us a “last-iterate” bound, as well as parameter estimation guarantees.\n\n\n\nAnother aside: $L$ smooth and strongly convex? Then $L$ is both lower and upper bounded by quadratics. Therefore, optimization is easy; exponential ($e^{-t}$) rate of convergence. Fill this in.\n\n\n\nSo the hierarchy is as follows:\n\n\n  \n    GD assuming smoothness: $\\frac{1}{\\sqrt{t}}$ rate\n  \n  \n    GD assuming smoothness + convexity: $\\frac{1}{t}$ rate\n  \n  \n    Momentum accelerated GD: $\\frac{1}{t^2}$ rate. We won’t prove this; see the paper by Nesterov. Remarkably, this is the best possible one can do with first-order methods such as GD.\n  \n  \n    GD assuming smoothness and strong convexity: $\\exp(-t)$ rate .\n  \n\n\nThe Polyak-Lojasiewicz (PL) condition\n\nAbove, we saw how leveraging smoothness, along with (strong) convexity, of the loss results in exponential convergence of GD. However (strong) convexity is not that relevant in the context of deep learning. This is because losses are very rarely convex in their parameters.\n\nHowever, there is a different characterization of functions (other than convexity) that also implies fast convergence rates of GD. This property was introduced by Polyak8 in 1963, but has somehow not been very widely publicized. It was re-introduced to the ML optimization literature by Karimi et al.9 and its relevance (particularly in the context of deep learning) is slowly becoming apparent.\n\nDefinition\n  A function $L$ (whose optimum is $L_{\\text{opt}}$) is said to satisfy the Polyak-Lojasiewicz (PL) condition with parameter $\\alpha$ if:\n\n\\[\\lVert \\nabla L(u) \\rVert^2 \\geq 2 \\alpha (L(u) - L_{\\text{opt}} )\\]\n\nfor all $u$ in its domain.\n\nThe reason for the “2” sitting before $\\alpha$ will become clear. Intuitively, the PL condition says that if $L(u) \\gg  L_{\\text{opt}}$ then $\\lVert \\nabla L(u) \\rVert$ is also large. More precisely, the norm of the gradient at any point grows at least as the square root of the (functional) distance to the optimum.\n\nNotice that there is no requirement of convexity in the definition of the PL condition. For example, the function:\n\n\\[L(x) = x^2 + 3 \\sin^2 x\\]\n\nhas a plot that looks like\n\n\n\nwhich is non-convex upon inspection, but nonetheless satisfies the PL condition with constant $\\alpha = \\frac{1}{32}$.\n\n(However, the converse is true. Strong convexity implies the PL condition, but PL is far more general. We return to this in Chapter 6.)\n\nWe immediately get the following result.\n\nTheorem\n  If $L$ is $\\beta$-smooth and satisfies the PL condition with parameter $\\alpha$, then GD exponentially converges to the optimum.\n\nProof\n  Follows trivially. Let $\\eta = \\frac{1}{\\beta}$. Then\n\n\\[w_{t+1} = w_t - \\frac{1}{\\beta} \\nabla L(w_t) .\\]\n\nPlug $w_{t+1} - w_t$ into the smoothness upper bound. We get:\n\n\\[L(w_{t+1}) \\leq L(w_t) - \\frac{1}{2\\beta} \\lVert \\nabla L(w_t) \\rVert^2 .\\]\n\nBut since $L$ satisfies PL, we get:\n\n\\[L(w_{t+1}) \\leq L(w_t) - \\frac{\\alpha}{\\beta} \\left( L(w_{t+1}) - L(w_\\text{opt}) \\right).\\]\n\nSimplifying notation, we get:\n\n\\[L_{t+1} - L_{\\text{opt}} \\leq \\left(1 - \\frac{\\alpha}{\\beta} \\right) \\left( L_{t} - L_{\\text{opt}} \\right) .\\]\n\nwhich implies that GD converges at $\\exp(-\\frac{\\alpha}{\\beta}t)$ rate.\n\n\n\nWhy is the PL condition interesting? It has been shown that several neural net training problems satisfy PL.\n\n\n  \n    Single neurons (with leaky ReLUs.)\n  \n  \n    Linear neural networks.\n  \n  \n    Linear residual networks (with square weight matrices).\n  \n  \n    Others? Wide networks? Complete.\n  \n\n\nStochastic gradient descent (SGD)\n\nWe have obtained a reasonable picture of how GD works, how many iterations it needs, etc. But how does the picture change with inexact (stochastic) gradients?\n\nThis question is paramount in deep learning practice, since nobody really does full-batch GD. Datasets are massive, and since the loss is decomposable across all the data points, gradients of the loss require making a full sweep of the training dataset for each iteration, which no one really has time.\n\nEven more so, it seems that stochastic gradients (instead of full gradients) may influences generalization behavior. Anecdotally, it was observed that models trained by SGD typically improved over models trained with full-batch gradient descent. Therefore, there may be some hidden benefit of stochasticity.\n\nTo explain this, there were a ton of papers discussing the distinction between “sharp” versus “flat” minima10, and how the latter type of minima generalize better, and how minibatch methods (such as SGD) favor flat minima, and therefore SGD gives better solutions period. Folks generally went along with this explanation. However, since this initial flurry of papers this common belief has since been somewhat upended.\n\nFirst off, it is not really clear how “sharpness” or “flatness” should be formally defined. A paper by Dinh et al.11 showed that good generalization can be obtained even if the model corresponds to a very “sharp” minimum in the loss landscape (for most commonly accepted definitions of “sharp”). So even if SGD finds flatter minima, it is unclear whether such minima are somehow inherently better.\n\nA very recent paper by Geiping et al.12 in fact finds the opposite; performance by GD (with properly tuned hyperparameters and regularization) matches that of SGD. Theory is silent on this matter and I am not aware of any concrete separation-type between GD and SGD for neural networks.\n\nStill, independent of whether GD is theoretically better than SGD or not, it is instructive to analyze SGD (since everyone uses it.) Let us derive an analogous bound on the error rates of SGD.\n\nIn SGD, our updates look like:\n\n\\[w_{i+1} = w_i - \\eta_i g_i\\]\n\nHere $g_i$ is a noisy version to the full gradient $\\nabla L(w_i)$ (where the “noise” here is due to minibatch sampling). Due to stochasticity, $g_i$ (and therefore, $w_i$) are random variables, and we should analyze convergence in terms of their expected value.\n\nIntuitively, meaningful progress is possible only when the noise variance is not too large; this is achieved if the minibatch size is not too small.\nThe following assumptions are somewhat standard (although rigorously proving this takes some effort.)\n\nProperty 1: Unbiased gradients: We will assume that in expectation, $g_i$ is an unbiased estimate of the true gradient. In other words, if $\\varepsilon_i = g_i - \\nabla L(w_i)$, then\n\n\\[\\mathbb{E}{\\varepsilon_i} = 0 .\\]\n\nProperty 2: Bounded gradients: We will assume that the gradients are uniformly bounded in magnitude by a constant:\n\n\\[\\max_i \\lVert g_i \\rVert \\leq G.\\]\n\nRemark\nProperty 1 is fine if we sample the terms in the gradient uniformly at random. Property 2 is hard to justify in practice. (In fact, for convex functions this is not even true!) But better proofs such as this13 and this14 have shown that similar rates of convergence for SGD are possible even if we relax this assumption, so let’s just go with this for now and assume it can be fixed.\n\nAssuming the above two properties, we will prove:\n\nTheorem\n  If $L$ is $\\beta$-smooth, then SGD converges (in expectation) to an $\\varepsilon$-approximate critical point in $O\\left(\\frac{\\beta}{\\varepsilon^4}\\right)$ steps.\n\nProof\n  Let’s run SGD with fixed step-size $\\eta$ for $t$ steps. From the smoothness upper bound, we get:\n\n\\[L(w_{t+1}) \\leq L(w_t) + \\langle \\nabla L(w_t), w_{t+1} - w_t \\rangle + \\frac{\\beta}{2} \\lVert w_{t+1} - w_t \\rVert^2 .\\]\n\nBut $w_{t+1} - w_t = - \\eta g_t$. Plugging into the above bound and rearranging terms:\n\n\\[\\eta \\langle L(w_t), g_t \\rangle \\leq L(w_t) - L(w_{t+1}) + \\frac{\\beta}{2} \\eta^2 \\lVert g_t \\rVert^2 .\\]\n\nTake expectation on both sides. Property 1 and 2 give us:\n\n\\[\\eta \\mathbb{E} \\lVert L(w_t) \\rVert^2 \\leq \\mathbb{E} \\left( L(w_t) - L(w_{t+1}) \\right) + \\frac{\\beta}{2} \\eta^2 G^2 .\\]\n\nTelescope from 0 through $T$, and divide by $\\eta T$. Then we get:\n\n\\[\\min_{t &lt; T} \\mathbb{E} \\lVert L(w_t) \\rVert^2 \\leq \\frac{L_0 - L_T}{\\eta T} + \\frac{\\beta \\eta G^2}{2}.\\]\n\nThis is true for all $\\eta$. In order to get the tightest upper bound and minimize the right hand side, we need to balance the two terms on the right. This is achieved if:\n\n\\[\\eta = O(\\frac{1}{\\sqrt{T}}).\\]\n\nPlugging this in, ignoring all other constants, we get:\n\n\\[\\min_{t &lt; T} \\mathbb{E} \\lVert L(w_t) \\rVert^2 \\lesssim \\frac{1}{\\sqrt{T}}, \\quad \\text{or} \\quad  \\mathbb{E} \\lVert L(w_t) \\rVert \\lesssim \\frac{1}{T^{1/4}}.\\]\n\nThis concludes the proof.\n\n\n\nHierarchy:\n\n\n  \n    SGD assuming Lipschitz smoothness: $\\frac{1}{t^{1/4}}$\n  \n  \n    SGD assuming Lipschitz smoothness + convexity: $\\frac{1}{\\sqrt{t}}$\n  \n\n\nOther rates?\n\nExtensions\n\n\n  Nesterov momemtum\n\n\n(COMPLETE).\n\n\n\n\n  \n    \n\n      D. Kingma and J. Ba, Adam: A Method for Stochastic Optimization, 2014. &#x21a9;&#xfe0e;\n    \n    \n\n      H. Li, Z. Xu, G. Taylor, C. Studer, T. Goldstein, Visualizing the Loss Landscape of Neural Nets, 2018. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep learning requires rethinking generalization, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      Z. Ji and M. Telgarsky, Directional convergence and alignment in deep learning, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      M. Telgarsky, Deep Learning Theory, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      There is an entire literature on online optimization that uses this interpretation of gradient descent, but they call it the “Follow-the-leader” strategy. See here for an explanation. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Du, C. Jin, J. Lee, M. Jordan, B. Poczos, A. Singh, Gradient Descent Can Take Exponential Time to Escape Saddle Points, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      B. Polyak, ГРАДИЕНТНЫЕ МЕТОДЫ МИНИМИЗАЦИИ ФУНКЦИОНАЛОВ (Gradient methods for minimizing functionals), 1963. &#x21a9;&#xfe0e;\n    \n    \n\n      H. Karimi, J. Nutini, M. Schmidt, Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Lojasiewicz Condition, 2016. &#x21a9;&#xfe0e;\n    \n    \n\n      N. Keskar, D. Mudigere, J. Nocedal, P. Tang, On Large-Batch Training for Deep Learning, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      L. Dinh, R. Pascanu, S. Bengio, Y. Bengio, Sharp Minima Can Generalize For Deep Nets, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      J. Geiping, M. Goldblum, P. Pope, M. Moeller, T. Goldstein, Stochastic Training is Not Necessary for Generalization, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      L. Bottou, F. Curtis, J. Nocedal, Optimization Methods for Large-Scale Machine Learning, 2018. &#x21a9;&#xfe0e;\n    \n    \n\n      L. Nguyen, P. Ha Nguyen, M. van Dijk, P. Richtarik, K. Scheinberg, M. Takac, SGD and Hogwild! Convergence Without the Bounded Gradients Assumption, 2018. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      6,
    title:   "Chapter 3 - The role of depth",
    author:  null,
    layout:  "page",
    content: "For shallow networks, we now know upper bounds on the number of neurons required to “represent” data, where representation is measured either in the sense of exact interpolation (or memorization), or in the sense of universal approximation.\n\nWe will continue to revisit these bounds when other important questions such as optimization/learning and out-of-sample generalization arise. In some cases, these bounds are even tight, meaning that we could not hope to do any better.\n\nWhich leads us to the natural next question:\n\nDoes depth buy us anything at all?\n\n\nIf we have already gotten (some) tight results using shallow models, should there be any theoretical benefits in pursuing analysis of deep networks? Two reasons why answering this question is important:\n\nOne, after all, this is a course on “deep” learning theory, so we cannot avoid this question.\n\nBut two, for the last decade or so, a lot of folks have been trying very hard to replicate the success of deep networks with highly tuned shallow models (such as kernel machines), but so far have come up short. Understanding precisely why and where shallow models fall short (while deep models succeed) is therefore of importance.\n\nA full answer to the question that we posed above remains elusive, and indeed theory does not tell us too much about why depth in neural networks is important (or whether it is necessary at all!) See the last paragraph of Belkin’s monograph1 for some speculation. To quote another paper by Shankar et al.2:\n\n\n  “…the question remains open whether the performance gap between kernels and neural networks indicates a fundamental limitation of kernel methods or merely an engineering hurdle that can be overcome.”\n\n\nNonetheless: progress can be made in some limited cases. In this note, we will visit several interesting results that highlight the importance of network depth in the context of representation power. Later we will revisit this in the context of optimization.\n\nLet us focus on “reasonable-width” networks of depth $L &gt; 2$ (because we already know from universal approximation that exponential-width networks of depth-2 can represent pretty much anything we like.) There are two angles of inquiry:\n\n\n  \n    Approach 1: prove that there exist datasets of some large enough size that can only be memorized by networks of depth $\\Omega(L)$, but not by networks of depth $o(L)$.\n  \n  \n    Approach 2: prove that there exist classes of functions that can be $\\varepsilon$-approximated only by networks of depth $\\Omega(L)$, but not by networks of depth $o(L)$.\n  \n\n\nTheorems of either type are called depth separation results. Let us start with the latter.\n\nDepth separation in function approximation\n\nWe will first prove a depth separation result for dense feed-forward networks with ReLU activations and univariate (scalar) inputs. This result will generally hold for other families of activations, but for simplicity let’s focus on the ReLU.\n\nWe will explicitly construct a (univariate) function $g$ that can be exactly represented by a deep neural network (with error $\\varepsilon = 0$) but is provably inapproximable by a much shallower network. This result is by Telgarsky3.\n\nTheorem\n  There exists a function $g : [0,1] \\rightarrow \\R$ that is exactly realized by a ReLU network of constant width and depth $O(L^2)$, but for any neural network $f$ with depth $\\leq L$ and number of units, $\\leq 2^{L^\\delta}$ for any $0 &lt; \\delta \\leq 1$, $f$ is at least $\\varepsilon$-far from $g$, i.e.:\n\n\\[\\int_0^1 |f(x) - g(x)| dx \\geq \\varepsilon\\]\n\nfor some absolute constant $\\varepsilon &gt; \\frac{1}{32}$.\n\nThe proof of this theorem is elegant and will inform us also while proving memorization-style depth barriers. But let us first make several remarks on the implications of the results.\n\nRemark\n  The “hard” example function $g$ constructed in the above Theorem is for scalar inputs. What happens for the general case of $d$-variate inputs? Eldan and Shamir4 showed that there exist 3-layer ReLU networks (and $\\text{poly}(d)$ width) that cannot be $\\varepsilon$-approximated by any two-layer ReLU network unless they have $\\Omega(2^d)$ hidden nodes. Therefore, there is already a separation between depth=2 and depth=3 in the high-dimensional case.\n\nRemark\n  In the general $d$-variate case, can we get depth separation results for networks of depth=4 or higher? Somewhat surprisingly, the answer appears to be no. Vardi and Shamir5 showed that a depth separation theorem between ReLU networks of $k \\geq 4$ and $k’ &gt; k$ would imply progress on long-standing open problems in circuit lower bounds6.  To be precise: this (negative) result only applies to vanilla dense feedforward networks. But it is disconcerting that even for the simplest of neural networks, proving clear benefits of depth remains outside the realm of current theoretical machinery.\n\nRemark\n  The “hard” example function $g$ constructed in the above Theorem is highly oscillatory within $[0,1]$ (see proof below). Therefore, it has an unreasonably large (super-polynomial) Lipschitz constant. Perhaps if we limited our attention to simple/natural Lipschitz functions, then it could be easier to prove depth-separation results? Not so: even if we only focused on “benign” functions (easy-to-compute functions with polynomially large Lipschitz constant), proving depth lower bonds would similarly imply progress in long-standing problems in computational complexity. See the recent result by Vardi et al.7.\n\nRemark\n  See this paper8 for an earlier depth-separation result for sum-product networks (which are somewhat less standard architectures).\n\nTelgarsky’s proof uses the following high level ideas:\n\n(a) observe that any ReLU network $g$ simulates a piecewise linear function.\n\n(b) prove that the number of pieces in the range of $g$ grows only polynomially with width but exponentially in depth.\n\n(c) construct a “hard” function $g$ that has an exponential number of linear pieces, and that can be exactly computed by a deep network\n\n(d) but, from part (b), we know that a significantly shallower network cannot simulate so many pieces, thus giving our separation.\n\n\n\nBefore diving into each of them, let us first define (and study) a simple “gadget” neural network that simulates a function $m : \\R \\rightarrow \\R$ which will be helpful throughout. It’s easier to just draw $m(x)$ first:\n\n\n\nand then observe that:\n\n\\[m(x) = \\begin{cases}\n0, \\qquad \\quad ~~ x &lt; 0, \\\\\n2x, \\qquad \\quad 0 \\leq x &lt; 1/2,\\\\\n2 - 2x,~ 1/2 \\leq x &lt; 1,\\\\\n0, \\qquad \\quad ~~ 1 \\leq x.\n\\end{cases}\\]\n\nWhat happens when we compose $m$ with itself several times? Define:\n\n\\[m^{(2)}(x) := m(m(x)), \\ldots, m^{(L)}(x) := m(m^{(L-1)}(x)).\\]\n\nThen, we start seeing an actual sawtooth function. For example, for $L=2$, we see:\n\n\n\nIterating this $L$ times, we get a sawtooth that oscillates a bunch of times over the interval $[0,1]$ and is zero outside9. In fact, for any $L$, an easy induction will show that there will be $2^{L-1}$ “triangles”, which are formed using $2^L$ pieces.\n\nBut we also can observe that $m(x)$ can be written in terms of ReLUs. Specifically, if $\\psi$ is the ReLU activation then:\n\n\\[m(x) = 2\\left(\\psi(x) - 2\\psi(x-\\frac{1}{2}) + \\psi(x-1)\\right),\\]\n\nwhich is a tiny (width-3, depth-2) ReLU network. Therefore, $m^{(L)}(x)$ is a univariate function that is exactly written out as width-3, depth-$2L$ ReLU network for any $L$.\n\nRemark\n  One can view $m^{(L)}(x)$ as a “bit-extractor” function since we have the property that $m^{(L)}(x) = m(\\text{frac}(2^{L-1} x))$, where $\\text{frac}(x) = x - \\lfloor x \\rfloor$ is the fractional part of any real number $x$. (Exercise: Prove this.) It is interesting that similar “bit-extractor” gadgets can be found in some earlier universal approximation proofs10.\n\n\n\nSo: we have constructed a neural network with depth $O(L)$ which simulates a piecewise linear function over the real line with $2^L$ pieces. In fact, this observation can be generalized quite significantly as follows.\n\nLemma\n  If $p(x)$ and $q(x)$ are univariate functions defined over $[0,1]$ with $s$ and $t$ linear pieces respectively, then:\n      (a) $\\alpha p(x) + \\beta q(x)$ has at most $s + t$ pieces over $[0,1]$.\n      (b) $q(p(x))$ has at most $st$ pieces over $[0,1]$.\n\nThe proof of this lemma is an easy counting exercise over the number of “breakpoints” over $[0,1]$. Most relevant to our discussion, we immediately get the important corollary (somewhat informally stated here, there may be hidden constants which I am ignoring):\n\nCorollary\n  For any feedforward ReLU network $f$ with depth $\\leq L$ and width $\\leq 2^{L^\\delta}$, then the total number of pieces  in the range of $f$ is strictly smaller than $2^{L^2}$.\n\nWe are now ready for the proof of the main Theorem.\n\nProof\n  Our target function $g$ will be the sawtooth function iterated $L^2 + 2$ times, i.e.,\n\n\\[g(x) = m^{(L^2 + 2)}(x).\\]\n\nUsing Corollary we will show that that if $f$ is any feedforard ReLU net with depth $\\leq L$ and sub-exponential width, then $f$ cannot approximate a significant fraction of the pieces in $g$. In terms of picture, let us consider the following figure:\n\n\n\nwhere $f$ is in red and $g$ (the target) is in navy blue. Let’s draw the horizontal line $y = 1/2$ in purple. We can count the total number of (tiny) triangles in $g$ to be (exactly) equal to\n\n\\[2^{L^2 + 2} - 1\\]\n\n(the -1 is because the two edge cases have only a half-triangle each). Each of these tiny triangles have height 1/2, so their area is:\n\n\\[\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2^{L^2 + 2}} = 2^{-L^2 - 4} .\\]\n\nBut any linear piece of $f$ has to have zero intersection at least half of these triangles (since this linear piece can either be above the purple line or below, not both). Therefore, if we look at the error restricted to this particular piece, the area under this curve should be at least the area of the missed triangles (shaded in blue). Therefore, the total error is lower bounded as follows:\n\n\\[\\begin{aligned}\n  \\int_0^1 |f - g | dx &amp;\\geq \\text{\\# missed triangles} \\times \\text{area of triangle} \\\\\n  &amp;&gt; \\frac{1}{2} \\cdot 2^{L^2} \\cdot 2^{-L^2 - 4} \\\\\n  &amp;= \\frac{1}{32} .\n  \\end{aligned}\\]\n\nThis completes the proof.\n\nLet us reflect a bit more on the above proof. The key ingredient was the fact that superpositions (adding units, essentially increasing the “width”) only have a polynomial increase on the number of pieces in the range of $g$, but compositions (essentially increasing the “depth”) have an exponential increase in the number of pieces.\n\nBut! this “hard” function $g$, which is the sawtooth over $[0,1]$, was very carefully constructed. To  achieve the exponential scaling law in the number of pieces, the breakpoints in $g$ have to be exactly equispaced, and therefore the weights in every layer in the network have to be identical. Even a tiny perturbation to $g$ dramatically reduces the number of linear pieces in the range of the network. See the following figure illustrated in Hanin and Rolnick (2019)11:\n\n\n\nThe plot on the left is the sawtooth function $g$, which, as we proved earlier, is representable via a depth-$O(L^2)$, width-3 ReLU net. The plot on the right is the function implemented by the same network as $g$ but with a tiny amount of noise added to its weights. So even when we did get a depth-separation result, it’s not at all “robust”.\n\nAll this to say: depth separation results for function approximation can be rather elusive; they seem to only exist for very special cases; and progress in this direction would result in several fundamental breakthroughs in complexity theory.\n\nDepth and memorization\n\nWe will now turn to Approach 1. Let’s say that our goal was a bit more modest, and merely wanted to memorize a bunch of $n$ training data points with $d$ input features. Recall that we already showed that $O(\\frac{n}{d})$ neurons are sufficient to memorize these points using a “peeling”-style proof.\n\nParaphrasing this fact: for depth-2 networks with $m$ hidden neurons, the memorization capacity is of the order of $d\\cdot m$. This is roughly the same as the number of parameters in the network, so parameter counting intuitively tells us that we cannot do much better. What does depth $&gt;2$ give us really?\n\nSeveral recent (very nice) papers have addressed this question. Let us start with the following result by Yun et al.12.\n\nTheorem\n  Let $X = \\lbrace (x_i, y_i)_{i=1}^N \\rbrace \\subset \\R^d \\times \\R$ be a dataset with distinct $x_i$ and $y_i \\in [-1,1]$. Then, there exists a depth-3 ReLU network with hidden units $d_1, d_2$ where:\n\n\\[N \\leq 4 \\lceil \\frac{d_1}{2} \\rceil \\lceil \\frac{d_2}{2} \\rceil .\\]\n\nthat exactly memorizes $X$.\n\nThe proof is somewhat involved, so we will give a brief sketch at the bottom of this page. But let us first study several implications of this result. First, we get the following corollary:\n\nCorollary\n  (Informal) A depth-3 ReLU network with width $d_1 = d_2 = O(\\sqrt{N})$ is sufficient to memorize $N$ points.\n\nThis indicates a concrete separation in terms of memorization capacity between depth-2 and depth-3 networks. Suppose we focus on the regime where $d \\ll N$. For this case, we have achieved a polynomial reduction in the number of hidden neurons in the network from $O(N)$ in the depth-2 case to $O(\\sqrt{N})$ in the depth-3 case.\n\nNotice that the number of parameters in the network still remains $\\Theta(N)$ (and the condition in the above Theorem ensures this, since the “middle” layer has $d_1 \\cdot d_2 \\gtrsim N$ connections.) But there are ancillary benefits in reducing the number of neurons themselves (for example, in the context of hardware implementation) which we won’t get into.\n\nRemark\n  A similar result on memorization capacity can be obtained for situations with multiple labels (e.g. in the multi-class classification setting). If the dimension of the label is $d_y &gt; 1$, then the condition is that $d_1 d_2 \\gtrsim N d_y$.\n\nRemark\n  The multi-label case can be directly applied to provide width-lower bounds on memorizing popular datasets. For example, the well-known ImageNet dataset has about 10M image samples and performs classification over 1000 classes. The width bound suggests that we need networks of width no smaller than $\\sqrt{10^7 \\times 10^3} \\approx 10^5$ ReLUs.\n\nYun et al.12 also obtain a version of their result for depth-$L$ networks:\n\nTheorem\n  Suppose a depth-$L$ ReLU network has widths of hidden layers $d_1, d_2, \\ldots, d_L$ then its memorization capacity is lower bounded by:\n\n\\[N := d_1 d_2 + d_2 d_3 + \\ldots d_{L-2}d_{L-1} ,\\]\n\ni.e., a network with this architecture can be tuned to memorize any dataset with at most $N$ points.\n\nThis result, while holding for general $L$-hidden-layer networks, doesn’t unfortunately paint a full picture; the proof starts with the result for $L = 2$, and then proceeds to show that all labels can be successively memorized “layer-by-layer”. In particular, to memorize $N$ data points, the width requirement remains $O(\\sqrt{N})$ and it is not entirely clear if depth plays a role. We will come back to this shortly.\n\n\n\nSeveral interesting questions arise. First, tightness. The above Theorem shows that depth-2, width-$\\sqrt{N}$ networks are sufficient to memorize training sets of size $N$. But is this width dependence of $\\sqrt{N}$ also necessary? Parameter counting suggests that this is indeed the case. Formally, Yun et al.12 provide an elegant proof for a (matching) lower bound:\n\nTheorem\n  Suppose a depth-$3$ ReLU network has widths of hidden layers $d_1, d_2$. If\n\n\\[2 d_1 d_2 + d_2 + 2 &lt; N,\\]\n\nthen there exists a dataset with $N$ points that cannot be memorized.\n\nProof\n  (Complete).\n\nObserve that this does not directly give lower bounds on the number of parameters needed to memorize data. We will come back to this question below.\n\nNext, extensions to other networks. The above results are for feedforward ReLU networks; Yun et al.12 also showed this for “hard-tanh” activations (which is a “clipped” ReLU activation that saturates at +1 for $x \\geq 1$). Similar results (with number of connections approximately equal to the number of data points) have been obtained for polynomial networks13 and residual networks14.\n\nWhat about more exotic families of networks? Could it be that there is some yet-undiscovered model that may give better memorization?\n\nIn a very nice (and surprisingly general) result, Vershynin15 showed the following:\n\nTheorem\n  (Informal) Consider well-separated data of unit norm and binary labels. Consider depth-$L$ networks ($L \\geq 3$) with arbitrary activations across neurons but without exponentially narrow bottlenecks. If the number of “wires” in the second layer of a network and later:\n\n\\[W := d_1 d_2 + d_2 d_3 + \\ldots + d_{L-1} d_L ,\\]\n\nis slightly larger than $N$, specifically:\n\n\\[W \\geq N \\log^5 N ,\\]\n\nthen some choice of weights can memorize this dataset.\n\nThe high level idea in the proof of this result is to use the first layer as a preconditioner that separates data points into an almost-orthogonal set (in fact, a simple random Gaussian projection layer will do), and then any sequence of final layers that will memorize label assignments.\n\nThe precise definitions of “well-separatedness” and “bottlenecks” can be found in the paper, but the key here is that this bound is independent of depth, choice of activations (whether ReLU or threshold or some mixture of both), and any other architectural details. Again, we see that there doesn’t seem to be a clear impact of the depth parameter $L$ on network capacity.\n\nFor a more precise discussion along these lines, see the review section of a very nice (and recent) paper by Rajput et al.16.\n\nDepth versus number of parameters\n\nIn the above discussion we saw that that moving from depth-2 to depth-3 networks helped significantly reduced the number of neurons needed to memorize $N$ arbitrary data points from $O(N)$ to $O(\\sqrt{N})$ in several cases. The number of parameters in all these constructions remained $O(N)$ (but no better).\n\nIs this the best possible we can do, or does depth help? We already encountered the result by Sontag17 showing an $\\Omega(N)$ lower bound; specifically, given any network with sub-linear ($o(N)$) parameters, there exists at least one “worst-case” dataset that cannot be memorized.\n\nMaybe our definition of memorization is too pessimistic, and we don’t have to fret about worst case behavior? Let us contrast Sontag’s result with other lower bounds from learning theory. Until now, we have quantified the memorization capacity of a network in terms of its ability to exactly interpolate any dataset. But we can weaken this definition a bit.\n\nThe VC-dimension of any family of models is defined as the maximum number of data points that the model can “shatter” (i.e., exactly interpolate labels). Notice that this is a “best-case” definition; if the VC dimension is $N$ there should exist at least one dataset of $N$ points (with arbitrary labels) that the network is able to memorize.\n\nExisting VC dimension bounds state that if a network architecture has $W$ weights then the VC dimension is no greater than $O(W^2)$18 no matter the depth. Therefore, in the “best-case” scenario, to memorize $N$ samples with arbitrary labels, we would require at least $\\Omega(\\sqrt{N})$ parameters, and we could not really hope to do any better19.\n\nCan we reconcile this gap between the “best” and “worst” cases of memorization capacity? In a very recent result, Vardi et al.5 have been able to show that the $\\sqrt{N}$ dependence is in fact tight (up to log factors). Under the assumption that the data is bounded norm and well-separated, then a width-12, depth-$\\tilde{O}(\\sqrt{N})$ network with $\\tilde{O}(\\sqrt{N})$ parameters can memorize any dataset. This result improves upon a previous result20 that had initially achieved a sub-linear upper bound of $O(N^{\\frac{2}{3}})$ parameters.\n\nThe proof of this result is somewhat combinatorial in nature. We see (again!) the bit-extractor gadget network being used here. There is another catch: the bit complexity of the network is very large (it scales as $\\sqrt{N}$), so the price to pay for a very small number of weights is that we end up stuffing many more bits in each weight.\n\nProof of 3-layer memorization capacity\n\n(COMPLETE)\n\n\n\n\n  \n    \n\n      Mikhail Belkin, Fit without Fear, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      V. Shankar, A. Fang, W. Guo, S. Fridovich-Keil, L. Schmidt, J. Ragan-Kelley, B. Recht, Neural Kernels without Tangents, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      M. Telgarsky, Benefits of depth in neural networks, 2016. &#x21a9;&#xfe0e;\n    \n    \n\n      R. Eldan and O. Shamir, The Power of Depth for Feedforward Neural Networks, 2016. &#x21a9;&#xfe0e;\n    \n    \n\n      G. Vardi, G. Yehudai, O. Shamir, On the Optimal Memorization Power of ReLU Neural Networks, 2022. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      A. Razborov and S. Rudich. Natural proofs, 1997. &#x21a9;&#xfe0e;\n    \n    \n\n      G. Vardi, D. Reichmann, T. Pitassi, and O. Shamir, Size and Depth Separation in Approximating Benign Functions with Neural Networks, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      O. Delalleau and Y. Bengio, Shallow vs. Deep Sum-Product Networks, 2011. &#x21a9;&#xfe0e;\n    \n    \n\n      Somewhat curiously, these kinds of oscillatory (“sinusoidal”/periodic) functions are common occurrences while proving cryptographic lower bounds for neural networks. See, for example, Song, Zadik, and Bruna, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      H. Siegelmann and D. Sontag, On the Computational Power of Neural Networks, 1992. &#x21a9;&#xfe0e;\n    \n    \n\n      B. Hanin and D. Rolnick, Complexity of Linear Regions in Deep Networks, 2019. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Yun, A. Jadbabaie, S. Sra, Small ReLU Networks are Powerful Memorizers, 2019. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2 &#x21a9;&#xfe0e;3 &#x21a9;&#xfe0e;4\n    \n    \n\n      R. Ge, R. Wang, H. Zhao, Mildly Overparametrized Neural Nets can Memorize Training Data Efficiently, 2019. &#x21a9;&#xfe0e;\n    \n    \n\n      M. Hardt and T. Ma, Identity Matters in Deep Learning, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      R. Vershynin, Memory capacity of neural networks with threshold and ReLU activations, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Rajput, K. Sreenivasan, D. Papailiopoulos, A. Karbasi, An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      E. Sontag, Shattering All Sets of k Points in “General Position” Requires (k − 1)/2 Parameters, 1997. &#x21a9;&#xfe0e;\n    \n    \n\n      P. Bartlett, V. Maiorov, R. Meir, Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks , 1998. &#x21a9;&#xfe0e;\n    \n    \n\n      Aside: this is not quite precise; a sharper VC dimension bound of $O(WL \\log W)$ can be obtained for depth-$L$ networks. See P. Bartlett, N. Harvey, C. Liaw, A. Mehrobian, Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks, 2019. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Park, J. Lee, C. Yun, J. Shin, Provable Memorization via Deep Neural Networks using Sublinear Parameters, 2021. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      7,
    title:   "Chapter 2 - Universal approximators",
    author:  null,
    layout:  "page",
    content: "Previously, we visited several results that showed how (shallow) neural networks can effectively memorize training data. However, memorization of a finite dataset may not the end goal1. In the ideal case, we would like to our network to simulate a (possibly complicated) prediction function that works well on most input data points. So a more pertinent question might be:\n\nCan neural networks simulate arbitrary functions?\n\n\nIn this note we will study the representation power of (shallow) neural networks through the lens of their ability to approximate (continuous) functions. This line of work has a long and rich history. The field of function approximation, independent of the context of neural networks, is a vast body of work which we can only barely touch upon. See here2 for a recent (and fantastic) survey.\n\nAs before, intuition tells us that an infinite number of neurons should be good enough to approximate pretty much anything. Therefore, our guiding principle will be to achieve as succinct a neural representation as possible. Moreover, if there is an efficient computational routine that gives this representation, that would be the icing on the cake.\n\nWarmup: Function approximation\n\n\n\nLet’s again start simple. This time, we don’t have any training data to work with; let’s just assume we seek some (purported) prediction function $g(x)$. To approximate $g$, we have a candidate hypothesis class $\\f_m$ of shallow (two-layer) neural networks of the form:\n\n\\[ f(x) = \\sum_{i=1}^m \\alpha_i \\psi(\\langle w_i, x \\rangle + b_i) . \\]\n\nOur goal is to get reasonable bounds on how large $m$ needs to be in terms of various parameters of $g$. We have to be clear about what “approximate” means here. It is typical to measure approximation in terms of $p$-norms between measurable functions; for example, in the case of $L_2$-norms we will try to control\n\n\\[\\int_{\\text{dom}(g)} |f -g|^2 d \\mu\\]\n\nwhere $\\mu$ is some measure defined over $\\text{dom}(g)$. Likewise for the $L_\\infty$- (or the sup-)norm, and so on.\n\nUnivariate functions\n\nWe begin with the special case of $d=1$ (i.e., the prediction function $g$ is univariate). Let us first define a useful property to characterize univariate functions.\n\nDefinition\n  (Univariate Lipschitz.) A function $g : \\R \\rightarrow \\R$ is $L$-Lipschitz if for all $u,v \\in \\R$, we have that $|f(u) - f(v) | \\leq L |u - v|$.\n\nWhy is this an interesting property? Any smooth function with bounded derivative is Lipschitz; in fact, certain non-smooth functions (such as the ReLU) are also Lipschitz. Lipschitz-ness does not quite capture everything we care about (e.g. discontinuous functions are not Lipschitz, which can be somewhat problematic if there are “jumps” in the label space). But it serves as a large enough class of functions to prove interesting results.\n\nAn additional benefit of Lipschitzness is due to approximability. If our target function $f$ is $L$-Lipschitz with reasonable $L$, then we can show that it can be well-approximated by a two-layer network with threshold activations: $\\psi(z) = \\mathbb{I}(z \\geq 0)$. We prove:\n\nTheorem\n  Let $g : [0,1] \\rightarrow \\R$ be $L$-Lipschitz. Then, it can be  $\\varepsilon$-approximated in the sup-norm by a two-layer network with $O(\\frac{L}{\\varepsilon})$ hidden threshold neurons.\n\nProof.\n  A more careful derivation of this fact (and the next one below) can be found in Telgarsky3. The proof follows from the same picture we might have seen while first learning about integrals and Riemann sums. The high level idea is to tile the interval $[0,1]$ using “buildings” of appropriate height. Since the derivatives are bounded (due to Lipschitzness), the top of each “building” cannot be too far away from the corresponding function value. Here is a picture:\n\n\n\nMore formally: partition $[0,1]$ into equal intervals of size $\\varepsilon/L$. Let the $i$-th interval be $[u_i,u_{i+1})$. Define a sequence of functions $f_i(x)$ where each $f_i$ is zero everywhere, except within the $i$-th interval where it attains the value $g(u_i)$. Then $f_i$ can be written down as the difference of two threshold functions:\n\n\\[f_i(x) = g(u_i) \\left(\\psi(x - u_i) - \\psi(x - u_{i+1})\\right).\\]\n\nOur network will be the sum of all the $f_i$’s (and there are $L/\\varepsilon$ of them). Moreover, for any $x \\in [0,1]$, if $u_i$ is the left end of the interval corresponding to $x$, then we have:\n\n\\[\\begin{aligned}\n  |f(x) - g(x)| &amp;= |g(x) - g(u_i)| \\\\\n  &amp;\\leq L |x - u_i | \\qquad \\text{(Lipschitzness)} \\\\\n  &amp;\\leq L \\frac{\\varepsilon}{L} = \\varepsilon,\n  \\end{aligned}\\]\n\nTaking the supremum over all $x \\in [0,1]$ completes the proof.\n\nRemark\nSo we can approximate $L$-Lipschitz functions with $O(L/\\varepsilon)$ threshold neurons. Would the answer change if we used ReLU activations? (Hint: no, up to constants; prove this.)\n\nMultivariate functions\n\nOf course, in deep learning we rarely care about univariate functions (i.e., where the input is 1-dimensional). We can ask a similar question in the more general case. Suppose we have $L$-Lipschitz functions over $d$ input variables and we want to approximate it using shallow neural networks. How many neurons do we need?\n\nWe answer this question using two approaches. First, we give a construction using standard real analysis that uses two hidden layers of neurons. Then, with some more mathematical powerful machinery we will get better (and much more general) results with only one hidden layer (i.e., using the hypothesis class $\\f$).\n\nFirst, we have to define Lipschitzness for $d$-variate functions.\n\nDefinition\n  (Multivariate Lipschitz.) A function $g : \\R^d \\rightarrow \\R$ is $L$-Lipschitz if for all $u,v \\in \\R^d$, we have that $|f(u) - f(v) | \\leq L \\lVert u - v \\rVert_\\infty$.\n\nTheorem\n  Let $g : [0,1]^d \\rightarrow \\R$ be $L$-Lipschitz. Then, $g$ can be  $\\varepsilon$-approximated in the $L_1$-norm by a three-layer network $f$ with $O(\\frac{L}{\\varepsilon^d})$ hidden threshold neurons.\n\nProof sketch.\n  The proof follows the above construction for univariate functions. We will tile $[0,1]^d$ with equally spaced multidimensional rectangles; there are $O(\\frac{1}{\\varepsilon^d})$ of them. The value of the function $f$ within each rectangle will be held constant (and due to the definition of Lipschitzness, the error with respect to $g$ cannot be too large). If we can figure out how to approximate $g$ within each rectangle, then we are done.\n\nThe key idea is to figure out how to realize “indicator functions” for every rectangle. We have seen that in the univariate case, indicators can be implemented using the difference of two threshold neurons. In the $d$-variate case, an indicator over a rectangle is the Cartesian product over the $d$ axis. however, Boolean/Cartesian products can be implemented by a layer of threshold activations on top of these differences.\n\nFormally, consider any arbitrary piece with $[u_j,v_j), j=1,2,\\ldots,d$ as sides. The domain can be written as the Cartesian product:\n\n\\[S = \\times_{j=1}^d [u_j, v_j).\\]\n\nTherefore, we can realize an indicator function over this domain as follows. Localize within each coordinate by the “difference-of-threshold neurons”:\n\n\\[h_j(z) = \\psi(z-v_j) - \\psi(z - u_j)\\]\n\nand implement the entire rectangle is implemented via a “Boolean AND” over all the coordinates:\n\n\\[h(x) = \\psi(\\sum_{j=1}^d h_j(x_j) - (d-1)),\\]\n\nwhere $x_j$ is the $j$-th coordinate of $x$. There is one such $h$ for every rectangle, and the output edge from this neuron is assigned a constant value approximating $g$ within that rectangle. This completes the proof.\n\nRemark\nWould the answer change if we used ReLU activations? (Hint: no, up to constants; prove this.)\n\nBefore proceeding, let’s just reflect on the bound (and the nature of the network) that we constructed in the proof. Each neuron in the first layer looks at the right “interval” independently each input coordinate; there are $d$ such coordinates, and therefore $O(\\frac{dL}{\\varepsilon})$ intervals.\n\nThe second layer is where the real difficulty lies. Each neuron picks exactly the right set of intervals to define a unique hyper-rectangle. There are $O(\\frac{1}{\\varepsilon^d})$ such rectangles. Therefore, the last layer becomes very, very wide with increasing $d$. This is unfortunate, since we desire succinct representations.\n\nSo the next natural question is: can we get better upper bounds? Also, do we really need two hidden layers (or is the hypothesis class $\\f_m$ good enough for sufficiently large $m$)?\n\nThe answer to both questions is a (qualified) yes, but first we need to gather a few more tools.\n\nUniversal approximators\n\nThe idea of defining succinct hypothesis classes to approximate functions had been well studied well before neural networks were introduced. In fact, we can go all the way back to:\n\nTheorem\n  (Weierstrass, 1865.) Let $g : [0,1] \\rightarrow \\R$ be any continuous function. Then, $g$ can be  $\\varepsilon$-approximated in the sup-norm by some polynomial of sufficiently high degree.\n\nWeierstrass proved this via an interesting trick: he took the function $g$, convolved this with a Gaussian (which made everything smooth/analytic) and then did a Taylor series. Curiously, we will return this property much later when we study adversarial robustness of neural networks.\n\nIn fact, there is a more direct constructive proof of this result by Bernstein4; we won’t go over it but see, for example here. The key idea is to construct a sufficiently large set of interpolating basis functions (in Bernstein’s case, his eponymous polynomials), whose combinations densely span the entire space of continuous functions.\n\nOther than polynomials, what other families of “basis” functions lead to successful approximation? To answer this, we first define the concept of a universal approximator.\n\nDefinition\n  Let $\\f$ be a given hypothesis class. Then, $\\f$ is a universal approximator over some domain $S$ if for every continuous function $g : S \\rightarrow \\R$ and approximation parameter $\\varepsilon &gt; 0$, there exists $f \\in \\f$ such that:\n  \\(\\sup_{x \\in S} |f(x) - g(x) | \\leq \\varepsilon .\\)\n\nThe Weierstrass theorem showed that that the set of all polynomials is a universal approximator. In fact, a generalization of this theorem shows that other families of functions that behave like polynomials are also universal approximators. This is called the Stone-Weierstrass theorem, stated as follows.\n\nTheorem\n  (Stone-Weierstrass, 1948.) If the following hold:\n\n\n  (Continuity) Every $f \\in \\f$ is continuous.\n  (Identity) $\\forall~x$, there exists $f \\in \\f$ s.t. $f(x) \\neq 0$.\n  (Separation) $\\forall~x, x’,~x\\neq x’,$ there exists $f \\in \\f$ s.t. $f(x) \\neq f(x’)$.\n  (Closure) $\\f$ is closed under additions and multiplications.\n\n\nthen $\\f$ is a universal approximator.\n\nWe will use this property to show that (in very general situations), several families of neural networks are universal approximators. To be precise, let $f(x)$ be a single neuron:\n\n\\[f_{\\alpha,w,b} : x \\mapsto \\alpha \\psi(\\langle w, x \\rangle + b)\\]\n\nand define\n\n\\[\\f = \\text{span}_{\\alpha,w,b} \\lbrace f_{\\alpha,w,b} \\rbrace\\]\n\nas the space of all possible single-hidden-layer networks with activation $\\psi$. We prove the following several results, and follow these with several remarks.\n\nTheorem\n  If we use the cosine activation $\\psi(\\cdot) = \\cos(\\cdot)$, then $\\f$ is a universal approximator.\n\nProof\n  This result is the OG “universal approximation theorem” and can be attributed to Hornik, Stinchcombe, and White5. Contemporary results of basically the same flavor are due to Cybenko6 and Funahashi7 but using techniques other than Stone-Weierstrass.\n\nAll we need to do is to show that the space of (possibly unbounded width) single-hidden-layer networks satisfies the four conditions of Stone-Weierstrass.\n\n  (Continuity) Obvious. Check.\n  (Identity) For every $x$, $\\cos(\\langle 0, x \\rangle) = \\cos(0) = 1 \\neq 0$. Check.\n  (Separation) For every $x \\neq x’$, $f(z) = \\cos\\left(\\frac{1}{\\lVert x-x’ \\rVert_2^2}\\langle x-x’, z-x’ \\rangle \\right)$ separates $x,x’$. Check.\n  (Closure) This is the most crucial one. Closure under additions is trivial (just add more hidden units!) Closure under multiplications is due to trigonometry: we know that $\\cos(\\langle u, x \\rangle) \\cos(\\langle v, x \\rangle) = \\frac{1}{2} (\\cos(\\langle u+v, x \\rangle) + \\cos(\\langle u-v, x \\rangle))$. Therefore, products of two $\\cos$ neurons can be equivalently expressed by the sum of two (other) $\\cos$ neurons. Check.\n  This completes the proof.\n\n\nTheorem\n  If we use the exponential activation $\\psi(\\cdot) = \\exp(\\cdot)$, then $ \\f $ is a universal approximator.\n\nProof\n  Even easier than $\\cos$. (COMPLETE)\n\nThe OG paper by Hornik et al5 showed a more general result for sigmoidal activations.  Here a sigmoidal activation is any function $\\psi$ such that $\\lim_{z \\rightarrow -\\infty} = 0$ and $\\lim_{z \\rightarrow +\\infty} = 1$. This result covers “threshold” activations, hard/soft tanh, other regular sigmoids, etc.\n\nTheorem\n  If we use any sigmoidal activation $\\psi(\\cdot)$ that is continuous, then $\\f$ is a universal approximator.\n\nProof\n  (COMPLETE)\n\nRemark\n  Corollary: can you show that if sigmoids work, then ReLUs also work?\n\nRemark\n  The use of cosine activations is not standard in deep learning, although they have found use in some fantastic new applications in the context of solving partial differential equations8. Later we will explore other (theoretical) applications of cosines.\n\nRemark\n  Notice here that these results are silent on how large $m$ needs to be in terms of $\\varepsilon$. If we unpack terms carefully, we again see a scaling of $m$ with $O(\\frac{1}{\\varepsilon^d})$, similar to what we had before. (This property arises to due to the last property in Stone-Weierstass, i.e., closure under products.) The curse of dimensionality strikes yet again.\n\nRemark\n  Somewhat curiously, if we use $\\psi(\\cdot)$ to be a polynomial activation function (of a fixed-degree), then $\\f$ is not a universal approximator. Can you see why this is the case? (Hint: which property of Stone-Weierstrass is violated?)\n  In fact, polynomial activations are the only ones which don’t work! $\\f$ is a universal approximator iff $\\psi$ is non-polynomial; see Leshno et al. (1993)9 for a proof.\n\nBarron’s method\n\nUniversal approximation results of the form discussed above are interesting but, at the end, not very satisfactory. Recall that we wanted to know if our prediction function can be simulated via a succinct neural network. However, we could only muster a bound of $O(\\frac{1}{\\varepsilon^d})$.\n\nCan we do better than this? Maybe our original approach (trying to approximate all $L$-Lipschitz functions) was a bit too ambitious. Perhaps we want to narrow our focus down to a smaller target class (that are still rich enough to capture interesting function behavior). In any case, can we get dimension-independent bounds on the number of neurons needed to approximate target functions?\n\nIn a seminal paper10, Barron identified an interesting class of functions that can be indeed well-approximated with small (still shallow) neural networks. Again, we have to pick up a few extra tools to establish this, so we will first state the main result, and then break down the proof.\n\nTheorem\n  Suppose $g : \\R^d \\rightarrow \\R$ is in $L_1$. Then, there exists a one-hidden-layer neural network $f$ with sigmoidal activations and $m$ hidden neurons such that:\n  \\(\\int | f(x) - g(x) |^2 dx \\leq \\varepsilon\\)\n  where:\n  \\(m = \\frac{C_g^2}{\\varepsilon^2} .\\)\n  Here,\n  \\(C_g = \\lVert \\widehat{\\nabla g} \\rVert_1 = \\int \\lVert \\widehat{\\nabla g} \\rVert d\\omega\\)\n  is the $L_1$-norm of the Fourier transform of the gradient of $g$, and is called the Barron norm of $g$:\n\nWe will outline the proof of this theorem below, but first some reflections. Notice now that $m$ does not explicitly depend on $d$; therefore, we escape the dreaded curse of dimensionality. As long as we control the Barron norm of $g$ to be something reasonable, we can succinctly approximate it using shallow networks.\n\nIn his paper10, Barron shows that indeed Barron norms can be small for a large number of interesting target function classes – polynomials, sufficiently smooth functions, families such as Gaussian mixture models, even functions over discrete domains (such as decision trees).\n\nSecond, the bound is an “existence”-style result. Somewhat interestingly, the proof will also reveal a constructive (although unfortunately not very computationally friendly) approach to finding $f$. We will discuss this at the very end.\n\nThird, notice that the approximation error is measured in terms of the $L_2$ (squared difference) norm. This is due to the tools used in the proof; I’m not sure if there exist results for other norms (such as $L_\\infty$).\n\nLastly, other Barron style bounds assuring “dimension-free” convergence of representation error exist, using similar analysis techniques. See Jones11, Girosi12, and these lecture notes by Recht13.\n\n\n\nLet’s now give a proof sketch of Barron’s Theorem. We will be somewhat handwavy, focusing on intuition and being sloppy with derivations; for a more careful treatment, see Telgarsky’s notes3. The proof follows from two observations:\n\n\n  \n    Write out the function $g$ exactly in terms of the Fourier basis functions (with possibly infinitely many coefficients), and map this to an infinitely-wide neural network.\n  \n  \n    Using Maurey’s empirical method (also sometimes called the “probabilistic method”), show that one can sample from an appropriate distribution defined on the basis functions, and get a succinct (but good enough) approximation of $g$. Specifically, to get $\\varepsilon$-accurate approximation, we need $m = O(\\frac{1}{\\varepsilon^2})$ samples.\n  \n\n\nProof Part 1: Fourier decomposition\n\n** COMPLETE **.\n\nProof part 2: The empirical method of Maurey\n\n** COMPLETE **.\n\nEpilogue: A constructive approximation via Frank-Wolfe\n\n** COMPLETE **.\n\n\n\nFootnotes and references\n\n\n  \n    \n\n      Although: exactly interpolating training labels seems standard in modern deep networks; see here and Fig 1a  of this paper. &#x21a9;&#xfe0e;\n    \n    \n\n      R. DeVore, B. Hanin, G. Petrova, Neural network approximation, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      M. Telgarsky, Deep Learning Theory, 2021. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      Bernstein polynomials have several other practical use cases, including in computer graphics (see Bezier curves). &#x21a9;&#xfe0e;\n    \n    \n\n      K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators, 1989. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      G. Cybenko, Approximation by superpositions of a sigmoidal function, 1989. &#x21a9;&#xfe0e;\n    \n    \n\n      K. Funahashi, On the approximate realization of continuous mappings by neural networks, 1989. &#x21a9;&#xfe0e;\n    \n    \n\n      V. Sitzmann, J. Martell, A. Bregman, D. Lindell, G. Wetzstein, Implicit Neural Representations with Periodic Activation Functions, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      M. Leshno, V. Lin, A. Pinkus, S. Schocken, Multilayer feedforward networks with a nonpolynomial activation function can approximate any function, 1993. &#x21a9;&#xfe0e;\n    \n    \n\n      A. Barron, Universal Approximation Bounds for Superpositions of a Sigmoidal Function, 1993. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      L. Jones, A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates for Projection Pursuit Regression and Neural Network Training, 1992. &#x21a9;&#xfe0e;\n    \n    \n\n      F. Girosi, Regularization Theory, Radial Basis Functions and Networks, 1994. &#x21a9;&#xfe0e;\n    \n    \n\n      B. Recht, Approximation theory, 2008. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      8,
    title:   "Chapter 1 - Memorization",
    author:  null,
    layout:  "page",
    content: "Let us begin by trying to rigorously answer a very simple question:\n\nHow many neurons suffice?\n\n\nUpon reflection it should clear that this question isn’t well-posed. Suffice for what? For good performance? On what task? Do we even know if this answer is well-defined – perhaps it depends on some hard-to-estimate quantity related to the learning problem? Even if we were able to get a handle on this quantity, does it matter how the neurons are connected – should the network be wide/shallow, or narrow/deep?\n\nLet us begin simple. Suppose all we have is a bunch of training data points:\n\\[ X = \\lbrace (x_i, y_i)\\rbrace_{i=1}^n \\subset \\mathbb{R}^d \\times \\mathbb{R} \\]\nand our goal will be to discover a network that exactly memorizes $X$. That is, we will learn a function $f$ that, when evaluated on every data point $x_i$ in the training data, returns $y_i$. Equivalently, if we define empirical risk via the squared error loss:\n\n\\[\\begin{aligned}\n\\hat{y} &amp;= f(x), \\\\\nl(y,\\hat{y}) &amp;= 0.5(y - \\hat{y})^2, \\\\\nR(f) &amp;= \\sum_i \\frac{1}{n} l(y_i, \\hat{y_i}),\n\\end{aligned}\\]\n\nthen $f$ achieves zero empirical risk. Our intuition says (and we will prove more versions of this later) is that a very large (very wide, or very deep, or both) network is likely enough to fit basically anything we like. So really, we want reasonable upper bounds on the number of neurons needed for exact memorization.\n\nBut why should we care about memorization anyway? After all, machine learning folks are taught to be wary of overfitting to the training set. In introductory ML courses we spend several hours (and homework sets) covering the bias-variance tradeoff, the importance of adding a regularizer to decrease variance (at the expense of incurring extra “bias”), etc etc.\n\nUnfortunately, deep learning practice throws this classical way of ML thinking out of the window. We seldom use explicit regularizers, instead relying on standard losses. We typically train deep neural networks to achieve 100\\% (train) accuracy. Later, we will try to understand why networks trained to perfectly interpolate the training data still generalize well, but for now let’s focus on just achieving a representation that enables perfect memorization.\n\nBasics\n\nFirst, a basic definition of a “neural network”.\n\nFor our purposes, neural network is composed of several primitive “units”, each of which we will call a neuron. Given an input vector $x \\in \\mathbb{R}^d$, a neuron transforms the input according to the following functional form:\n\n\\[z = \\psi(\\sum_{j=1}^d w_j x_j + b) .\\]\n\nHere, $w_j$ are the weights, $b$ is a scalar called the bias, and $\\psi$ is a nonlinear scalar transformation called the activation function; a typical activation function is the “ReLU function” $\\psi(z) = \\max(0,z)$ but we will also consider others.\n\n\n\nA neural network is a feedforward composition of several neurons, typically arranged in the form of layers. So if we imagine several neurons participating at the $l^{\\text{th}}$ layer, we can stack up their weights (row-wise) in the form of a weight matrix $W^{(l)}$. The output of neurons forming each layer forms the corresponding input to all of the neurons in the next layer. So a neural network with two layers of neurons would have\n\n\\[\\begin{aligned}\nz_{1} &amp;= \\psi(W_{1} x + b_{1}), \\\\\nz_{2} &amp;= \\psi(W_{2} z_{1} + b_{2}), \\\\\ny &amp;= W_{3} z_{2} + b_{3} .\n\\end{aligned}\\]\n\nAnalogously, one can extend this definition to $L$ layers for any $L \\geq 1$. The nomenclature is a bit funny sometimes. The above example is either called a “3-layer network” or “2-hidden-layer network”, depending on who you ask. The output $y$ is considered as its own layer and not considered as “hidden” (also notice that it doesn’t have any activation in this case; that’s typical.)\n\nMemorization capacity: Standard results\n\n\n\nA lot of interesting quirks arise even in the simplest cases.\n\nLet us focus our attention on the ability of two-layer networks (or one-hidden-layer networks) to memorize $n$ data points. We will restrict discussion to ReLU activations but the arguments below are generally applicable. If there are $m$ hidden neurons and if $\\psi$ is the ReLU then our hypothesis class $\\f_m$ comprises all functions $f$ such that for suitable weight parameters $(\\alpha_i, w_i)$ and bias parameters $b_i$, we have:\n\n\\[ f(x) = \\sum_{i=1}^m \\alpha_i \\psi(\\langle w_i, x \\rangle + b_i) . \\]\n\nIntuitively, $m &lt; \\infty$ is a trivial upper bound on any dataset (we will be more rigorous about this when we prove universal approximation results). If we have infinitely many parameters then memorization should be trivial. Let us get a better upper bound for $m$. Our first result shows that $m = n$ should suffice.\n\nTheorem\n  Let $f$ be a two-layer ReLU network with $m = n$ hidden neurons. For any arbitrary dataset $X = \\lbrace (x_i, y_i)_{i=1}^n\\rbrace \\subset \\mathbb{R}^d \\times \\mathbb{R}$ where $x_i$ are in general position, the weights and biases of $f$ can be chosen such that $f$ exactly interpolates $X$.\n\nProof\n  This result is non-constructive and seems to be folklore, dating back to at least Baum1. For modern versions of this proof, see Bach2 or Bubeck et al.3.\n\nDefine the space of arbitrary width two-layer networks:\n  \\[\\f = \\bigcup_{m \\geq 0} \\f_m . \\]\n  The high level idea is that $\\f$ forms a vector space. This is easy to see, since it is closed under additions and scalar multiplications. Formally, fix $x$ and consider the element $\\psi_{w,b}: x \\mapsto \\psi(\\langle w, x \\rangle + b)$. Then, $\\text{span}(\\psi_{w,b})$ forms a vector space.  Now, consider the linear pointwise evaluation operator $\\Psi : V \\rightarrow \\mathbb{R}^n$:\n  \\[\\Psi(f) = (f(x_1), f(x_2), \\ldots, f(x_n)) .\\]\n  We know from classical universal approximation (Chapter 2) that every vector in $\\mathbb{R}^n$ can be written as some (possibly infinite)  combination of neurons. Therefore, $\\text{Range}(\\Psi) = \\mathbb{R}^n$, or $\\text{dim(Range}(\\Psi)) = n$. Therefore, there exists some basis of size $n$ with the same span! Call this basis $\\lbrace \\psi_1, \\ldots,\\psi_n\\rbrace$. This basis can be used to express any set of labels by choosing appropriate coefficients in a standard basis representation $y = \\sum_{i=1}^n \\alpha_i \\psi_i$.\n  The result follows.\n\nIn fact, the above result holds for any activation function $\\psi$ that is not a polynomial4; we will revisit this curious property later.\n\nReally, we didn’t do much here. Since the “information content” in $n$ labels has dimension $n$, we can extract any arbitrary basis (written in the form of neurons) and write down the expansion of the labels in terms of this basis. Since this approach may be a bit abstract, let’s give an alternate constructive proof.\n\nProof (Alternate.)\n  This proof can be attributed to Zhang et al5. Suppose $m = n.$ Since all $x_i$’s are distinct and in general position, we can pick a $w$ such that if define $z_i := \\langle w, x_i \\rangle$ then without loss of generality (or by re-indexing the data points):\n  \\[ z_1 &lt; z_2 &lt; \\ldots z_n . \\]\n  One way to pick $w$ is by random projection: pick $w$ from a standard $d$-variate Gaussian distribution; then the above holds with high probability. If the above relation between $z_i$ holds, we can find some sequence of $b_i$ such that:\n  \\[ b_1 &lt; z_1 &lt; b_2 &lt; z_2 &lt; \\ldots &lt; b_n &lt; z_n . \\]\n  Now, let’s define an $n \\times n$ matrix $A$ such that\n  \\[ A_{ij} := \\text{ReLU}(z_i - b_j) = \\max(z_i - b_j, 0) . \\]\n  Since by definition, each $z_i$ is only bigger than all $b_j$ for $1 \\leq j \\leq i$, we have that $A$ is a lower triangular matrix with positive entries on the diagonal, and therefore full rank. Moreover, for any $\\alpha \\in \\mathbb{R}^n$, the product $A \\alpha$ is the superposition of exactly $n$ ReLU neurons (the weights are the same for all of them, but the biases are distinct). Set $\\alpha = A^{-1} y$ and we are done.\n\nRemark\nThe above proofs used biases, but if we restrict our attention to bias-free networks, that’s fine too, we just need to use different weights for the $n$ hidden neurons. Such a network is called a random feature model; see here and here.\n\nMemorization capacity: Improved results\n\nThe above result shows that $m = n$ neurons are sufficient to memorize pretty much any dataset. Can we get away with fewer neurons? Notice that really the network has to “remember” only the $n$ labels; but since there are $n$ neurons, each with $d$ input edges, the number of parameters is $nd$.  (Note: not technically correct; the second proof only uses $n$ distinct weights and $n$ biases.) It turns out that we can indeed do better.\n\nTheorem\n  For any dataset of $n$ points in general position, $m = 4 \\lceil \\frac{n}{d} \\rceil$ neurons suffice to memorize it.\n\nProof\n  For ReLU activations, this proof is in Bubeck et al3, although Baum1 proved a similar result for threshold activations and binary labels.\n\nNotice that $d = \\Omega(1)$ is actually beneficial here; higher input dimension implies easier memorization. In some ways this is a blessing of dimensionality phenomenon.\n\nWe will prove Baum’s result first. Suppose we have threshold activations (i.e., $\\psi(z) = \\mathbb{I}(z \\geq 0)$) and binary labels $y_i \\in \\pm {1}$. We iteratively build a network as follows. Without loss of generality, we can assume that there are at least $d$ points with label equal to $y_i = 1$; index them as $x_1, x_2, \\ldots, x_d$. Since the points are in general position, we can find an affine subspace that exactly interpolates these points:\n\n\\[\\langle w_1, x_i \\rangle = b_1, \\quad i \\in [d]\\]\n\nand record $(w_1, b_1)$. (Importantly, again since the points are in general position no other points lie in this subspace.)\n\n\n\nNow, form a very thin indicator slab using for this affine subspace using exactly two neurons:\n\n\\[x \\mapsto \\psi(\\langle w_1,x \\rangle - (b_1-\\varepsilon)) - \\psi(\\langle w_1,x \\rangle - (b_1+\\varepsilon))\\]\n\nfor some small enough $\\varepsilon &gt; 0$. This function is equal to 1 for exactly the points in the subspace, and zero for all other points. For this group of $d$ points we can assign the output weight $\\alpha_1 = 1$. Iterate this argument $\\lceil \\frac{n}{d} \\rceil$ times and we are done! Therefore, $2 \\lceil \\frac{n}{d} \\rceil$ threshold neurons suffice if the labels are binary.\n\nThe exact same argument can be extended to ReLU activations and arbitrary (scalar) labels. Again, we iteratively build the network. We pick an arbitrary set of $d$ points, through which we can interpolate an affine subspace:\n\n\\[\\langle u, x_i \\rangle = b, \\quad i \\in [d] .\\]\n\nWe now show that we can memorize these $d$ data points using 4 ReLU neurons. The trick is to look at the “directional derivative” of the ReLU:\n\n\\[g: x \\mapsto \\frac{\\psi(\\langle u + \\delta v, x \\rangle - b) - \\psi(\\langle u, v \\rangle - b)}{\\delta} .\\]\n\nAs $\\delta \\rightarrow 0$, the right hand side approaches the quantity:\n\n\\[g: x \\mapsto \\psi'(\\langle u, x \\rangle - b) \\langle v, x \\rangle .\\]\n\nBut: the first part is the “derivative” of the ReLU, which is exactly the threshold function! Using the thin-slab-indicator trick in the proof above, the difference of two such functions (with slightly different $b$) forms an indicator on a thin slab around these $d$ points:\n\n\\[f = g_{u,v,b-\\varepsilon} - g_{u,v,b+\\varepsilon} .\\]\n\nSince we are using differences-of-differences, we need 4 ReLUs to realize $f$. It now remains to pick $v$. But this is easy: since the data are in general position, just solve for $v$ such that\n\n\\[\\langle v, x_i \\rangle = y_i .\\]\n\nRepeat this fitting procedure $\\lceil \\frac{n}{d} \\rceil$ times and we are done.\n\nRemark\nThe above construction is somewhat wacky/combinatorial. The weights of each neuron was picked myopically (we never revisited data points) and locally (each neuron only depended on a small subset of data points).\n\nRemark\nThe above construction says very little about how large networks need to be in order for “typical” learning algorithms (such as SGD) to succeed. We will revisit this in the Optimization chapters. For a recent result exploring the properties of “typical” gradient-based learning methods in the $O(n/d)$ regime, see here6.\n\nRemark\nAll the above results used a standard dense feedforward architecture. Analogous memorization results have been shown for other architectures commonly used in practice today: convnets7, ResNets8, transformers9, etc.\n\nLower bounds\n\nThe above results show that we can memorize $n$ (scalar) labels with no more than $O(n/d)$ ReLU neurons – each with $d$ incoming edges, which means that the number of learnable weights in this network is $O(n)$. Is this upper bound tight, or is there hope for doing any better?\n\nThe answer seems to be no, and intuitively it makes sense from a parameter counting perspective. Sontag10 proved an early result along these lines, showing that if some function $f$ that is described with $o(n)$ parameters is analytic (meaning that it is smooth and has convergent power series) and definable (meaning that it can be expressed by some arbitrary composition of rational operations and exponents), then there exists at least one dataset of $n$ points that the network cannot memorize. This result also holds for functions that are piecewise analytic and definable, meaning that neural networks (of arbitrary depth! not just two layers!) are applicable to this theorem.\n\n(Note: this observation is not technically correct; we can get better by “bit stuffing”. If we assume slightly more restrictive properties on the data and allow the network weights to be unbounded, then this bound can be improved to $O(\\sqrt{n})$ parameters. We will revisit this later.)\n\nSo it may seem interesting that we were able to get best-possible memorization capacity using simple 2-layer networks. So does additional depth buy us anything at all? The answer for this is yes: we can decrease the number of hidden neurons in the network significantly when we move from 1- to 2-hidden-layer networks. We will revisit this in Chapter 3.\n\nRobust interpolation\n\n(Complete)\n\n\n\n\n  \n    \n\n      E. Baum, On the capabilities of multilayer perceptrons, 1989. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      F. Bach, Breaking the Curse of Dimensionality with Convex Neural Networks, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      S. Bubeck, R. Eldan, Y. Lee, D. Mikulincer, Network size and weights size for memorization with two-layers neural networks, 2020. &#x21a9;&#xfe0e; &#x21a9;&#xfe0e;2\n    \n    \n\n      M. Leshno, V. Lin, A. Pinkus, S. Schocken, Multilayer feedforward networks with a nonpolynomial activation function can approximate any function, 1993. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep learning requires rethinking generalization, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      J. Zhang, Y. Zhang, M. Hong, R. Sun, Z.-Q. Luo, When Expressivity Meets Trainability: Fewer than $n$ Neurons Can Work, 2021. &#x21a9;&#xfe0e;\n    \n    \n\n      Q. Nguyen and M. Hein, Optimization Landscape and Expressivity of Deep CNNs, 2018. &#x21a9;&#xfe0e;\n    \n    \n\n      M. Hardt and T. Ma, Identity Matters in Deep Learning, 2017. &#x21a9;&#xfe0e;\n    \n    \n\n      C. Yun, Y. Chang, S. Bhojanapalli, A. Rawat, S. Reddi, S. Kumar, $O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers, 2020. &#x21a9;&#xfe0e;\n    \n    \n\n      E. Sontag, Shattering All Sets of k Points in “General Position” Requires (k − 1)/2 Parameters, 1997. &#x21a9;&#xfe0e;\n    \n  \n\n",
});
index.addDoc({
    id:      9,
    title:   "Introduction",
    author:  null,
    layout:  "page",
    content: "Our goal is to reason about (deep) neural networks from the lens of theory.\n\nUnlike many other scientific fields, there currently exists a wide gap between:\n\n  what the best available tools in theoretical computer science can tell us about modern neural networks, and\n  the actual ways in which modern neural network models work in practice.\n\n\nThis gap between theory and practice is, in my view, unsatisfactory. Yann LeCun invokes the “streetlight effect”: we search for lost keys where we can, not where they really lie.\n\nBut here is why (I think) theory matters: by asking carefully crafted (but precise) questions about deep networks, one can precisely answer why certain aspects of neural networks work (or don’t), and what is in the realm of the possible (versus what isn’t). A major motivation behind these course notes is to identify the landscape of how wide exactly these gaps are at present, and how to bridge them.\n\nThese notes are by no means the first attempt to do so. Other excellent courses/lecture notes/surveys include:\n\n\n  Fit without Fear by Misha Belkin (UCSD).\n  Mathematics of Deep Learning by Joan Bruna (NYU).\n  Deep Learning Theory by Matus Telgarsky (UIUC).\n  Expository videos by Sebastian Bubeck (Microsoft).\n\n\nSetup\n\nAs typical in (supervised) machine learning, our starting point is a list of $n$ example data-label pairs:\n\\[ X = {(x_i, y_i)}_{i=1}^n \\subset \\mathbb{R}^d \\times \\mathbb{R} \\]\nwhich we will call the training set. This dataset is assumed to acquired via iid sampling with respect to some underlying probability measure $\\mu$ defined over $\\mathbb{R}^d \\times \\mathbb{R}$.\n\nOur goal will be to predict the label $y \\in \\mathbb{R}$ associated with some (hitherto unseen) data point $x \\in \\mathbb{R}^d$. In order to do so, we will seek a prediction function $f$ that can be expressed as a neural network. Later we will more precisely define neural networks, but the familiar picture is appropriate for now. The key point is that we want an $f$ that performs “well” on “most” input data points.\n\nLet us first agree to measure “goodness” of performance via a two-argument loss function\n\n\\[l(\\cdot,\\cdot) : \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}_{\\geq 0}\\]\n\nwhich takes in a predicted label, compares with the truth, and spits out a non-negative cost of fit (smaller is better). Then, quantitatively, our prediction function should be such that the population risk:\n\n\\[R(f) = \\mathbb{E}_{(x,y) \\sim \\mu} l(y, f(x))\\]\n\nis small.\n\nThis immediately poses a Major Challenge, since the population risk $R(f)$ is not an easy object to study. For starters, the probability measure $\\mu$ may not be be known. Even if magically $\\mu$ is available, calculating the expected value with respect to $\\mu$ can be difficult. However, we do have training data lying around. Therefore, instead of directly dealing with $R(f)$, we will instead use a proxy quantity called the empirical risk:\n\n\\[\\hat{R}(f) = \\frac{1}{n} \\sum_{i=1}^n l(y_i, f(x_i))\\]\n\nand seek an $f$ that makes this small.\n\n\n\nNow, reducing the empirical risk $\\hat{R}(f)$ to as small as possible is akin to function optimization. To make this numerically tractable, we will first cook up a hypothesis class $\\f$. In deep learning, this can be thought of as the set of all neural network models that obey a certain architecture1.\n\nBut: this approach now poses another Major Challenge. What’s a good family of architectures? How do we know whether a certain architecture is rich enough to solve our prediction problem? Can it go the other way (i.e., could we somehow pick a network architecture that is far too rich for our purposes?)\n\nLet us set aside such troubling questions for now. Once the network architecture optimization over $\\f$ boils down to tuning the weights and biases of $f$ such that $\\hat{R}(f)$ is as small as possible. In other words, we will wish to solve for $f_b$, the “best model” in the hypothesis class $\\f$:\n\n\\[f_b = \\arg \\min_{f \\in \\f} \\hat{R}(f) .\\]\n\nAlas, yet another Major Challenge. Tuning weights and biases to optimality is extremely difficult, except in the simplest of hypothesis classes (such as linear/affine models). In practice, we never solve this optimization problem, but rather just run some kind of incremental “training” procedure for some number of steps that iteratively decreases $\\hat{R}(f)$ until everyone is satisfied. Let us assume that we are somehow able to get a decent answer. Let the final result of this training procedure be called $\\hat{f}$.\n\nSo, to recap: we have introduced two definitions of risk ($R, \\hat{R}$), and defined two models ($f_b, \\hat{f}$). This final network $\\hat{f}$ is what we end up using to perform all future predictions. Our hope is that $\\hat{f}$ performs “well” on “most” future data points. Quantitatively, we would like to ensure that the population risk\n\\[R(\\hat{f}) \\]\nis small.\n\nBut can we prove that this is the case? Again, the classical theory of supervised learning breaks this down into three components:\n\n\\[\\begin{aligned}\nR(\\hat{f}) = &amp; \\quad R(\\hat{f}) - \\hat{R}(\\hat{f}) \\\\\n          &amp;+ \\hat{R}(\\hat{f}) - \\hat{R}(f_b) \\\\\n          &amp;+ \\hat{R}(f_b) .\n\\end{aligned}\\]\n\nIf all three components are on the right hand side are provably small, then we are in the clear. Let us parse these three terms in reverse order.\n\nFirst, if\n\\[ \\hat{R}(f_b) \\]\nis small then this means our network architecture $\\f$ is rich enough for our purposes (or at least, rich enough to fit the training data well). We call this the representation error in deep learning, and a conclusive proof showing that this quantity is small would address the middle Major Challenge identified above.\n\nSecond, if\n\\[ \\hat{R}(\\hat{f}) - \\hat{R}(f_b) \\]\nis small then our numerical training procedure used to find $\\hat{f}$ has been reasonably successful. We call this the optimization error in deep learning, and a conclusive proof showing that this quantity is small would address the last Major Challenge identified above.\n\nThird, if\n\\[R(\\hat{f}) - \\hat{R}(\\hat{f}) \\]\nis small then $R$ and $\\hat{R}$ are not too different for $\\hat{f}$. In other words, we need to prove that the empirical risk is a good proxy for the population risk. we call this the generalization problem in deep learning, and a decisive solution to this problem would address the first Major Challenge identified above.\n\nOutline\n\nThe above narrative is all very classical and can be found in any introductory text on statistical machine learning. For simple cases (such as linear models), good bounds can be derived for all three quantities.\n\nFor us, the main difference lies in how we address these questions in the context of deep learning. Somewhat troublingly, clean answers to each of the above foundational problems do not seem to (yet) exist for deep neural network models. It is likely that these problems cannot be studied in isolation, and that theoretical results on these problems interact in mysterious ways2.\n\nStill, for the purposes of organizing the existing literature with some degree of coherence, we will follow this classical narrative. Within the context of (deep) neural network learning, we will cover:\n\n\n  \n    Representation: Making as few assumptions as possible, we will derive bounds on the number of neurons (and the shapes of neural networks) required to achieve low representation error. These are in the form of universal approximation theorems. We will explore both the finite setting (where we are trying to memorize a finite training dataset) and the infinite setting (where we are trying to approximate continuous functions). We will see how trading off depth versus width leads to interesting behaviors.\n  \n  \n    Optimization: We will study natural first-order algorithms for neural network training, and derive bounds on the number of training steps required to achieve low optimization error. In some cases, our results will show that the solution achieved by these training procedures are locally optimal. In other cases, we will prove that our training procedures achieve global optimality. Of particular interest to us are interesting connections to classical kernel-learning called the Neural Tangent Kernel (NTK).\n  \n  \n    Generalization: We will study ways to obtain bounds on the number of training examples required to achieve low generalization error. In many cases, some existing techniques from classical learning theory may lead to vacuous bounds, while other techniques are more successful; our focus will be to get non-vacuous generalization guarantees. We will also study double-descent phenomena that reveal a curious relationships between the number of parameters and the generalization error.\n  \n  \n    Miscellaneous topics:  And finally, to round things off, we will analyze other aspects of neural networks of relevance to practice beyond just achieving good prediction. Questions surrounding the robustness of neural networks have already emerged. Issues such as privacy and security of data/models are paramount. Beyond just label prediction, neural networks are increasingly being used to solve more challenging tasks, including  synthesis and generation of new examples.\n  \n\n\n\n\n\n  \n    \n\n      We will instantiate $\\f$ with specific instances as needed. &#x21a9;&#xfe0e;\n    \n    \n\n      For example, funnily enough, networks that exhibit very low optimization gap also sometimes lead to excellent generalization, in contradiction to what we would expect from classical learning theory. &#x21a9;&#xfe0e;\n    \n  \n\n",
});

// Builds reference data (maybe not necessary for us, to check)
var store = [
    {
	"title":  "Chapter 9 - Generalization bounds via stability",
	"author": null,
	"layout": "page",
	"link":   "/generalization03/",
    },
    {
	"title":  "Chapter 8 - PAC learning primer and error bounds",
	"author": null,
	"layout": "page",
	"link":   "/generalization02/",
    },
    {
	"title":  "Chapter 7 - Implicit regularization",
	"author": null,
	"layout": "page",
	"link":   "/generalization01/",
    },
    {
	"title":  "Chapter 6 - Beyond NTK",
	"author": null,
	"layout": "page",
	"link":   "/optimization03/",
    },
    {
	"title":  "Chapter 5 - Optimizing wide networks",
	"author": null,
	"layout": "page",
	"link":   "/optimization02/",
    },
    {
	"title":  "Chapter 4 - A primer on optimization",
	"author": null,
	"layout": "page",
	"link":   "/optimization01/",
    },
    {
	"title":  "Chapter 3 - The role of depth",
	"author": null,
	"layout": "page",
	"link":   "/representation03/",
    },
    {
	"title":  "Chapter 2 - Universal approximators",
	"author": null,
	"layout": "page",
	"link":   "/representation02/",
    },
    {
	"title":  "Chapter 1 - Memorization",
	"author": null,
	"layout": "page",
	"link":   "/representation01/",
    },
    {
	"title":  "Introduction",
	"author": null,
	"layout": "page",
	"link":   "/introduction/",
    }
]

// Query
var searchDiv = $("#x-search-query");
var resultDiv = $("#results");
var searchParams = new URLSearchParams(window.location.search)
if (searchParams.has('q')) {
    searchDiv.value = searchParams.get('q');
}

function doSearch() {
    var query = searchDiv.val();
    if (query.length <= 3) {
	resultDiv.empty();
	return;
    }

    // The search is then launched on the index built with Lunr
    var result = index.search(query);
    resultDiv.empty();
    if (result.length == 0) {
	resultDiv.append('<p class="">No results found.</p>');
    } else if (result.length == 1) {
	resultDiv.append('<p class="">Found '+result.length+' result</p>');
    } else {
	resultDiv.append('<p class="">Found '+result.length+' results</p>');
    }
    // Loop through, match, and add results
    for (var item in result) {
	var ref = result[item].ref;
	var searchitem = '<div class="result"><p><a href="/fodl'+store[ref].link+'?q='+query+'">'+store[ref].title+'</a></p></div>';
	resultDiv.append(searchitem);
    }

}

// something not working
$(document).ready(function() {
    if (searchParams.has('q')) {
	searchDiv.val(searchParams.get('q'));
	doSearch();
    }
    searchDiv.on('keyup', doSearch);
});
