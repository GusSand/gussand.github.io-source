<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text" xml:lang="en">Foundations of Deep Learning</title>
  <link type="application/atom+xml" href="http://localhost:4000/fodl/atom.xml" rel="self"/>
  <link type="text/html" href="http://localhost:4000/fodl/" rel="alternate"/>
  <updated>2023-03-09T16:50:39-05:00</updated>
  <id>http://localhost:4000</id>
  <rights>CC BY-SA</rights>

  
  <entry>
    <title>Introduction</title>
    <link href="http://localhost:4000/introduction/"/>
    <updated>2022-01-19T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/introduction</id>
    <content type="html">&lt;p&gt;Our goal is to reason about (deep) neural networks from the lens of &lt;em&gt;theory&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Unlike many other scientific fields, there currently exists a wide gap between:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;what the best available tools in theoretical computer science can tell us about modern neural networks, and&lt;/li&gt;
  &lt;li&gt;the actual ways in which modern neural network models work in practice.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This gap between theory and practice is, in my view, unsatisfactory. Yann LeCun invokes the &lt;a href=&quot;https://www.youtube.com/watch?v=gG5NCkMerHU&amp;amp;t=3210s&quot;&gt;“streetlight effect”&lt;/a&gt;: we search for lost keys where we can, not where they really lie.&lt;/p&gt;

&lt;p&gt;But here is why (I think) theory matters: by asking carefully crafted (but precise) questions about deep networks, one can precisely answer why certain aspects of neural networks work (or don’t), and what is in the realm of the possible (versus what isn’t). A major motivation behind these course notes is to identify the landscape of how wide exactly these gaps are at present, and how to bridge them.&lt;/p&gt;

&lt;p&gt;These notes are by no means the first attempt to do so. Other excellent courses/lecture notes/surveys include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.14368.pdf&quot;&gt;Fit without Fear&lt;/a&gt; by Misha Belkin (UCSD).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.notion.so/Mathematics-of-Deep-Learning-05cd9255f03842489083ec7cbb6338d5&quot;&gt;Mathematics of Deep Learning&lt;/a&gt; by Joan Bruna (NYU).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mjt.cs.illinois.edu/dlt/&quot;&gt;Deep Learning Theory&lt;/a&gt; by Matus Telgarsky (UIUC).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.princeton.edu/imabandit/2020/10/13/2020/&quot;&gt;Expository videos&lt;/a&gt; by Sebastian Bubeck (Microsoft).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class=&quot;label&quot; id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;As typical in (supervised) machine learning, our starting point is a list of $n$ example data-label pairs:
\[ X = {(x_i, y_i)}_{i=1}^n \subset \mathbb{R}^d \times \mathbb{R} \]
which we will call the &lt;em&gt;training set&lt;/em&gt;. This dataset is assumed to acquired via &lt;em&gt;iid sampling&lt;/em&gt; with respect to some underlying probability measure $\mu$ defined over $\mathbb{R}^d \times \mathbb{R}$.&lt;/p&gt;

&lt;p&gt;Our goal will be to &lt;em&gt;predict&lt;/em&gt; the label $y \in \mathbb{R}$ associated with some (hitherto unseen) data point $x \in \mathbb{R}^d$. In order to do so, we will seek a prediction function $f$ that can be expressed as a &lt;em&gt;neural network&lt;/em&gt;. Later we will more precisely define neural networks, but the familiar &lt;a href=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/640px-Neural_network_example.svg.png&quot;&gt;picture&lt;/a&gt; is appropriate for now. The key point is that we want an $f$ that performs “well” on “most” input data points.&lt;/p&gt;

&lt;p&gt;Let us first agree to measure “goodness” of performance via a two-argument loss function&lt;/p&gt;

\[l(\cdot,\cdot) : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}_{\geq 0}\]

&lt;p&gt;which takes in a predicted label, compares with the truth, and spits out a non-negative cost of fit (smaller is better). Then, quantitatively, our prediction function should be such that the population risk:&lt;/p&gt;

\[R(f) = \mathbb{E}_{(x,y) \sim \mu} l(y, f(x))\]

&lt;p&gt;is small.&lt;/p&gt;

&lt;p&gt;This immediately poses a &lt;strong&gt;Major Challenge&lt;/strong&gt;, since the population risk $R(f)$ is not an easy object to study. For starters, the probability measure $\mu$ may not be be known. Even if magically $\mu$ is available, calculating the expected value with respect to $\mu$ can be difficult. However, we do have training data lying around. Therefore, instead of directly dealing with $R(f)$, we will instead use a proxy quantity called the &lt;em&gt;empirical risk&lt;/em&gt;:&lt;/p&gt;

\[\hat{R}(f) = \frac{1}{n} \sum_{i=1}^n l(y_i, f(x_i))\]

&lt;p&gt;and seek an $f$ that makes this small.&lt;/p&gt;

&lt;script&gt;
macros[&quot;\\f&quot;] = &quot;\\mathscr{F}&quot;
&lt;/script&gt;

&lt;p&gt;Now, reducing the empirical risk $\hat{R}(f)$ to as small as possible is akin to function optimization. To make this numerically tractable, we will first cook up a hypothesis class $\f$. In deep learning, this can be thought of as the set of all neural network models that obey a certain architecture&lt;sup id=&quot;fnref:fn1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fn1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;But: this approach now poses another &lt;strong&gt;Major Challenge&lt;/strong&gt;. What’s a good family of architectures? How do we know whether a certain architecture is rich enough to solve our prediction problem? Can it go the other way (i.e., could we somehow pick a network architecture that is far too rich for our purposes?)&lt;/p&gt;

&lt;p&gt;Let us set aside such troubling questions for now. Once the network architecture optimization over $\f$ boils down to tuning the weights and biases of $f$ such that $\hat{R}(f)$ is as small as possible. In other words, we will wish to solve for $f_b$, the “best model” in the hypothesis class $\f$:&lt;/p&gt;

\[f_b = \arg \min_{f \in \f} \hat{R}(f) .\]

&lt;p&gt;Alas, yet another &lt;strong&gt;Major Challenge&lt;/strong&gt;. Tuning weights and biases &lt;em&gt;to optimality&lt;/em&gt; is extremely difficult, except in the simplest of hypothesis classes (such as linear/affine models). In practice, we never solve this optimization problem, but rather just run some kind of incremental “training” procedure for some number of steps that iteratively decreases $\hat{R}(f)$ until everyone is satisfied. Let us assume that we are somehow able to get a decent answer. Let the final result of this training procedure be called $\hat{f}$.&lt;/p&gt;

&lt;p&gt;So, to recap: we have introduced two definitions of risk ($R, \hat{R}$), and defined two models ($f_b, \hat{f}$). This final network $\hat{f}$ is what we end up using to perform all future predictions. Our hope is that $\hat{f}$ performs “well” on “most” future data points. Quantitatively, we would like to ensure that the population risk
\[R(\hat{f}) \]
is small.&lt;/p&gt;

&lt;p&gt;But can we &lt;em&gt;prove&lt;/em&gt; that this is the case? Again, the classical theory of supervised learning breaks this down into three components:&lt;/p&gt;

\[\begin{aligned}
R(\hat{f}) = &amp;amp; \quad R(\hat{f}) - \hat{R}(\hat{f}) \\
          &amp;amp;+ \hat{R}(\hat{f}) - \hat{R}(f_b) \\
          &amp;amp;+ \hat{R}(f_b) .
\end{aligned}\]

&lt;p&gt;If all three components are on the right hand side are &lt;em&gt;provably&lt;/em&gt; small, then we are in the clear. Let us parse these three terms in reverse order.&lt;/p&gt;

&lt;p&gt;First, if
\[ \hat{R}(f_b) \]
is small then this means our network architecture $\f$ is rich enough for our purposes (or at least, rich enough to fit the training data well). We call this the &lt;strong&gt;representation error&lt;/strong&gt; in deep learning, and a conclusive proof showing that this quantity is small would address the middle Major Challenge identified above.&lt;/p&gt;

&lt;p&gt;Second, if
\[ \hat{R}(\hat{f}) - \hat{R}(f_b) \]
is small then our numerical training procedure used to find $\hat{f}$ has been reasonably successful. We call this the &lt;strong&gt;optimization error&lt;/strong&gt; in deep learning, and a conclusive proof showing that this quantity is small would address the last Major Challenge identified above.&lt;/p&gt;

&lt;p&gt;Third, if
\[R(\hat{f}) - \hat{R}(\hat{f}) \]
is small then $R$ and $\hat{R}$ are not too different for $\hat{f}$. In other words, we need to prove that the empirical risk is a &lt;em&gt;good proxy&lt;/em&gt; for the population risk. we call this the &lt;strong&gt;generalization problem&lt;/strong&gt; in deep learning, and a decisive solution to this problem would address the first Major Challenge identified above.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;The above narrative is all very classical and can be found in any introductory text on statistical machine learning. For simple cases (such as linear models), good bounds can be derived for all three quantities.&lt;/p&gt;

&lt;p&gt;For us, the main difference lies in how we address these questions in the context of deep learning. Somewhat troublingly, clean answers to each of the above foundational problems do not seem to (yet) exist for deep neural network models. It is likely that these problems cannot be studied in isolation, and that theoretical results on these problems interact in mysterious ways&lt;sup id=&quot;fnref:fn2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fn2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Still, for the purposes of organizing the existing literature with some degree of coherence, we will follow this classical narrative. Within the context of (deep) neural network learning, we will cover:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Representation&lt;/strong&gt;: Making as few assumptions as possible, we will derive bounds on the &lt;strong&gt;number of neurons&lt;/strong&gt; (and the shapes of neural networks) required to achieve low representation error. These are in the form of &lt;strong&gt;universal approximation&lt;/strong&gt; theorems. We will explore both the finite setting (where we are trying to memorize a finite training dataset) and the infinite setting (where we are trying to approximate continuous functions). We will see how trading off &lt;strong&gt;depth versus width&lt;/strong&gt; leads to interesting behaviors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: We will study natural first-order algorithms for neural network training, and derive bounds on the number of training steps required to achieve low optimization error. In some cases, our results will show that the solution achieved by these training procedures are &lt;strong&gt;locally optimal&lt;/strong&gt;. In other cases, we will prove that our training procedures achieve &lt;strong&gt;global&lt;/strong&gt; optimality. Of particular interest to us are interesting connections to classical kernel-learning called the &lt;strong&gt;Neural Tangent Kernel&lt;/strong&gt; (NTK).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generalization&lt;/strong&gt;: We will study ways to obtain bounds on the number of training examples required to achieve low generalization error. In many cases, some existing techniques from classical learning theory may lead to vacuous bounds, while other techniques are more successful; our focus will be to get &lt;strong&gt;non-vacuous&lt;/strong&gt; generalization guarantees. We will also study &lt;strong&gt;double-descent&lt;/strong&gt; phenomena that reveal a curious relationships between the number of parameters and the generalization error.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Miscellaneous&lt;/strong&gt; topics:  And finally, to round things off, we will analyze other aspects of neural networks of relevance to practice beyond just achieving good prediction. Questions surrounding the &lt;strong&gt;robustness&lt;/strong&gt; of neural networks have already emerged. Issues such as &lt;strong&gt;privacy and security&lt;/strong&gt; of data/models are paramount. Beyond just label prediction, neural networks are increasingly being used to solve more challenging tasks, including  &lt;strong&gt;synthesis and generation&lt;/strong&gt; of new examples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:fn1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;We will instantiate $\f$ with specific instances as needed. &lt;a href=&quot;#fnref:fn1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fn2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;For example, funnily enough, networks that exhibit very low optimization gap &lt;em&gt;also&lt;/em&gt; sometimes lead to excellent generalization, in contradiction to what we would expect from classical learning theory. &lt;a href=&quot;#fnref:fn2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 1 - Memorization</title>
    <link href="http://localhost:4000/representation01/"/>
    <updated>2022-01-18T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/representation01</id>
    <content type="html">&lt;p&gt;Let us begin by trying to rigorously answer a very simple question:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;How many neurons suffice?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Upon reflection it should clear that this question isn’t well-posed. Suffice for what? For good performance? On what task? Do we even know if this answer is well-defined – perhaps it depends on some hard-to-estimate quantity related to the learning problem? Even if we were able to get a handle on this quantity, does it matter how the neurons are connected – should the network be wide/shallow, or narrow/deep?&lt;/p&gt;

&lt;p&gt;Let us begin simple. Suppose all we have is a bunch of training data points:
\[ X = \lbrace (x_i, y_i)\rbrace_{i=1}^n \subset \mathbb{R}^d \times \mathbb{R} \]
and our goal will be to discover a network that &lt;em&gt;exactly&lt;/em&gt; memorizes $X$. That is, we will learn a function $f$ that, when evaluated on every data point $x_i$ in the training data, returns $y_i$. Equivalently, if we define empirical risk via the squared error loss:&lt;/p&gt;

\[\begin{aligned}
\hat{y} &amp;amp;= f(x), \\
l(y,\hat{y}) &amp;amp;= 0.5(y - \hat{y})^2, \\
R(f) &amp;amp;= \sum_i \frac{1}{n} l(y_i, \hat{y_i}),
\end{aligned}\]

&lt;p&gt;then $f$ achieves &lt;em&gt;zero empirical risk&lt;/em&gt;. Our intuition says (and we will prove more versions of this later) is that a very large (very wide, or very deep, or both) network is likely enough to fit basically anything we like. So really, we want &lt;em&gt;reasonable&lt;/em&gt; upper bounds on the number of neurons needed for exact memorization.&lt;/p&gt;

&lt;p&gt;But why should we care about memorization anyway? After all, machine learning folks are taught to be wary of &lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting&quot;&gt;overfitting&lt;/a&gt; to the training set. In introductory ML courses we spend several hours (and homework sets) covering the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&quot;&gt;bias-variance&lt;/a&gt; tradeoff, the importance of adding a regularizer to decrease variance (at the expense of incurring extra “bias”), etc etc.&lt;/p&gt;

&lt;p&gt;Unfortunately, deep learning practice throws this classical way of ML thinking out of the window. We seldom use explicit regularizers, instead relying on standard losses. We typically train deep neural networks to achieve 100\% (train) accuracy. Later, we will try to understand why networks trained to &lt;em&gt;perfectly&lt;/em&gt; interpolate the training data still generalize well, but for now let’s focus on just achieving a representation that enables perfect memorization.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;basics&quot;&gt;Basics&lt;/h2&gt;

&lt;p&gt;First, a basic definition of a “neural network”.&lt;/p&gt;

&lt;p&gt;For our purposes, neural network is composed of several primitive “units”, each of which we will call a &lt;em&gt;neuron&lt;/em&gt;. Given an input vector $x \in \mathbb{R}^d$, a neuron transforms the input according to the following functional form:&lt;/p&gt;

\[z = \psi(\sum_{j=1}^d w_j x_j + b) .\]

&lt;p&gt;Here, $w_j$ are the weights, $b$ is a scalar called the &lt;em&gt;bias&lt;/em&gt;, and $\psi$ is a nonlinear scalar transformation called the &lt;em&gt;activation function&lt;/em&gt;; a typical activation function is the “ReLU function” $\psi(z) = \max(0,z)$ but we will also consider others.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/nn.png&quot; alt=&quot;Structure of a deep neural network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A neural network is a &lt;em&gt;feedforward composition&lt;/em&gt; of several neurons, typically arranged in the form of &lt;em&gt;layers&lt;/em&gt;. So if we imagine several neurons participating at the $l^{\text{th}}$ layer, we can stack up their weights (row-wise) in the form of a &lt;em&gt;weight matrix&lt;/em&gt; $W^{(l)}$. The output of neurons forming each layer forms the corresponding input to all of the neurons in the next layer. So a neural network with two layers of neurons would have&lt;/p&gt;

\[\begin{aligned}
z_{1} &amp;amp;= \psi(W_{1} x + b_{1}), \\
z_{2} &amp;amp;= \psi(W_{2} z_{1} + b_{2}), \\
y &amp;amp;= W_{3} z_{2} + b_{3} .
\end{aligned}\]

&lt;p&gt;Analogously, one can extend this definition to $L$ layers for any $L \geq 1$. The nomenclature is a bit funny sometimes. The above example is either called a “3-layer network” or “2-hidden-layer network”, depending on who you ask. The output $y$ is considered as its own layer and not considered as “hidden” (also notice that it doesn’t have any activation in this case; that’s typical.)&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;memorization-capacity-standard-results&quot;&gt;Memorization capacity: Standard results&lt;/h2&gt;

&lt;script&gt;
macros[&quot;\\f&quot;] = &quot;\\mathscr{F}&quot;
&lt;/script&gt;

&lt;p&gt;A lot of interesting quirks arise even in the simplest cases.&lt;/p&gt;

&lt;p&gt;Let us focus our attention on the ability of &lt;em&gt;two-layer&lt;/em&gt; networks (or one-hidden-layer networks) to memorize $n$ data points. We will restrict discussion to ReLU activations but the arguments below are generally applicable. If there are $m$ hidden neurons and if $\psi$ is the ReLU then our hypothesis class $\f_m$ comprises all functions $f$ such that for suitable weight parameters $(\alpha_i, w_i)$ and bias parameters $b_i$, we have:&lt;/p&gt;

&lt;p&gt;\[ f(x) = \sum_{i=1}^m \alpha_i \psi(\langle w_i, x \rangle + b_i) . \]&lt;/p&gt;

&lt;p&gt;Intuitively, $m &amp;lt; \infty$ is a trivial upper bound on any dataset (we will be more rigorous about this when we prove universal approximation results). If we have infinitely many parameters then memorization should be trivial. Let us get a better upper bound for $m$. Our first result shows that $m = n$ should suffice.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationBasic&quot;&gt;Theorem&lt;/strong&gt;
  Let $f$ be a two-layer ReLU network with $m = n$ hidden neurons. For any &lt;em&gt;arbitrary&lt;/em&gt; dataset $X = \lbrace (x_i, y_i)_{i=1}^n\rbrace \subset \mathbb{R}^d \times \mathbb{R}$ where $x_i$ are in general position, the weights and biases of $f$ can be chosen such that $f$ exactly interpolates $X$.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationBasicProof1&quot;&gt;Proof&lt;/strong&gt;
  This result is non-constructive and seems to be folklore, dating back to at least Baum&lt;sup id=&quot;fnref:baum&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:baum&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. For modern versions of this proof, see Bach&lt;sup id=&quot;fnref:bach&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bach&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; or Bubeck et al.&lt;sup id=&quot;fnref:bubeck1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bubeck1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p class=&quot;proof&quot;&gt;Define the space of &lt;em&gt;arbitrary width&lt;/em&gt; two-layer networks:
  \[\f = \bigcup_{m \geq 0} \f_m . \]
  The high level idea is that $\f$ forms a &lt;em&gt;vector space&lt;/em&gt;. This is easy to see, since it is closed under additions and scalar multiplications. Formally, fix $x$ and consider the element $\psi_{w,b}: x \mapsto \psi(\langle w, x \rangle + b)$. Then, $\text{span}(\psi_{w,b})$ forms a vector space.  Now, consider the linear &lt;em&gt;pointwise&lt;/em&gt; evaluation operator $\Psi : V \rightarrow \mathbb{R}^n$:
  \[\Psi(f) = (f(x_1), f(x_2), \ldots, f(x_n)) .\]
  We know from classical universal approximation (Chapter 2) that &lt;em&gt;every vector&lt;/em&gt; in $\mathbb{R}^n$ can be written as &lt;em&gt;some&lt;/em&gt; (possibly infinite)  combination of neurons. Therefore, $\text{Range}(\Psi) = \mathbb{R}^n$, or $\text{dim(Range}(\Psi)) = n$. Therefore, there &lt;em&gt;exists&lt;/em&gt; some basis of size $n$ with the same span! Call this basis $\lbrace \psi_1, \ldots,\psi_n\rbrace$. This basis can be used to express any set of labels by choosing appropriate coefficients in a standard basis representation $y = \sum_{i=1}^n \alpha_i \psi_i$.
  The result follows.&lt;/p&gt;

&lt;p&gt;In fact, the above result holds for any activation function $\psi$ that is not a polynomial&lt;sup id=&quot;fnref:leshno&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:leshno&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;; we will revisit this curious property later.&lt;/p&gt;

&lt;p&gt;Really, we didn’t do much here. Since the “information content” in $n$ labels has dimension $n$, we can extract any arbitrary basis (written in the form of neurons) and write down the expansion of the labels in terms of this basis. Since this approach may be a bit abstract, let’s give an alternate &lt;em&gt;constructive&lt;/em&gt; proof.&lt;/p&gt;

&lt;p class=&quot;proof&quot;&gt;&lt;strong id=&quot;MemorizationBasicProof2&quot;&gt;Proof (Alternate.)&lt;/strong&gt;
  This proof can be attributed to Zhang et al&lt;sup id=&quot;fnref:zhang&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:zhang&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. Suppose $m = n.$ Since all $x_i$’s are distinct and in general position, we can pick a $w$ such that if define $z_i := \langle w, x_i \rangle$ then without loss of generality (or by re-indexing the data points):
  \[ z_1 &amp;lt; z_2 &amp;lt; \ldots z_n . \]
  One way to pick $w$ is by random projection: pick $w$ from a standard $d$-variate Gaussian distribution; then the above holds with high probability. If the above relation between $z_i$ holds, we can find some sequence of $b_i$ such that:
  \[ b_1 &amp;lt; z_1 &amp;lt; b_2 &amp;lt; z_2 &amp;lt; \ldots &amp;lt; b_n &amp;lt; z_n . \]
  Now, let’s define an $n \times n$ matrix $A$ such that
  \[ A_{ij} := \text{ReLU}(z_i - b_j) = \max(z_i - b_j, 0) . \]
  Since by definition, each $z_i$ is only bigger than all $b_j$ for $1 \leq j \leq i$, we have that $A$ is a &lt;em&gt;lower triangular&lt;/em&gt; matrix with positive entries on the diagonal, and therefore full rank. Moreover, for any $\alpha \in \mathbb{R}^n$, the product $A \alpha$ is the superposition of exactly $n$ ReLU neurons (the weights are the same for all of them, but the biases are distinct). Set $\alpha = A^{-1} y$ and we are done.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationBasic2&quot;&gt;Remark&lt;/strong&gt;
The above proofs used biases, but if we restrict our attention to &lt;em&gt;bias-free&lt;/em&gt; networks, that’s fine too, we just need to use different weights for the $n$ hidden neurons. Such a network is called a &lt;em&gt;random feature model&lt;/em&gt;; see &lt;a href=&quot;https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1810.04374&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;memorization-capacity-improved-results&quot;&gt;Memorization capacity: Improved results&lt;/h2&gt;

&lt;p&gt;The above result shows that $m = n$ neurons are sufficient to memorize pretty much any dataset. Can we get away with fewer neurons? Notice that really the network has to “remember” only the $n$ labels; but since there are $n$ neurons, each with $d$ input edges, the number of &lt;em&gt;parameters&lt;/em&gt; is $nd$.  (&lt;em&gt;Note: not technically correct; the second proof only uses $n$ &lt;strong&gt;distinct&lt;/strong&gt; weights and $n$ biases.&lt;/em&gt;) It turns out that we can indeed do better.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationBetter&quot;&gt;Theorem&lt;/strong&gt;
  For any dataset of $n$ points in general position, $m = 4 \lceil \frac{n}{d} \rceil$ neurons suffice to memorize it.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationBetterProof&quot;&gt;Proof&lt;/strong&gt;
  For ReLU activations, this proof is in Bubeck et al&lt;sup id=&quot;fnref:bubeck1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bubeck1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, although Baum&lt;sup id=&quot;fnref:baum:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:baum&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; proved a similar result for threshold activations and binary labels.&lt;/p&gt;

&lt;p&gt;Notice that $d = \Omega(1)$ is actually beneficial here; higher input dimension implies &lt;em&gt;easier&lt;/em&gt; memorization. In some ways this is a &lt;em&gt;blessing of dimensionality&lt;/em&gt; phenomenon.&lt;/p&gt;

&lt;p&gt;We will prove Baum’s result first. Suppose we have threshold activations (i.e., $\psi(z) = \mathbb{I}(z \geq 0)$) and binary labels $y_i \in \pm {1}$. We iteratively build a network as follows. Without loss of generality, we can assume that there are at least $d$ points with label equal to $y_i = 1$; index them as $x_1, x_2, \ldots, x_d$. Since the points are in general position, we can find an affine subspace that exactly interpolates these points:&lt;/p&gt;

\[\langle w_1, x_i \rangle = b_1, \quad i \in [d]\]

&lt;p&gt;and record $(w_1, b_1)$. (Importantly, again since the points are in general position no other points lie in this subspace.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/baum-construction.png&quot; alt=&quot;Iteratively fitting neurons to a dataset.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, form a &lt;em&gt;very thin indicator slab&lt;/em&gt; using for this affine subspace using exactly two neurons:&lt;/p&gt;

\[x \mapsto \psi(\langle w_1,x \rangle - (b_1-\varepsilon)) - \psi(\langle w_1,x \rangle - (b_1+\varepsilon))\]

&lt;p&gt;for some small enough $\varepsilon &amp;gt; 0$. This function is equal to 1 for exactly the points in the subspace, and zero for all other points. For this group of $d$ points we can assign the &lt;em&gt;output&lt;/em&gt; weight $\alpha_1 = 1$. Iterate this argument $\lceil \frac{n}{d} \rceil$ times and we are done! Therefore, $2 \lceil \frac{n}{d} \rceil$ threshold neurons suffice if the labels are binary.&lt;/p&gt;

&lt;p&gt;The exact same argument can be extended to ReLU activations and arbitrary (scalar) labels. Again, we iteratively build the network. We pick an arbitrary set of $d$ points, through which we can interpolate an affine subspace:&lt;/p&gt;

\[\langle u, x_i \rangle = b, \quad i \in [d] .\]

&lt;p&gt;We now show that we can memorize these $d$ data points using 4 ReLU neurons. The trick is to look at the “directional derivative” of the ReLU:&lt;/p&gt;

\[g: x \mapsto \frac{\psi(\langle u + \delta v, x \rangle - b) - \psi(\langle u, v \rangle - b)}{\delta} .\]

&lt;p&gt;As $\delta \rightarrow 0$, the right hand side approaches the quantity:&lt;/p&gt;

\[g: x \mapsto \psi'(\langle u, x \rangle - b) \langle v, x \rangle .\]

&lt;p&gt;But: the first part is the “derivative” of the ReLU, which is exactly the threshold function! Using the thin-slab-indicator trick in the proof above, the difference of two such functions (with slightly different $b$) forms an indicator on a thin slab around these $d$ points:&lt;/p&gt;

\[f = g_{u,v,b-\varepsilon} - g_{u,v,b+\varepsilon} .\]

&lt;p&gt;Since we are using differences-of-differences, we need 4 ReLUs to realize $f$. It now remains to pick $v$. But this is easy: since the data are in general position, just solve for $v$ such that&lt;/p&gt;

\[\langle v, x_i \rangle = y_i .\]

&lt;p class=&quot;proof&quot;&gt;Repeat this fitting procedure $\lceil \frac{n}{d} \rceil$ times and we are done.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationRem1&quot;&gt;Remark&lt;/strong&gt;
The above construction is somewhat wacky/combinatorial. The weights of each neuron was picked myopically (we never revisited data points) and locally (each neuron only depended on a small subset of data points).&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationRem3&quot;&gt;Remark&lt;/strong&gt;
The above construction says very little about how large networks need to be in order for “typical” learning algorithms (such as SGD) to succeed. We will revisit this in the Optimization chapters. For a recent result exploring the properties of “typical” gradient-based learning methods in the $O(n/d)$ regime, see here&lt;sup id=&quot;fnref:hong&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:hong&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;MemorizationRem5&quot;&gt;Remark&lt;/strong&gt;
All the above results used a standard dense feedforward architecture. Analogous memorization results have been shown for other architectures commonly used in practice today: convnets&lt;sup id=&quot;fnref:cnn&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cnn&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, ResNets&lt;sup id=&quot;fnref:resnet&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:resnet&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, transformers&lt;sup id=&quot;fnref:xformers&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:xformers&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;, etc.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;lower-bounds&quot;&gt;Lower bounds&lt;/h2&gt;

&lt;p&gt;The above results show that we can memorize $n$ (scalar) labels with no more than $O(n/d)$ ReLU neurons – each with $d$ incoming edges, which means that the number of learnable weights in this network is $O(n)$. Is this upper bound tight, or is there hope for doing any better?&lt;/p&gt;

&lt;p&gt;The answer seems to be &lt;em&gt;no&lt;/em&gt;, and intuitively it makes sense from a parameter counting perspective. Sontag&lt;sup id=&quot;fnref:sontag&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sontag&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; proved an early result along these lines, showing that if some function $f$ that is described with $o(n)$ parameters is &lt;em&gt;analytic&lt;/em&gt; (meaning that it is smooth and has convergent power series) and &lt;em&gt;definable&lt;/em&gt; (meaning that it can be expressed by some arbitrary composition of rational operations and exponents), then there exists at least one dataset of $n$ points that the network cannot memorize. This result also holds for functions that are &lt;em&gt;piecewise&lt;/em&gt; analytic and definable, meaning that neural networks (of arbitrary depth! not just two layers!) are applicable to this theorem.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Note: this observation is not technically correct; we can get better by “bit stuffing”. If we assume slightly more restrictive properties on the data and allow the network weights to be unbounded, then this bound can be improved to $O(\sqrt{n})$ parameters. We will revisit this later.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So it may seem interesting that we were able to get best-possible memorization capacity using simple 2-layer networks. So does additional depth buy us anything at all? The answer for this is &lt;em&gt;yes&lt;/em&gt;: we can decrease the number of &lt;em&gt;hidden neurons&lt;/em&gt; in the network significantly when we move from 1- to 2-hidden-layer networks. We will revisit this in Chapter 3.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;robust-interpolation&quot;&gt;Robust interpolation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;(Complete)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:baum&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;E. Baum, &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/0885064X88900209&quot;&gt;&lt;em&gt;On the capabilities of multilayer perceptrons&lt;/em&gt;&lt;/a&gt;, 1989. &lt;a href=&quot;#fnref:baum&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:baum:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bach&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;F. Bach, &lt;a href=&quot;https://jmlr.org/papers/v18/14-546.html&quot;&gt;&lt;em&gt;Breaking the Curse of Dimensionality with Convex Neural Networks&lt;/em&gt;&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:bach&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bubeck1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Bubeck, R. Eldan, Y. Lee, D. Mikulincer, &lt;a href=&quot;https://arxiv.org/abs/2006.02855&quot;&gt;Network size and weights size for memorization with two-layers neural networks&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:bubeck1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:bubeck1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:leshno&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Leshno, V. Lin, A. Pinkus, S. Schocken, &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315&quot;&gt;Multilayer feedforward networks with a nonpolynomial activation function can approximate any function&lt;/a&gt;, 1993. &lt;a href=&quot;#fnref:leshno&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zhang&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;Understanding deep learning requires rethinking generalization&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:zhang&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:hong&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;J. Zhang, Y. Zhang, M. Hong, R. Sun, Z.-Q. Luo, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html&quot;&gt;When Expressivity Meets Trainability: Fewer than $n$ Neurons Can Work&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:hong&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cnn&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Q. Nguyen and M. Hein, &lt;a href=&quot;https://arxiv.org/abs/1710.10928&quot;&gt;Optimization Landscape and Expressivity of Deep CNNs&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:cnn&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:resnet&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Hardt and T. Ma, &lt;a href=&quot;https://openreview.net/forum?id=ryxB0Rtxx&quot;&gt;Identity Matters in Deep Learning&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:resnet&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:xformers&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Yun, Y. Chang, S. Bhojanapalli, A. Rawat, S. Reddi, S. Kumar, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/9ed27554c893b5bad850a422c3538c15-Abstract.html&quot;&gt;$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:xformers&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sontag&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;E. Sontag, &lt;a href=&quot;http://www.sontaglab.org/FTPDIR/generic.pdf&quot;&gt;Shattering All Sets of k Points in “General Position” Requires (k − 1)/2 Parameters&lt;/a&gt;, 1997. &lt;a href=&quot;#fnref:sontag&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 2 - Universal approximators</title>
    <link href="http://localhost:4000/representation02/"/>
    <updated>2022-01-17T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/representation02</id>
    <content type="html">&lt;p&gt;Previously, we visited several results that showed how (shallow) neural networks can effectively memorize training data. However, memorization of a finite dataset may not the end goal&lt;sup id=&quot;fnref:fn1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fn1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. In the ideal case, we would like to our network to simulate a (possibly complicated) prediction function that works well on most input data points. So a more pertinent question might be:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Can neural networks simulate arbitrary functions?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this note we will study the representation power of (shallow) neural networks through the lens of their ability to approximate (continuous) functions. This line of work has a long and rich history. The field of function approximation, independent of the context of neural networks, is a vast body of work which we can only barely touch upon. See here&lt;sup id=&quot;fnref:devore&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:devore&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; for a recent (and fantastic) survey.&lt;/p&gt;

&lt;p&gt;As before, intuition tells us that an infinite number of neurons should be good enough to approximate pretty much anything. Therefore, our guiding principle will be to achieve as &lt;em&gt;succinct&lt;/em&gt; a neural representation as possible. Moreover, if there is an &lt;em&gt;efficient computational&lt;/em&gt; routine that gives this representation, that would be the icing on the cake.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;warmup-function-approximation&quot;&gt;Warmup: Function approximation&lt;/h2&gt;

&lt;script&gt;
macros[&quot;\\f&quot;] = &quot;\\mathscr{F}&quot;
&lt;/script&gt;

&lt;p&gt;Let’s again start simple. This time, we don’t have any training data to work with; let’s just assume we seek some (purported) prediction function $g(x)$. To approximate $g$, we have a candidate hypothesis class $\f_m$ of shallow (two-layer) neural networks of the form:&lt;/p&gt;

&lt;p&gt;\[ f(x) = \sum_{i=1}^m \alpha_i \psi(\langle w_i, x \rangle + b_i) . \]&lt;/p&gt;

&lt;p&gt;Our goal is to get reasonable bounds on how large $m$ needs to be in terms of various parameters of $g$. We have to be clear about what “approximate” means here. It is typical to measure approximation in terms of $p$-norms between measurable functions; for example, in the case of $L_2$-norms we will try to control&lt;/p&gt;

\[\int_{\text{dom}(g)} |f -g|^2 d \mu\]

&lt;p&gt;where $\mu$ is some measure defined over $\text{dom}(g)$. Likewise for the $L_\infty$- (or the sup-)norm, and so on.&lt;/p&gt;

&lt;h3 class=&quot;label&quot; id=&quot;univariate-functions&quot;&gt;Univariate functions&lt;/h3&gt;

&lt;p&gt;We begin with the special case of $d=1$ (i.e., the prediction function $g$ is univariate). Let us first define a useful property to characterize univariate functions.&lt;/p&gt;

&lt;p class=&quot;definition&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;Lipschitz&quot;&gt;Definition&lt;/strong&gt;
  (Univariate Lipschitz.) A function $g : \R \rightarrow \R$ is $L$-Lipschitz if for all $u,v \in \R$, we have that $|f(u) - f(v) | \leq L |u - v|$.&lt;/p&gt;

&lt;p&gt;Why is this an interesting property? Any smooth function with bounded derivative is Lipschitz; in fact, certain non-smooth functions (such as the ReLU) are also Lipschitz. Lipschitz-ness does not quite capture everything we care about (e.g. discontinuous functions are not Lipschitz, which can be somewhat problematic if there are “jumps” in the label space). But it serves as a large enough class of functions to prove interesting results.&lt;/p&gt;

&lt;p&gt;An additional benefit of Lipschitzness is due to approximability. If our target function $f$ is $L$-Lipschitz with reasonable $L$, then we can show that it can be well-approximated by a two-layer network with threshold activations: $\psi(z) = \mathbb{I}(z \geq 0)$. We prove:&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univariatesimple&quot;&gt;Theorem&lt;/strong&gt;
  Let $g : [0,1] \rightarrow \R$ be $L$-Lipschitz. Then, it can be  $\varepsilon$-approximated in the sup-norm by a two-layer network with $O(\frac{L}{\varepsilon})$ hidden threshold neurons.&lt;/p&gt;

&lt;p&gt;&lt;strong id=&quot;univariatesimpleproof&quot;&gt;Proof.&lt;/strong&gt;
  A more careful derivation of this fact (and the next one below) can be found in Telgarsky&lt;sup id=&quot;fnref:mjt&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:mjt&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. The proof follows from the same picture we might have seen while first learning about integrals and Riemann sums. The high level idea is to tile the interval $[0,1]$ using “buildings” of appropriate height. Since the derivatives are bounded (due to Lipschitzness), the top of each “building” cannot be too far away from the corresponding function value. Here is a picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/lipschitz.png&quot; alt=&quot;Approximating Lipschitz functions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More formally: partition $[0,1]$ into equal intervals of size $\varepsilon/L$. Let the $i$-th interval be $[u_i,u_{i+1})$. Define a sequence of functions $f_i(x)$ where each $f_i$ is zero everywhere, except within the $i$-th interval where it attains the value $g(u_i)$. Then $f_i$ can be written down as the difference of two threshold functions:&lt;/p&gt;

\[f_i(x) = g(u_i) \left(\psi(x - u_i) - \psi(x - u_{i+1})\right).\]

&lt;p&gt;Our network will be the sum of all the $f_i$’s (and there are $L/\varepsilon$ of them). Moreover, for any $x \in [0,1]$, if $u_i$ is the left end of the interval corresponding to $x$, then we have:&lt;/p&gt;

\[\begin{aligned}
  |f(x) - g(x)| &amp;amp;= |g(x) - g(u_i)| \\
  &amp;amp;\leq L |x - u_i | \qquad \text{(Lipschitzness)} \\
  &amp;amp;\leq L \frac{\varepsilon}{L} = \varepsilon,
  \end{aligned}\]

&lt;p class=&quot;proof&quot;&gt;Taking the supremum over all $x \in [0,1]$ completes the proof.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;UnivarianteRem1&quot;&gt;Remark&lt;/strong&gt;
So we can approximate $L$-Lipschitz functions with $O(L/\varepsilon)$ threshold neurons. Would the answer change if we used ReLU activations? (Hint: no, up to constants; prove this.)&lt;/p&gt;

&lt;h3 class=&quot;label&quot; id=&quot;multivariate-functions&quot;&gt;Multivariate functions&lt;/h3&gt;

&lt;p&gt;Of course, in deep learning we rarely care about univariate functions (i.e., where the input is 1-dimensional). We can ask a similar question in the more general case. Suppose we have $L$-Lipschitz functions over $d$ input variables and we want to approximate it using shallow neural networks. How many neurons do we need?&lt;/p&gt;

&lt;p&gt;We answer this question using two approaches. First, we give a construction using standard real analysis that uses &lt;em&gt;two&lt;/em&gt; hidden layers of neurons. Then, with some more mathematical powerful machinery we will get better (and much more general) results with only one hidden layer (i.e., using the hypothesis class $\f$).&lt;/p&gt;

&lt;p&gt;First, we have to define Lipschitzness for $d$-variate functions.&lt;/p&gt;

&lt;p class=&quot;definition&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;Lipschitz&quot;&gt;Definition&lt;/strong&gt;
  (Multivariate Lipschitz.) A function $g : \R^d \rightarrow \R$ is $L$-Lipschitz if for all $u,v \in \R^d$, we have that $|f(u) - f(v) | \leq L \lVert u - v \rVert_\infty$.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;multivariatesimple&quot;&gt;Theorem&lt;/strong&gt;
  Let $g : [0,1]^d \rightarrow \R$ be $L$-Lipschitz. Then, $g$ can be  $\varepsilon$-approximated in the $L_1$-norm by a three-layer network $f$ with $O(\frac{L}{\varepsilon^d})$ hidden threshold neurons.&lt;/p&gt;

&lt;p&gt;&lt;strong id=&quot;multivariateproof&quot;&gt;Proof sketch.&lt;/strong&gt;
  The proof follows the above construction for univariate functions. We will tile $[0,1]^d$ with equally spaced multidimensional rectangles; there are $O(\frac{1}{\varepsilon^d})$ of them. The value of the function $f$ within each rectangle will be held constant (and due to the definition of Lipschitzness, the error with respect to $g$ cannot be too large). If we can figure out how to approximate $g$ within each rectangle, then we are done.&lt;/p&gt;

&lt;p&gt;The key idea is to figure out how to realize “indicator functions” for every rectangle. We have seen that in the univariate case, indicators can be implemented using the difference of two threshold neurons. In the $d$-variate case, an indicator over a rectangle is the &lt;em&gt;Cartesian product&lt;/em&gt; over the $d$ axis. however, Boolean/Cartesian products can be implemented by a layer of threshold activations &lt;em&gt;on top&lt;/em&gt; of these differences.&lt;/p&gt;

&lt;p&gt;Formally, consider any arbitrary piece with $[u_j,v_j), j=1,2,\ldots,d$ as sides. The domain can be written as the Cartesian product:&lt;/p&gt;

\[S = \times_{j=1}^d [u_j, v_j).\]

&lt;p&gt;Therefore, we can realize an indicator function over this domain as follows. Localize within each coordinate by the “difference-of-threshold neurons”:&lt;/p&gt;

\[h_j(z) = \psi(z-v_j) - \psi(z - u_j)\]

&lt;p&gt;and implement the entire rectangle is implemented via a “Boolean AND” over all the coordinates:&lt;/p&gt;

\[h(x) = \psi(\sum_{j=1}^d h_j(x_j) - (d-1)),\]

&lt;p class=&quot;proof&quot;&gt;where $x_j$ is the $j$-th coordinate of $x$. There is one such $h$ for every rectangle, and the output edge from this neuron is assigned a constant value approximating $g$ within that rectangle. This completes the proof.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;MultivariateRem1&quot;&gt;Remark&lt;/strong&gt;
Would the answer change if we used ReLU activations? (Hint: no, up to constants; prove this.)&lt;/p&gt;

&lt;p&gt;Before proceeding, let’s just reflect on the bound (and the nature of the network) that we constructed in the proof. Each neuron in the first layer looks at the right “interval” independently each input coordinate; there are $d$ such coordinates, and therefore $O(\frac{dL}{\varepsilon})$ intervals.&lt;/p&gt;

&lt;p&gt;The second layer is where the real difficulty lies. Each neuron picks exactly the right set of intervals to define a unique hyper-rectangle. There are $O(\frac{1}{\varepsilon^d})$ such rectangles. Therefore, the last layer becomes very, very wide with increasing $d$. This is unfortunate, since we desire succinct representations.&lt;/p&gt;

&lt;p&gt;So the next natural question is: can we get better upper bounds? Also, do we really need two hidden layers (or is the hypothesis class $\f_m$ good enough for sufficiently large $m$)?&lt;/p&gt;

&lt;p&gt;The answer to both questions is a (qualified) &lt;em&gt;yes&lt;/em&gt;, but first we need to gather a few more tools.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;universal-approximators&quot;&gt;Universal approximators&lt;/h2&gt;

&lt;p&gt;The idea of defining succinct hypothesis classes to approximate functions had been well studied well before neural networks were introduced. In fact, we can go all the way back to:&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;multivariatesimple&quot;&gt;Theorem&lt;/strong&gt;
  (&lt;em&gt;Weierstrass, 1865.&lt;/em&gt;) Let $g : [0,1] \rightarrow \R$ be any continuous function. Then, $g$ can be  $\varepsilon$-approximated in the sup-norm by some polynomial of sufficiently high degree.&lt;/p&gt;

&lt;p&gt;Weierstrass proved this via an interesting trick: he took the function $g$, &lt;em&gt;convolved&lt;/em&gt; this with a Gaussian (which made everything smooth/analytic) and then did a Taylor series. Curiously, we will return this property much later when we study adversarial robustness of neural networks.&lt;/p&gt;

&lt;p&gt;In fact, there is a more direct &lt;em&gt;constructive&lt;/em&gt; proof of this result by Bernstein&lt;sup id=&quot;fnref:bernstein&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bernstein&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;; we won’t go over it but see, for example &lt;a href=&quot;http://nonagon.org/ExLibris/bernstein-proves-weierstrass&quot;&gt;here&lt;/a&gt;. The key idea is to construct a sufficiently large set of &lt;em&gt;interpolating basis functions&lt;/em&gt; (in Bernstein’s case, his eponymous polynomials), whose combinations densely span the entire space of continuous functions.&lt;/p&gt;

&lt;p&gt;Other than polynomials, what other families of “basis” functions lead to successful approximation? To answer this, we first define the concept of a &lt;em&gt;universal approximator&lt;/em&gt;.&lt;/p&gt;

&lt;p class=&quot;definition&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxdef&quot;&gt;Definition&lt;/strong&gt;
  Let $\f$ be a given hypothesis class. Then, $\f$ is a universal approximator over some domain $S$ if for every continuous function $g : S \rightarrow \R$ and approximation parameter $\varepsilon &amp;gt; 0$, there exists $f \in \f$ such that:
  \(\sup_{x \in S} |f(x) - g(x) | \leq \varepsilon .\)&lt;/p&gt;

&lt;p&gt;The Weierstrass theorem showed that that the set of &lt;em&gt;all&lt;/em&gt; polynomials is a universal approximator. In fact, a generalization of this theorem shows that other families of functions that behave like polynomials are also universal approximators. This is called the &lt;em&gt;Stone-Weierstrass&lt;/em&gt; theorem, stated as follows.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;stoneweierstrass&quot;&gt;Theorem&lt;/strong&gt;
  (&lt;em&gt;Stone-Weierstrass, 1948.&lt;/em&gt;) If the following hold:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(&lt;em&gt;Continuity&lt;/em&gt;) Every $f \in \f$ is continuous.&lt;/li&gt;
  &lt;li&gt;(&lt;em&gt;Identity&lt;/em&gt;) $\forall~x$, there exists $f \in \f$ s.t. $f(x) \neq 0$.&lt;/li&gt;
  &lt;li&gt;(&lt;em&gt;Separation&lt;/em&gt;) $\forall~x, x’,~x\neq x’,$ there exists $f \in \f$ s.t. $f(x) \neq f(x’)$.&lt;/li&gt;
  &lt;li&gt;(&lt;em&gt;Closure&lt;/em&gt;) $\f$ is closed under additions and multiplications.&lt;/li&gt;
&lt;/ol&gt;

&lt;p class=&quot;theorem&quot;&gt;then $\f$ is a universal approximator.&lt;/p&gt;

&lt;p&gt;We will use this property to show that (in very general situations), several families of neural networks are universal approximators. To be precise, let $f(x)$ be a single neuron:&lt;/p&gt;

\[f_{\alpha,w,b} : x \mapsto \alpha \psi(\langle w, x \rangle + b)\]

&lt;p&gt;and define&lt;/p&gt;

\[\f = \text{span}_{\alpha,w,b} \lbrace f_{\alpha,w,b} \rbrace\]

&lt;p&gt;as the space of all possible single-hidden-layer networks with activation $\psi$. We prove the following several results, and follow these with several remarks.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxcos&quot;&gt;Theorem&lt;/strong&gt;
  If we use the cosine activation $\psi(\cdot) = \cos(\cdot)$, then $\f$ is a universal approximator.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxcosproof&quot;&gt;Proof&lt;/strong&gt;
  This result is the OG “universal approximation theorem” and can be attributed to Hornik, Stinchcombe, and White&lt;sup id=&quot;fnref:hornik&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:hornik&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. Contemporary results of basically the same flavor are due to Cybenko&lt;sup id=&quot;fnref:cybenko&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cybenko&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and Funahashi&lt;sup id=&quot;fnref:funashashi&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:funashashi&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; but using techniques other than Stone-Weierstrass.&lt;/p&gt;

&lt;p&gt;All we need to do is to show that the space of (possibly unbounded width) single-hidden-layer networks satisfies the four conditions of Stone-Weierstrass.&lt;/p&gt;
&lt;ul class=&quot;proof&quot;&gt;
  &lt;li&gt;(Continuity) Obvious. Check.&lt;/li&gt;
  &lt;li&gt;(Identity) For every $x$, $\cos(\langle 0, x \rangle) = \cos(0) = 1 \neq 0$. Check.&lt;/li&gt;
  &lt;li&gt;(Separation) For every $x \neq x’$, $f(z) = \cos\left(\frac{1}{\lVert x-x’ \rVert_2^2}\langle x-x’, z-x’ \rangle \right)$ separates $x,x’$. Check.&lt;/li&gt;
  &lt;li&gt;(Closure) This is the most crucial one. Closure under additions is trivial (just add more hidden units!) Closure under multiplications is due to trigonometry: we know that $\cos(\langle u, x \rangle) \cos(\langle v, x \rangle) = \frac{1}{2} (\cos(\langle u+v, x \rangle) + \cos(\langle u-v, x \rangle))$. Therefore, products of two $\cos$ neurons can be equivalently expressed by the &lt;em&gt;sum&lt;/em&gt; of two (other) $\cos$ neurons. Check.
  This completes the proof.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxexp&quot;&gt;Theorem&lt;/strong&gt;
  If we use the exponential activation $\psi(\cdot) = \exp(\cdot)$, then $ \f $ is a universal approximator.&lt;/p&gt;

&lt;p class=&quot;proof&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxcosproof&quot;&gt;Proof&lt;/strong&gt;
  Even easier than $\cos$. &lt;em&gt;(COMPLETE)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The OG paper by Hornik et al&lt;sup id=&quot;fnref:hornik:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:hornik&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; showed a more general result for sigmoidal activations.  Here a sigmoidal activation is any function $\psi$ such that $\lim_{z \rightarrow -\infty} = 0$ and $\lim_{z \rightarrow +\infty} = 1$. This result covers “threshold” activations, hard/soft tanh, other regular sigmoids, etc.&lt;/p&gt;

&lt;p class=&quot;theeorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxsigmoid&quot;&gt;Theorem&lt;/strong&gt;
  If we use any sigmoidal activation $\psi(\cdot)$ that is continuous, then $\f$ is a universal approximator.&lt;/p&gt;

&lt;p class=&quot;proof&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxcosproof&quot;&gt;Proof&lt;/strong&gt;
  &lt;em&gt;(COMPLETE)&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;remunivapprox0&quot;&gt;Remark&lt;/strong&gt;
  Corollary: can you show that if sigmoids work, then ReLUs also work?&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;remunivapprox1&quot;&gt;Remark&lt;/strong&gt;
  The use of cosine activations is not standard in deep learning, although they have found use in some fantastic new applications in the context of solving partial differential equations&lt;sup id=&quot;fnref:siren&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:siren&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. Later we will explore other (theoretical) applications of cosines.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;remunivapprox2&quot;&gt;Remark&lt;/strong&gt;
  Notice here that these results are silent on how large $m$ needs to be in terms of $\varepsilon$. If we unpack terms carefully, we again see a scaling of $m$ with $O(\frac{1}{\varepsilon^d})$, similar to what we had before. (This property arises to due to the last property in Stone-Weierstass, i.e., closure under products.) The curse of dimensionality strikes yet again.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;remunivapprox3&quot;&gt;Remark&lt;/strong&gt;
  Somewhat curiously, if we use $\psi(\cdot)$ to be a polynomial activation function (of a &lt;em&gt;fixed-degree&lt;/em&gt;), then $\f$ is &lt;em&gt;not&lt;/em&gt; a universal approximator. Can you see why this is the case? (Hint: which property of Stone-Weierstrass is violated?)
  In fact, polynomial activations are the only ones which don’t work! $\f$ is a universal approximator &lt;em&gt;iff $\psi$ is non-polynomial&lt;/em&gt;; see Leshno et al. (1993)&lt;sup id=&quot;fnref:leshno&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:leshno&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; for a proof.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;barrons-method&quot;&gt;Barron’s method&lt;/h2&gt;

&lt;p&gt;Universal approximation results of the form discussed above are interesting but, at the end, not very satisfactory. Recall that we wanted to know if our prediction function can be simulated via a &lt;em&gt;succinct&lt;/em&gt; neural network. However, we could only muster a bound of $O(\frac{1}{\varepsilon^d})$.&lt;/p&gt;

&lt;p&gt;Can we do better than this? Maybe our original approach (trying to approximate all $L$-Lipschitz functions) was a bit too ambitious. Perhaps we want to narrow our focus down to a smaller target class (that are still rich enough to capture interesting function behavior). In any case, can we get &lt;em&gt;dimension-independent&lt;/em&gt; bounds on the number of neurons needed to approximate target functions?&lt;/p&gt;

&lt;p&gt;In a seminal paper&lt;sup id=&quot;fnref:barron&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:barron&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, Barron identified an interesting class of functions that can be indeed well-approximated with &lt;em&gt;small&lt;/em&gt; (still shallow) neural networks. Again, we have to pick up a few extra tools to establish this, so we will first state the main result, and then break down the proof.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;univapproxbarron&quot;&gt;Theorem&lt;/strong&gt;
  Suppose $g : \R^d \rightarrow \R$ is in $L_1$. Then, there exists a one-hidden-layer neural network $f$ with sigmoidal activations and $m$ hidden neurons such that:
  \(\int | f(x) - g(x) |^2 dx \leq \varepsilon\)
  where:
  \(m = \frac{C_g^2}{\varepsilon^2} .\)
  Here,
  \(C_g = \lVert \widehat{\nabla g} \rVert_1 = \int \lVert \widehat{\nabla g} \rVert d\omega\)
  is the $L_1$-norm of the &lt;em&gt;Fourier transform&lt;/em&gt; of the &lt;em&gt;gradient&lt;/em&gt; of $g$, and is called the &lt;em&gt;Barron norm&lt;/em&gt; of $g$:&lt;/p&gt;

&lt;p&gt;We will outline the proof of this theorem below, but first some reflections. Notice now that $m$ does &lt;em&gt;not explicitly depend&lt;/em&gt; on $d$; therefore, we escape the dreaded curse of dimensionality. As long as we control the Barron norm of $g$ to be something reasonable, we can succinctly approximate it using shallow networks.&lt;/p&gt;

&lt;p&gt;In his paper&lt;sup id=&quot;fnref:barron:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:barron&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, Barron shows that indeed Barron norms can be small for a large number of interesting target function classes – polynomials, sufficiently smooth functions, families such as Gaussian mixture models, even functions over discrete domains (such as decision trees).&lt;/p&gt;

&lt;p&gt;Second, the bound is an “existence”-style result. Somewhat interestingly, the proof will also reveal a &lt;em&gt;constructive&lt;/em&gt; (although unfortunately not very computationally friendly) approach to finding $f$. We will discuss this at the very end.&lt;/p&gt;

&lt;p&gt;Third, notice that the approximation error is measured in terms of the $L_2$ (squared difference) norm. This is due to the tools used in the proof; I’m not sure if there exist results for other norms (such as $L_\infty$).&lt;/p&gt;

&lt;p&gt;Lastly, other Barron style bounds assuring “dimension-free” convergence of representation error exist, using similar analysis techniques. See Jones&lt;sup id=&quot;fnref:jones&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:jones&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;, Girosi&lt;sup id=&quot;fnref:girosi&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:girosi&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;, and these lecture notes by Recht&lt;sup id=&quot;fnref:recht&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:recht&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Let’s now give a proof sketch of Barron’s &lt;a href=&quot;#univapproxbarron&quot;&gt;Theorem&lt;/a&gt;. We will be somewhat handwavy, focusing on intuition and being sloppy with derivations; for a more careful treatment, see Telgarsky’s notes&lt;sup id=&quot;fnref:mjt:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:mjt&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. The proof follows from two observations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Write out the function $g$ &lt;em&gt;exactly&lt;/em&gt; in terms of the &lt;em&gt;Fourier&lt;/em&gt; basis functions (with possibly infinitely many coefficients), and map this to an infinitely-wide neural network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Maurey’s empirical method (also sometimes called the “probabilistic method”), show that one can &lt;em&gt;sample&lt;/em&gt; from an appropriate distribution defined on the basis functions, and get a succinct (but good enough) approximation of $g$. Specifically, to get $\varepsilon$-accurate approximation, we need $m = O(\frac{1}{\varepsilon^2})$ samples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 class=&quot;label&quot; id=&quot;proof-part-1-fourier-decomposition&quot;&gt;Proof Part 1: Fourier decomposition&lt;/h3&gt;

&lt;p&gt;** &lt;em&gt;COMPLETE&lt;/em&gt; **.&lt;/p&gt;

&lt;h3 class=&quot;label&quot; id=&quot;proof-part-2-the-empirical-method-of-maurey&quot;&gt;Proof part 2: The empirical method of Maurey&lt;/h3&gt;

&lt;p&gt;** &lt;em&gt;COMPLETE&lt;/em&gt; **.&lt;/p&gt;

&lt;h3 class=&quot;label&quot; id=&quot;epilogue-a-constructive-approximation-via-frank-wolfe&quot;&gt;Epilogue: A constructive approximation via Frank-Wolfe&lt;/h3&gt;

&lt;p&gt;** &lt;em&gt;COMPLETE&lt;/em&gt; **.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;footnotes-and-references&quot;&gt;Footnotes and references&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:fn1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Although: exactly interpolating training labels seems standard in modern deep networks; see &lt;a href=&quot;https://paperswithcode.com/sota/image-classification-on-cifar-10&quot;&gt;here&lt;/a&gt; and Fig 1a  of &lt;a href=&quot;https://arxiv.org/pdf/1611.03530.pdf&quot;&gt;this paper&lt;/a&gt;. &lt;a href=&quot;#fnref:fn1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:devore&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;R. DeVore, B. Hanin, G. Petrova, &lt;a href=&quot;https://arxiv.org/pdf/2012.14501.pdf&quot;&gt;Neural network approximation&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:devore&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:mjt&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Telgarsky, &lt;a href=&quot;https://mjt.cs.illinois.edu/dlt/&quot;&gt;Deep Learning Theory&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:mjt&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:mjt:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bernstein&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Bernstein polynomials have several other practical use cases, including in computer graphics (see &lt;a href=&quot;https://en.wikipedia.org/wiki/B%C3%A9zier_curve&quot;&gt;Bezier curves&lt;/a&gt;). &lt;a href=&quot;#fnref:bernstein&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:hornik&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;K. Hornik, M. Stinchcombe, H. White, &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/0893608089900208&quot;&gt;Multilayer feedforward networks are universal approximators&lt;/a&gt;, 1989. &lt;a href=&quot;#fnref:hornik&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:hornik:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cybenko&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;G. Cybenko, &lt;a href=&quot;https://link.springer.com/article/10.1007/BF02551274&quot;&gt;Approximation by superpositions of a sigmoidal function&lt;/a&gt;, 1989. &lt;a href=&quot;#fnref:cybenko&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:funashashi&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;K. Funahashi, &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/0893608089900038&quot;&gt;On the approximate realization of continuous mappings by neural networks&lt;/a&gt;, 1989. &lt;a href=&quot;#fnref:funashashi&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:siren&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;V. Sitzmann, J. Martell, A. Bregman, D. Lindell, G. Wetzstein, &lt;a href=&quot;https://arxiv.org/abs/2006.09661&quot;&gt;Implicit Neural Representations with Periodic Activation Functions&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:siren&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:leshno&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Leshno, V. Lin, A. Pinkus, S. Schocken, &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315&quot;&gt;Multilayer feedforward networks with a nonpolynomial activation function can approximate any function&lt;/a&gt;, 1993. &lt;a href=&quot;#fnref:leshno&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:barron&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;A. Barron, &lt;a href=&quot;http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf&quot;&gt;Universal Approximation Bounds for Superpositions of a Sigmoidal Function&lt;/a&gt;, 1993. &lt;a href=&quot;#fnref:barron&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:barron:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:jones&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;L. Jones, &lt;a href=&quot;https://projecteuclid.org/journals/annals-of-statistics/volume-20/issue-1/A-Simple-Lemma-on-Greedy-Approximation-in-Hilbert-Space-and/10.1214/aos/1176348546.full&quot;&gt;A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates for Projection Pursuit Regression and Neural Network Training&lt;/a&gt;, 1992. &lt;a href=&quot;#fnref:jones&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:girosi&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;F. Girosi, &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-79119-2_8&quot;&gt;Regularization Theory, Radial Basis Functions and Networks&lt;/a&gt;, 1994. &lt;a href=&quot;#fnref:girosi&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:recht&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;B. Recht, &lt;a href=&quot;https://www.mit.edu/~9.520/spring08/Classes/recht_040708.pdf&quot;&gt;Approximation theory&lt;/a&gt;, 2008. &lt;a href=&quot;#fnref:recht&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 3 - The role of depth</title>
    <link href="http://localhost:4000/representation03/"/>
    <updated>2022-01-16T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/representation03</id>
    <content type="html">&lt;p&gt;For shallow networks, we now know upper bounds on the number of neurons required to “represent” data, where representation is measured either in the sense of exact interpolation (or memorization), or in the sense of universal approximation.&lt;/p&gt;

&lt;p&gt;We will continue to revisit these bounds when other important questions such as optimization/learning and out-of-sample generalization arise. In some cases, these bounds are even &lt;em&gt;tight&lt;/em&gt;, meaning that we could not hope to do any better.&lt;/p&gt;

&lt;p&gt;Which leads us to the natural next question:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Does depth buy us anything at all?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we have already gotten (some) tight results using shallow models, should there be any &lt;em&gt;theoretical&lt;/em&gt; benefits in pursuing analysis of deep networks? Two reasons why answering this question is important:&lt;/p&gt;

&lt;p&gt;One, after all, this is a course on “deep” learning theory, so we cannot avoid this question.&lt;/p&gt;

&lt;p&gt;But two, for the last decade or so, a lot of folks have been trying very hard to replicate the success of deep networks with &lt;em&gt;highly&lt;/em&gt; tuned shallow models (such as kernel machines), but so far have come up short. Understanding precisely why and where shallow models fall short (while deep models succeed) is therefore of importance.&lt;/p&gt;

&lt;p&gt;A full answer to the question that we posed above remains elusive, and indeed theory does not tell us too much about why depth in neural networks is important (or whether it is necessary at all!) See the last paragraph of Belkin’s monograph&lt;sup id=&quot;fnref:belkin&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:belkin&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; for some speculation. To quote another paper by Shankar et al.&lt;sup id=&quot;fnref:shankar&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:shankar&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“…the question remains open whether the performance gap between kernels and neural networks indicates a fundamental limitation of kernel methods or merely an engineering hurdle that can be overcome.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nonetheless: progress &lt;em&gt;can&lt;/em&gt; be made in some limited cases. In this note, we will visit several interesting results that highlight the importance of network depth in the context of representation power. Later we will revisit this in the context of optimization.&lt;/p&gt;

&lt;p&gt;Let us focus on “reasonable-width” networks of depth $L &amp;gt; 2$ (because we already know from universal approximation that exponential-width networks of depth-2 can represent pretty much anything we like.) There are two angles of inquiry:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Approach 1: prove that there exist datasets of some large enough size that can &lt;em&gt;only&lt;/em&gt; be memorized by networks of depth $\Omega(L)$, but not by networks of depth $o(L)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Approach 2: prove that there exist classes of functions that can be $\varepsilon$-approximated &lt;em&gt;only&lt;/em&gt; by networks of depth $\Omega(L)$, but not by networks of depth $o(L)$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Theorems of either type are called &lt;em&gt;depth separation&lt;/em&gt; results. Let us start with the latter.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;depth-separation-in-function-approximation&quot;&gt;Depth separation in function approximation&lt;/h2&gt;

&lt;p&gt;We will first prove a depth separation result for dense feed-forward networks with ReLU activations and univariate (scalar) inputs. This result will generally hold for other families of activations, but for simplicity let’s focus on the ReLU.&lt;/p&gt;

&lt;p&gt;We will explicitly construct a (univariate) function $g$ that can be exactly represented by a deep neural network (with error $\varepsilon = 0$) but is provably inapproximable by a much shallower network. This result is by Telgarsky&lt;sup id=&quot;fnref:telgarsky&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:telgarsky&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;DepthSeparation&quot;&gt;Theorem&lt;/strong&gt;
  There exists a function $g : [0,1] \rightarrow \R$ that is exactly realized by a ReLU network of constant width and depth $O(L^2)$, but for &lt;em&gt;any&lt;/em&gt; neural network $f$ with depth $\leq L$ and number of units, $\leq 2^{L^\delta}$ for any $0 &amp;lt; \delta \leq 1$, $f$ is at least $\varepsilon$-far from $g$, i.e.:&lt;/p&gt;

\[\int_0^1 |f(x) - g(x)| dx \geq \varepsilon\]

&lt;p class=&quot;theorem&quot;&gt;for some absolute constant $\varepsilon &amp;gt; \frac{1}{32}$.&lt;/p&gt;

&lt;p&gt;The proof of this theorem is elegant and will inform us also while proving memorization-style depth barriers. But let us first make several remarks on the implications of the results.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;DepthSepRem1&quot;&gt;Remark&lt;/strong&gt;
  The “hard” example function $g$ constructed in the above Theorem is for scalar inputs. What happens for the general case of $d$-variate inputs? Eldan and Shamir&lt;sup id=&quot;fnref:eldan&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:eldan&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; showed that there exist 3-layer ReLU networks (and $\text{poly}(d)$ width) that cannot be $\varepsilon$-approximated by any two-layer ReLU network unless they have $\Omega(2^d)$ hidden nodes. Therefore, there is already a separation between depth=2 and depth=3 in the high-dimensional case.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;DepthSepRem2&quot;&gt;Remark&lt;/strong&gt;
  In the general $d$-variate case, can we get depth separation results for networks of depth=4 or higher? Somewhat surprisingly, the answer appears to be &lt;em&gt;no&lt;/em&gt;. Vardi and Shamir&lt;sup id=&quot;fnref:vardi&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:vardi&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; showed that a depth separation theorem between ReLU networks of $k \geq 4$ and $k’ &amp;gt; k$ would imply progress on long-standing open problems in &lt;em&gt;circuit lower bounds&lt;/em&gt;&lt;sup id=&quot;fnref:razborov&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:razborov&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.  To be precise: this (negative) result only applies to vanilla dense feedforward networks. But it is disconcerting that even for the simplest of neural networks, proving clear benefits of depth remains outside the realm of current theoretical machinery.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;DepthSepRem3&quot;&gt;Remark&lt;/strong&gt;
  The “hard” example function $g$ constructed in the above Theorem is highly oscillatory within $[0,1]$ (see proof below). Therefore, it has an unreasonably large (super-polynomial) Lipschitz constant. Perhaps if we limited our attention to simple/natural Lipschitz functions, then it could be easier to prove depth-separation results? Not so: even if we only focused on “benign” functions (easy-to-compute functions with polynomially large Lipschitz constant), proving depth lower bonds would similarly imply progress in long-standing problems in computational complexity. See the recent result by Vardi et al.&lt;sup id=&quot;fnref:vardi2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:vardi2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;DepthSepRem4&quot;&gt;Remark&lt;/strong&gt;
  See this paper&lt;sup id=&quot;fnref:bengio&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bengio&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; for an earlier depth-separation result for sum-product networks (which are somewhat less standard architectures).&lt;/p&gt;

&lt;p&gt;Telgarsky’s proof uses the following high level ideas:&lt;/p&gt;

&lt;p&gt;(a) observe that any ReLU network $g$ simulates a piecewise linear function.&lt;/p&gt;

&lt;p&gt;(b) prove that the number of pieces in the range of $g$ grows only polynomially with width but exponentially in depth.&lt;/p&gt;

&lt;p&gt;(c) construct a “hard” function $g$ that has an exponential number of linear pieces, and that can be exactly computed by a deep network&lt;/p&gt;

&lt;p&gt;(d) but, from part (b), we know that a significantly shallower network cannot simulate so many pieces, thus giving our separation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Before diving into each of them, let us first define (and study) a simple “gadget” neural network that simulates a function $m : \R \rightarrow \R$ which will be helpful throughout. It’s easier to just draw $m(x)$ first:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/gadget.png&quot; alt=&quot;The sawtooth gadget.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and then observe that:&lt;/p&gt;

\[m(x) = \begin{cases}
0, \qquad \quad ~~ x &amp;lt; 0, \\
2x, \qquad \quad 0 \leq x &amp;lt; 1/2,\\
2 - 2x,~ 1/2 \leq x &amp;lt; 1,\\
0, \qquad \quad ~~ 1 \leq x.
\end{cases}\]

&lt;p&gt;What happens when we compose $m$ with itself several times? Define:&lt;/p&gt;

\[m^{(2)}(x) := m(m(x)), \ldots, m^{(L)}(x) := m(m^{(L-1)}(x)).\]

&lt;p&gt;Then, we start seeing an actual sawtooth function. For example, for $L=2$, we see:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/gadget2.png&quot; alt=&quot;The sawtooth function with $L = 2$.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Iterating this $L$ times, we get a sawtooth that oscillates a bunch of times over the interval $[0,1]$ and is zero outside&lt;sup id=&quot;fnref:fn1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fn1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;. In fact, for any $L$, an easy induction will show that there will be $2^{L-1}$ “triangles”, which are formed using $2^L$ pieces.&lt;/p&gt;

&lt;p&gt;But we also can observe that $m(x)$ can be written in terms of ReLUs. Specifically, if $\psi$ is the ReLU activation then:&lt;/p&gt;

\[m(x) = 2\left(\psi(x) - 2\psi(x-\frac{1}{2}) + \psi(x-1)\right),\]

&lt;p&gt;which is a tiny (width-3, depth-2) ReLU network. Therefore, $m^{(L)}(x)$ is a univariate function that is exactly written out as width-3, depth-$2L$ ReLU network for any $L$.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;bitextract&quot;&gt;Remark&lt;/strong&gt;
  One can view $m^{(L)}(x)$ as a “bit-extractor” function since we have the property that $m^{(L)}(x) = m(\text{frac}(2^{L-1} x))$, where $\text{frac}(x) = x - \lfloor x \rfloor$ is the fractional part of any real number $x$. (&lt;em&gt;Exercise: Prove this.&lt;/em&gt;) It is interesting that similar “bit-extractor” gadgets can be found in some earlier universal approximation proofs&lt;sup id=&quot;fnref:sieg&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sieg&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So: we have constructed a neural network with depth $O(L)$ which simulates a piecewise linear function over the real line with $2^L$ pieces. In fact, this observation can be generalized quite significantly as follows.&lt;/p&gt;

&lt;p class=&quot;lemma&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;numpieces&quot;&gt;Lemma&lt;/strong&gt;
  If $p(x)$ and $q(x)$ are univariate functions defined over $[0,1]$ with $s$ and $t$ linear pieces respectively, then:
      (a) $\alpha p(x) + \beta q(x)$ has at most $s + t$ pieces over $[0,1]$.
      (b) $q(p(x))$ has at most $st$ pieces over $[0,1]$.&lt;/p&gt;

&lt;p&gt;The proof of this lemma is an easy counting exercise over the number of “breakpoints” over $[0,1]$. Most relevant to our discussion, we immediately get the &lt;strong&gt;important&lt;/strong&gt; corollary (somewhat informally stated here, there may be hidden constants which I am ignoring):&lt;/p&gt;

&lt;p class=&quot;corollary&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;numpiecesrelu&quot;&gt;Corollary&lt;/strong&gt;
  For any feedforward ReLU network $f$ with depth $\leq L$ and width $\leq 2^{L^\delta}$, then the total number of pieces  in the range of $f$ is strictly smaller than $2^{L^2}$.&lt;/p&gt;

&lt;p&gt;We are now ready for the proof of the main &lt;a href=&quot;#DepthSeparation&quot;&gt;Theorem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;DepthSeparationProof&quot;&gt;Proof&lt;/strong&gt;
  Our target function $g$ will be the sawtooth function iterated $L^2 + 2$ times, i.e.,&lt;/p&gt;

\[g(x) = m^{(L^2 + 2)}(x).\]

&lt;p&gt;Using &lt;a href=&quot;#numpiecesrelu&quot;&gt;Corollary&lt;/a&gt; we will show that that if $f$ is any feedforard ReLU net with depth $\leq L$ and sub-exponential width, then $f$ cannot approximate a significant fraction of the pieces in $g$. In terms of picture, let us consider the following figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/triangleslice.png&quot; alt=&quot;Approximating the sawtooth.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where $f$ is in red and $g$ (the target) is in navy blue. Let’s draw the horizontal line $y = 1/2$ in purple. We can count the &lt;em&gt;total&lt;/em&gt; number of (tiny) triangles in $g$ to be (exactly) equal to&lt;/p&gt;

\[2^{L^2 + 2} - 1\]

&lt;p&gt;(the -1 is because the two edge cases have only a half-triangle each). Each of these tiny triangles have height 1/2, so their area is:&lt;/p&gt;

\[\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2^{L^2 + 2}} = 2^{-L^2 - 4} .\]

&lt;p&gt;But &lt;em&gt;any&lt;/em&gt; linear piece of $f$ has to have &lt;em&gt;zero&lt;/em&gt; intersection at least half of these triangles (since this linear piece can either be above the purple line or below, not both). Therefore, if we look at the error restricted to this particular piece, the area under this curve should be &lt;em&gt;at least&lt;/em&gt; the area of the missed triangles (shaded in blue). Therefore, the &lt;em&gt;total&lt;/em&gt; error is lower bounded as follows:&lt;/p&gt;

\[\begin{aligned}
  \int_0^1 |f - g | dx &amp;amp;\geq \text{\# missed triangles} \times \text{area of triangle} \\
  &amp;amp;&amp;gt; \frac{1}{2} \cdot 2^{L^2} \cdot 2^{-L^2 - 4} \\
  &amp;amp;= \frac{1}{32} .
  \end{aligned}\]

&lt;p class=&quot;proof&quot;&gt;This completes the proof.&lt;/p&gt;

&lt;p&gt;Let us reflect a bit more on the above proof. The key ingredient was the fact that superpositions (adding units, essentially increasing the “width”) only have a polynomial increase on the number of pieces in the range of $g$, but compositions (essentially increasing the “depth”) have an &lt;em&gt;exponential&lt;/em&gt; increase in the number of pieces.&lt;/p&gt;

&lt;p&gt;But! this “hard” function $g$, which is the sawtooth over $[0,1]$, was &lt;em&gt;very carefully constructed&lt;/em&gt;. To  achieve the exponential scaling law in the number of pieces, the breakpoints in $g$ &lt;em&gt;have&lt;/em&gt; to be exactly equispaced, and therefore the weights in every layer in the network have to be identical. Even a tiny perturbation to $g$ dramatically reduces the number of linear pieces in the range of the network. See the following figure illustrated in Hanin and Rolnick (2019)&lt;sup id=&quot;fnref:hanin&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:hanin&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/sawtooth.png&quot; alt=&quot;(left) The sawtooth function $g$, representable via a depth-$O(L^2)$, width-3 ReLU net. (right) Range of the same network as $g$ but with a tiny amount of noise added to its weights.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plot on the left is the sawtooth function $g$, which, as we proved earlier, is representable via a depth-$O(L^2)$, width-3 ReLU net. The plot on the right is the function implemented by the same network as $g$ but with a tiny amount of noise added to its weights. So even when we did get a depth-separation result, it’s not at all “robust”.&lt;/p&gt;

&lt;p&gt;All this to say: depth separation results for function approximation can be rather elusive; they seem to only exist for very special cases; and progress in this direction would result in several fundamental breakthroughs in complexity theory.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;depth-and-memorization&quot;&gt;Depth and memorization&lt;/h2&gt;

&lt;p&gt;We will now turn to Approach 1. Let’s say that our goal was a bit more modest, and merely wanted to memorize a bunch of $n$ training data points with $d$ input features. Recall that we already showed that $O(\frac{n}{d})$ neurons are sufficient to memorize these points using a “peeling”-style proof.&lt;/p&gt;

&lt;p&gt;Paraphrasing this fact: for depth-2 networks with $m$ hidden neurons, the memorization capacity is of the order of $d\cdot m$. This is roughly the same as the number of parameters in the network, so parameter counting intuitively tells us that we cannot do much better. What does depth $&amp;gt;2$ give us really?&lt;/p&gt;

&lt;p&gt;Several recent (very nice) papers have addressed this question. Let us start with the following result by Yun et al.&lt;sup id=&quot;fnref:yun&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:yun&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;ThreeLayerMemo&quot;&gt;Theorem&lt;/strong&gt;
  Let $X = \lbrace (x_i, y_i)_{i=1}^N \rbrace \subset \R^d \times \R$ be a dataset with distinct $x_i$ and $y_i \in [-1,1]$. Then, there exists a depth-3 ReLU network with hidden units $d_1, d_2$ where:&lt;/p&gt;

\[N \leq 4 \lceil \frac{d_1}{2} \rceil \lceil \frac{d_2}{2} \rceil .\]

&lt;p class=&quot;theorem&quot;&gt;that exactly memorizes $X$.&lt;/p&gt;

&lt;p&gt;The proof is somewhat involved, so we will give a brief sketch at the bottom of this page. But let us first study several implications of this result. First, we get the following corollary:&lt;/p&gt;

&lt;p class=&quot;corollary&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;RootN&quot;&gt;Corollary&lt;/strong&gt;
  (Informal) A depth-3 ReLU network with width $d_1 = d_2 = O(\sqrt{N})$ is sufficient to memorize $N$ points.&lt;/p&gt;

&lt;p&gt;This indicates a concrete separation in terms of memorization capacity between depth-2 and depth-3 networks. Suppose we focus on the regime where $d \ll N$. For this case, we have achieved a polynomial reduction in the &lt;em&gt;number of hidden neurons&lt;/em&gt; in the network from $O(N)$ in the depth-2 case to $O(\sqrt{N})$ in the depth-3 case.&lt;/p&gt;

&lt;p&gt;Notice that the number of &lt;em&gt;parameters&lt;/em&gt; in the network still remains $\Theta(N)$ (and the condition in the above &lt;a href=&quot;#ThreeLayerMemo&quot;&gt;Theorem&lt;/a&gt; ensures this, since the “middle” layer has $d_1 \cdot d_2 \gtrsim N$ connections.) But there are ancillary benefits in reducing the number of neurons themselves (for example, in the context of hardware implementation) which we won’t get into.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;multilabel&quot;&gt;Remark&lt;/strong&gt;
  A similar result on memorization capacity can be obtained for situations with multiple labels (e.g. in the multi-class classification setting). If the dimension of the label is $d_y &amp;gt; 1$, then the condition is that $d_1 d_2 \gtrsim N d_y$.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;multilabel3&quot;&gt;Remark&lt;/strong&gt;
  The multi-label case can be directly applied to provide width-lower bounds on memorizing popular datasets. For example, the well-known ImageNet dataset has about 10M image samples and performs classification over 1000 classes. The width bound suggests that we need networks of width no smaller than $\sqrt{10^7 \times 10^3} \approx 10^5$ ReLUs.&lt;/p&gt;

&lt;p&gt;Yun et al.&lt;sup id=&quot;fnref:yun:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:yun&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; also obtain a version of their result for depth-$L$ networks:&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;ThreeLayerMemo&quot;&gt;Theorem&lt;/strong&gt;
  Suppose a depth-$L$ ReLU network has widths of hidden layers $d_1, d_2, \ldots, d_L$ then its memorization capacity is lower bounded by:&lt;/p&gt;

\[N := d_1 d_2 + d_2 d_3 + \ldots d_{L-2}d_{L-1} ,\]

&lt;p class=&quot;theorem&quot;&gt;i.e., a network with this architecture can be tuned to memorize any dataset with at most $N$ points.&lt;/p&gt;

&lt;p&gt;This result, while holding for general $L$-hidden-layer networks, doesn’t unfortunately paint a full picture; the proof starts with the result for $L = 2$, and then proceeds to show that all labels can be successively memorized “layer-by-layer”. In particular, to memorize $N$ data points, the width requirement remains $O(\sqrt{N})$ and it is not entirely clear if depth plays a role. We will come back to this shortly.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Several interesting questions arise. First, tightness. The above &lt;a href=&quot;#ThreeLayerMemo&quot;&gt;Theorem&lt;/a&gt; shows that depth-2, width-$\sqrt{N}$ networks are sufficient to memorize training sets of size $N$. But is this width dependence of $\sqrt{N}$ also &lt;em&gt;necessary&lt;/em&gt;? Parameter counting suggests that this is indeed the case. Formally, Yun et al.&lt;sup id=&quot;fnref:yun:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:yun&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; provide an elegant proof for a (matching) lower bound:&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;ThreeLayerLB&quot;&gt;Theorem&lt;/strong&gt;
  Suppose a depth-$3$ ReLU network has widths of hidden layers $d_1, d_2$. If&lt;/p&gt;

\[2 d_1 d_2 + d_2 + 2 &amp;lt; N,\]

&lt;p class=&quot;theorem&quot;&gt;then there exists a dataset with $N$ points that cannot be memorized.&lt;/p&gt;

&lt;p class=&quot;proof&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;ThreeLayerLBProof&quot;&gt;Proof&lt;/strong&gt;
  &lt;strong&gt;&lt;em&gt;(Complete)&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Observe that this does not directly give lower bounds on the number of &lt;em&gt;parameters&lt;/em&gt; needed to memorize data. We will come back to this question below.&lt;/p&gt;

&lt;p&gt;Next, extensions to other networks. The above results are for feedforward ReLU networks; Yun et al.&lt;sup id=&quot;fnref:yun:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:yun&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; also showed this for “hard-tanh” activations (which is a “clipped” ReLU activation that saturates at +1 for $x \geq 1$). Similar results (with number of connections approximately equal to the number of data points) have been obtained for polynomial networks&lt;sup id=&quot;fnref:ge&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ge&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; and residual networks&lt;sup id=&quot;fnref:resnet&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:resnet&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;What about more exotic families of networks? Could it be that there is some yet-undiscovered model that may give better memorization?&lt;/p&gt;

&lt;p&gt;In a very nice (and surprisingly general) result, Vershynin&lt;sup id=&quot;fnref:vershynin&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:vershynin&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; showed the following:&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;VershLLayer&quot;&gt;Theorem&lt;/strong&gt;
  (Informal) Consider well-separated data of unit norm and binary labels. Consider depth-$L$ networks ($L \geq 3$) with arbitrary activations across neurons but without exponentially narrow bottlenecks. If the number of “wires” in the second layer of a network and later:&lt;/p&gt;

\[W := d_1 d_2 + d_2 d_3 + \ldots + d_{L-1} d_L ,\]

&lt;p&gt;is slightly larger than $N$, specifically:&lt;/p&gt;

\[W \geq N \log^5 N ,\]

&lt;p&gt;then some choice of weights can memorize this dataset.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;The high level idea in the proof of this result is to use the first layer as a preconditioner that separates data points into an almost-orthogonal set (in fact, a simple random Gaussian projection layer will do), and then any sequence of final layers that will memorize label assignments.&lt;/p&gt;

&lt;p&gt;The precise definitions of “well-separatedness” and “bottlenecks” can be found in the paper, but the key here is that this bound is independent of depth, choice of activations (whether ReLU or threshold or some mixture of both), and any other architectural details. Again, we see that there doesn’t seem to be a clear impact of the depth parameter $L$ on network capacity.&lt;/p&gt;

&lt;p&gt;For a more precise discussion along these lines, see the review section of a very nice (and recent) paper by Rajput et al.&lt;sup id=&quot;fnref:rajput&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:rajput&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;depth-versus-number-of-parameters&quot;&gt;Depth versus number of parameters&lt;/h2&gt;

&lt;p&gt;In the above discussion we saw that that moving from depth-2 to depth-3 networks helped significantly reduced the number of &lt;em&gt;neurons&lt;/em&gt; needed to memorize $N$ arbitrary data points from $O(N)$ to $O(\sqrt{N})$ in several cases. The number of &lt;em&gt;parameters&lt;/em&gt; in all these constructions remained $O(N)$ (but no better).&lt;/p&gt;

&lt;p&gt;Is this the best possible we can do, or does depth help? We already encountered the result by Sontag&lt;sup id=&quot;fnref:sontag&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sontag&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; showing an $\Omega(N)$ lower bound; specifically, given any network with sub-linear ($o(N)$) parameters, there exists at least one “worst-case” dataset that cannot be memorized.&lt;/p&gt;

&lt;p&gt;Maybe our definition of memorization is too pessimistic, and we don’t have to fret about worst case behavior? Let us contrast Sontag’s result with other lower bounds from learning theory. Until now, we have quantified the memorization capacity of a network in terms of its ability to exactly interpolate &lt;em&gt;any&lt;/em&gt; dataset. But we can weaken this definition a bit.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;VC-dimension&lt;/em&gt; of any family of models is defined as the maximum number of data points that the model can “shatter” (i.e., exactly interpolate labels). Notice that this is a “best-case” definition; if the VC dimension is $N$ there should exist at least one dataset of $N$ points (with arbitrary labels) that the network is able to memorize.&lt;/p&gt;

&lt;p&gt;Existing VC dimension bounds state that if a network architecture has $W$ weights then the VC dimension is no greater than $O(W^2)$&lt;sup id=&quot;fnref:bartlett&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bartlett&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt; &lt;em&gt;no matter the depth&lt;/em&gt;. Therefore, in the “best-case” scenario, to memorize $N$ samples with arbitrary labels, we would require at least $\Omega(\sqrt{N})$ parameters, and we could not really hope to do any better&lt;sup id=&quot;fnref:bartlett2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bartlett2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Can we reconcile this gap between the “best” and “worst” cases of memorization capacity? In a very recent result, Vardi et al.&lt;sup id=&quot;fnref:vardi:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:vardi&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; have been able to show that the $\sqrt{N}$ dependence is in fact tight (up to log factors). Under the assumption that the data is bounded norm and well-separated, then a width-12, depth-$\tilde{O}(\sqrt{N})$ network with $\tilde{O}(\sqrt{N})$ parameters can memorize any dataset. This result improves upon a previous result&lt;sup id=&quot;fnref:yun2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:yun2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt; that had initially achieved a sub-linear upper bound of $O(N^{\frac{2}{3}})$ parameters.&lt;/p&gt;

&lt;p&gt;The proof of this result is somewhat combinatorial in nature. We see (again!) the bit-extractor gadget network being used here. There is another catch: the &lt;em&gt;bit complexity&lt;/em&gt; of the network is very large (it scales as $\sqrt{N}$), so the price to pay for a very small number of weights is that we end up stuffing many more bits in each weight.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;proof-of-3-layer-memorization-capacity&quot;&gt;Proof of 3-layer memorization capacity&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;(COMPLETE)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:belkin&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Mikhail Belkin, &lt;a href=&quot;https://arxiv.org/pdf/2105.14368.pdf&quot;&gt;Fit without Fear&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:belkin&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:shankar&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;V. Shankar, A. Fang, W. Guo, S. Fridovich-Keil, L. Schmidt, J. Ragan-Kelley, B. Recht, &lt;a href=&quot;https://arxiv.org/abs/2003.02237&quot;&gt;Neural Kernels without Tangents&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:shankar&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:telgarsky&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Telgarsky, &lt;a href=&quot;http://proceedings.mlr.press/v49/telgarsky16.pdf&quot;&gt;Benefits of depth in neural networks&lt;/a&gt;, 2016. &lt;a href=&quot;#fnref:telgarsky&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:eldan&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;R. Eldan and O. Shamir, &lt;a href=&quot;http://proceedings.mlr.press/v49/eldan16.pdf&quot;&gt;The Power of Depth for Feedforward Neural Networks&lt;/a&gt;, 2016. &lt;a href=&quot;#fnref:eldan&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vardi&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;G. Vardi, G. Yehudai, O. Shamir, &lt;a href=&quot;https://arxiv.org/pdf/2110.03187.pdf&quot;&gt;On the Optimal Memorization Power of ReLU Neural Networks&lt;/a&gt;, 2022. &lt;a href=&quot;#fnref:vardi&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:vardi:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:razborov&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;A. Razborov and S. Rudich. &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S002200009791494X&quot;&gt;Natural proofs&lt;/a&gt;, 1997. &lt;a href=&quot;#fnref:razborov&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vardi2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;G. Vardi, D. Reichmann, T. Pitassi, and O. Shamir, &lt;a href=&quot;http://proceedings.mlr.press/v134/vardi21a/vardi21a.pdf&quot;&gt;Size and Depth Separation in Approximating Benign Functions with Neural Networks&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:vardi2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bengio&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;O. Delalleau and Y. Bengio, &lt;a href=&quot;https://papers.nips.cc/paper/2011/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf&quot;&gt;Shallow vs. Deep Sum-Product Networks&lt;/a&gt;, 2011. &lt;a href=&quot;#fnref:bengio&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fn1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Somewhat curiously, these kinds of oscillatory (“sinusoidal”/periodic) functions are common occurrences while proving cryptographic lower bounds for neural networks. See, for example, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/f78688fb6a5507413ade54a230355acd-Abstract.html&quot;&gt;Song, Zadik, and Bruna&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:fn1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sieg&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;H. Siegelmann and D. Sontag, &lt;a href=&quot;https://binds.cs.umass.edu/papers/1992_Siegelmann_COLT.pdf&quot;&gt;On the Computational Power of Neural Networks&lt;/a&gt;, 1992. &lt;a href=&quot;#fnref:sieg&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:hanin&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;B. Hanin and D. Rolnick, &lt;a href=&quot;https://arxiv.org/pdf/1901.09021.pdf&quot;&gt;Complexity of Linear Regions in Deep Networks&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:hanin&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:yun&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Yun, A. Jadbabaie, S. Sra, &lt;a href=&quot;https://arxiv.org/pdf/1810.07770.pdf&quot;&gt;Small ReLU Networks are Powerful Memorizers&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:yun&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:yun:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:yun:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:yun:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ge&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;R. Ge, R. Wang, H. Zhao, &lt;a href=&quot;https://arxiv.org/pdf/1909.11837.pdf&quot;&gt;Mildly Overparametrized Neural Nets can Memorize Training Data Efficiently&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:ge&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:resnet&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Hardt and T. Ma, &lt;a href=&quot;https://openreview.net/forum?id=ryxB0Rtxx&quot;&gt;Identity Matters in Deep Learning&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:resnet&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vershynin&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;R. Vershynin, &lt;a href=&quot;https://arxiv.org/pdf/2001.06938.pdf&quot;&gt;Memory capacity of neural networks with threshold and ReLU activations&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:vershynin&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rajput&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Rajput, K. Sreenivasan, D. Papailiopoulos, A. Karbasi, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/69dd2eff9b6a421d5ce262b093bdab23-Paper.pdf&quot;&gt;An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:rajput&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sontag&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;E. Sontag, &lt;a href=&quot;http://www.sontaglab.org/FTPDIR/generic.pdf&quot;&gt;Shattering All Sets of k Points in “General Position” Requires (k − 1)/2 Parameters&lt;/a&gt;, 1997. &lt;a href=&quot;#fnref:sontag&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bartlett&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;P. Bartlett, V. Maiorov, R. Meir, &lt;a href=&quot;https://proceedings.neurips.cc/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf&quot;&gt;Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks &lt;/a&gt;, 1998. &lt;a href=&quot;#fnref:bartlett&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bartlett2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Aside: this is not quite precise; a sharper VC dimension bound of $O(WL \log W)$ can be obtained for depth-$L$ networks. See P. Bartlett, N. Harvey, C. Liaw, A. Mehrobian, &lt;a href=&quot;https://www.jmlr.org/papers/volume20/17-612/17-612.pdf&quot;&gt;Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:bartlett2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:yun2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Park, J. Lee, C. Yun, J. Shin, &lt;a href=&quot;https://arxiv.org/pdf/2010.13363.pdf&quot;&gt;Provable Memorization via Deep Neural Networks using Sublinear Parameters&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:yun2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 4 - A primer on optimization</title>
    <link href="http://localhost:4000/optimization01/"/>
    <updated>2022-01-15T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/optimization01</id>
    <content type="html">&lt;p&gt;In the first few chapters, we covered several results related to the representation power of neural networks. We obtained estimates — sometimes tight ones — for the sizes of neural networks needed to memorize a given dataset, or to simulate a target prediction function.&lt;/p&gt;

&lt;p&gt;However, most of our theoretical results used somewhat funny-looking constructions of neural networks. Our theoretically best-performing networks were all either too wide or too narrow, and didn’t really look like the typical deep networks that we see in practice.&lt;/p&gt;

&lt;p&gt;But even setting aside the issue of size, none of the (theoretically attractive) techniques that we used to memorize datasets resemble deep learning practice. When folks refer to “training models”, they almost always are talking about fitting datasets to neural networks via local, greedy, first-order algorithms such as gradient descent (GD), or stochastic variations (like SGD), or accelerations (like Adam), or some other such approach.&lt;/p&gt;

&lt;p&gt;In the next few chapters, we address the question:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Do practical approaches for model training work well?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This question is once again somewhat ill-posed, and we will have to be precise about what “work” and “well” mean in the above question. (From the practical perspective: an overwhelming amount of empirical evidence seems to suggest that they work just fine, as long as certain tricks/hacks are applied.)&lt;/p&gt;

&lt;p&gt;While there is very large variation in the way we train deep networks, we will focus on a handful of the most canonical settings, analyze them, and derive precise bounds on their behavior. The hope is that such analysis can illuminate differences/tradeoffs between different choices and provide useful thumb rules for practice.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Most (all?) popular approaches in deep learning do the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Write down the loss (or empirical risk) in terms of the training data points and the weights. (Usually the loss is decomposable across the data points). So if the data is $(x_i,y_i)_{i=1}^n$ then the loss looks something like:&lt;/p&gt;

\[L(w) = \frac{1}{n} \sum_{i=1}^n l(y_i,\hat{y_i}), \quad \hat{y_i} = f_w(x_i),\]

    &lt;p&gt;where $l(\cdot,\cdot) : \R \times \R \rightarrow \R_{\geq 0}$ is a non-negative measure of label fit, and $f_w$ is the function represented by the neural network with weight/bias parameters $w$. We are abusing notation here since we previously used $R$ for the risk, but let’s just use $L$ to denote loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then, we seek the weights/biases $w$ that optimize the loss:&lt;/p&gt;

\[\hat{w} = \arg \min_w L(w) .\]

    &lt;p&gt;Sometimes, we throw in an extra regularization term defined on $w$ for kicks, but let’s just stay simple for now.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Importantly, invariably, the above optimization is carried out using a &lt;em&gt;simple, iterative, first-order method&lt;/em&gt;. For example, if gradient descent (GD) is the method of choice, then discovering $\hat{w}$ amounts to iterating the following recursion:&lt;/p&gt;

\[w \leftarrow w - \eta \nabla_w L(w)\]

    &lt;p&gt;some number of times. Or, if SGD is the method of choice, then discovering $\hat{w}$ amounts to iterating the following recursion:&lt;/p&gt;

\[w \leftarrow w - \eta g_w\]

    &lt;p&gt;where $g_w$ is some (properly defined) stochastic approximation of $\nabla L(w)$. Or if Adam&lt;sup id=&quot;fnref:adam&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:adam&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; is the method of choice, then discovering $\hat{w}$ amounts to iterating…a slightly more &lt;a href=&quot;https://chinmayhegde.github.io/dl-notes/notes/lecture03/&quot;&gt;complicated recursion&lt;/a&gt;, but still a first-order method involving gradients and nothing more. You get the picture.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this chapter, let us focus on GD and SGD. The following questions come to mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do GD and SGD converge at all.&lt;/li&gt;
  &lt;li&gt;If so, do they converge to the set (or &lt;em&gt;a&lt;/em&gt; set?) of globally optimal weights.&lt;/li&gt;
  &lt;li&gt;How many steps do they need to converge reliably.&lt;/li&gt;
  &lt;li&gt;How to pick step size, or (in the case of SGD) batch size, or other parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;among many others.&lt;/p&gt;

&lt;p&gt;Before diving in to the details, let us qualitatively address the first couple of questions.&lt;/p&gt;

&lt;p&gt;Intuition tells us that convergence in a &lt;em&gt;local&lt;/em&gt; sense is to be expected, but converegence to &lt;em&gt;global&lt;/em&gt; minimizers seems unlikely. Indeed, it is easy to prove that for anything beyond the simplest neural networks, the loss function $L(w)$ is &lt;em&gt;extremely non-convex&lt;/em&gt;. Therefore the “loss landscape” of $L(w)$, viewed as a function of $w$, has many peaks, valleys, and ridges, and a myopic first-order approach such as GD may be very prone to get stuck in local optima, or saddle points, or other stationary points. A fantastic paper&lt;sup id=&quot;fnref:losslandscape&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:losslandscape&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; by Xu et al. proposes creative ways of visualizing these high-dimensional landscapes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/losslandscape.png&quot; alt=&quot;Loss landscapes of ResNet-56. Adding skips significantly influences ease of optimization, since the landscape is much better behaved.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Somewhat fascinatingly, however, it turns out that this intuition may not be correct. GD/SGD &lt;em&gt;can&lt;/em&gt; be used to train models all the way down to &lt;em&gt;zero&lt;/em&gt; train error (at least, this is common for deep networks used in classification.) This fact seems to have been folklore, but was systematically demonstrated in a series of interesting experiments by Zhang et al.&lt;sup id=&quot;fnref:zhang&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:zhang&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We will revisit this fact in the next two chapters. But for now, we limit ourselves to analyzing the local convergence behavior of GD/SGD. We establish the more modest claim:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(S)GD converges to (near) stationary points, provided $L(w)$ is smooth.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The last caveat — that $L(w)$ is required to be smooth — is actually a rather significant one, and excludes several widely used architectures used in practice. For example, the ubiquitous ReLU activation function, $\psi(z) = \max(z,0)$, is not smooth, and therefore neural networks involving ReLUs don’t lead to smooth losses.&lt;/p&gt;

&lt;p&gt;This should not deter us. The analysis is still very interesting and useful; it is still applicable to other widely used architectures; and extensions to ReLUs can be achieved with a bit more technical heavy lifting (which we won’t cover here). For a more formal treatment of local convergence in networks with nonsmooth activations, see, for example, the paper by Ji and Telgarsky&lt;sup id=&quot;fnref:ji&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ji&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, or these lecture notes&lt;sup id=&quot;fnref:mjt&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:mjt&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h2&gt;

&lt;p&gt;We start with analyzing gradient descent for smooth loss functions. (As asides, we will also discuss application of our results to &lt;em&gt;convex&lt;/em&gt; functions, but these functions are not common in deep learning and therefore not our focus.)&lt;/p&gt;

&lt;p&gt;Let us first define smoothness. (All norms below refer to the 2-norm, and all gradients are with respect to the parameters.)&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;Smoothness&quot;&gt;Definition&lt;/strong&gt;
  $L$ is said to be $\beta$-smooth if $L$ has $\beta$-Lipschitz gradients:&lt;/p&gt;

\[\lVert \nabla L(w) - \nabla L(u) \rVert \leq \beta \lVert w - u \rVert .\]

&lt;p class=&quot;definition&quot;&gt;With &lt;a href=&quot;https://xingyuzhou.org/blog/notes/Lipschitz-gradient&quot;&gt;some algebra&lt;/a&gt;, one can arrive at the following lemma.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;Quadratic&quot;&gt;Lemma&lt;/strong&gt;
  If $L$ is twice-differentiable and $\beta$-smooth, then the eigenvalues of its Hessian are less than $\beta$:&lt;/p&gt;

\[\nabla^2 L(w) \preceq \beta I.\]

&lt;p&gt;Equivalently, $L(w)$ is &lt;em&gt;upper-bounded&lt;/em&gt; by a quadratic function:&lt;/p&gt;

\[L(w) \leq L(u) + \langle \nabla L(u), w-u \rangle + \frac{\beta}{2} \lVert w-u \rVert^2 .\]

&lt;p class=&quot;lemma&quot;&gt;for all $w,u$.&lt;/p&gt;

&lt;p&gt;Basically, the smoothness condition (or its implications according to the above &lt;a href=&quot;#Quadratic&quot;&gt;Lemma&lt;/a&gt;) says that if $\beta$ is not unreasonably big, then the gradients of $L(w)$ are rather well-behaved.&lt;/p&gt;

&lt;p&gt;It is natural to see why something like this condition is needed to analyze GD. If smoothness did not hold and the gradient was not well-behaved, then first-order methods such as GD are not likely to be very informative.&lt;/p&gt;

&lt;p&gt;There is a second natural reason why this definition is relevant to GD. Imagine, for a moment, not minimizing $L(w)$, but rather minimizing the &lt;em&gt;upper bound&lt;/em&gt;:&lt;/p&gt;

\[B(w) := L(u) + \langle \nabla L(u), w-u \rangle + \frac{\beta}{2} \lVert w-u \rVert^2 .\]

&lt;p&gt;This is a convex (in fact, quadratic) function of $w$. Therefore, it can be optimized very easily by setting $\nabla B(w)$ to zero and solving for $w$:&lt;/p&gt;

\[\begin{aligned}
\nabla B(w) &amp;amp;= 0, \\
\nabla L(u) &amp;amp;+ \beta (w - u) = 0, \qquad \text{and thus} \\
w &amp;amp;= u - \frac{1}{\beta} \nabla L(u) .
\end{aligned}\]

&lt;p&gt;This is the same as a &lt;em&gt;single&lt;/em&gt; step of gradient descent starting from $u$ (with step size inversely proportional to the smoothness parameter). In other words, gradient descent is nothing but the successive optimization of a Lipschitz upper bound of &lt;em&gt;in every iteration&lt;/em&gt;&lt;sup id=&quot;fnref:oco&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:oco&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We are now ready to prove our first result.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GDLipchitz&quot;&gt;Theorem&lt;/strong&gt;
  If $L$ is $\beta$-smooth, then GD with fixed step size converges to stationary points.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;GDLipchitzProof&quot;&gt;Proof&lt;/strong&gt;
  Consider any iteration of GD (with a step size $\eta$ which we will specify shortly):&lt;/p&gt;

\[w = u - \eta \nabla L(u) .\]

&lt;p&gt;This means that:&lt;/p&gt;

\[w - u = - \eta \nabla L(u).\]

&lt;p&gt;Plug this value of $w-u$ into the quadratic upper bound to get:&lt;/p&gt;

\[\begin{aligned}
  L(w) &amp;amp;\leq L(u) - \eta \lVert \nabla L(u) \rVert^2 + \frac{\beta \eta^2}{2} \lVert \nabla L(u) \rVert^2, \quad \text{or} \\
  L(w) &amp;amp;\leq L(u) - \eta \left( 1 - \frac{\beta \eta}{2} \right) \lVert \nabla L(u) \rVert^2 .
  \end{aligned}\]

&lt;p&gt;This inequality already gives a proof of convergence. Suppose that the step size is small enough such that $\eta &amp;lt; \frac{2}{\beta}$. We are in one of two situations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Either $\nabla L(u) = 0$, in which case we are done — since $u$ is a stationary point.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Or $\nabla L(u) \neq 0$, in which case $\lVert \nabla L(u) \rVert &amp;gt; 0$ and the second term in the right hand side is strictly positive. Therefore GD makes progress (and decreases $L$ in the next step).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since $L$ is lower bounded by 0 (since we have assumed a non-negative loss), we get &lt;a href=&quot;https://en.wikipedia.org/wiki/Monotone_convergence_theorem&quot;&gt;convergence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This argument does not quite tell us how &lt;em&gt;many&lt;/em&gt; iterations are needed by GD. To estimate this, let us just set $\eta = \frac{1}{\beta}$. This choice is not precisely necessary to get similar bounds, but the algebra becomes simpler. Let’s just rename $w_i := u$ and $w_{i+1} := w$. Then, the last inequality becomes:&lt;/p&gt;

\[\frac{1}{2\beta} \lVert \nabla L(w_i) \rVert^2 \leq L(w_i) - L(w_{i+1}).\]

&lt;p&gt;Telescoping from $w_0, w_1, \ldots, w_t$, we get:&lt;/p&gt;

\[\begin{aligned}
  \frac{1}{2\beta} \sum_{i = 0}^{t-1} \lVert \nabla L(w_i) \rVert^2 &amp;amp;\leq L(w_0) - L(w_t) \\
  &amp;amp;\leq L(w_0) - L_{\text{opt}}
  \end{aligned}\]

&lt;p&gt;(since $L_\text{opt}$ is the smallest achievable loss.) Therefore, if we pick $i = \arg \min_{i &amp;lt; t} \nabla \lVert L(w_i) \rVert^2$ as the estimate with lowest gradient norm and set $\hat{w} = w_{i}$, then we get:&lt;/p&gt;

\[\frac{t}{2\beta} \lVert \nabla L(\hat{w}) \rVert^2 \leq L_0 - L_{\text{opt}},\]

&lt;p&gt;which implies that if $L_0$ is bounded (i.e.: we start somewhere reasonable) then GD reaches a point $\hat{w}$ within at most $t$ iterations whose gradient norm is&lt;/p&gt;

\[\lesssim \sqrt{\frac{\beta}{t}}\]

&lt;p&gt;at most. To put it a different way, to find an $\varepsilon$-stationary point, GD needs:&lt;/p&gt;

\[O\left( \frac{\beta}{\varepsilon^2} \right)\]

&lt;p class=&quot;proof&quot;&gt;iterations.&lt;/p&gt;

&lt;p&gt;Notice that the proof is simple: we didn’t use much information beyond the definition of Lipschitz smoothness. But it already reveals a lot.&lt;/p&gt;

&lt;p&gt;First, step sizes in standard GD can be set to a constant. Later, we will analyze SGD (where step sizes have to be variable.)&lt;/p&gt;

&lt;p&gt;Second, step sizes should be chosen inversely proportional to $\beta$. This makes intuitive sense: if $\beta$ is large then gradients are wiggling around, and therefore it is prudent to take small steps. On the other hand, it is not easy to estimate Lipschitz smoothness constants (particularly for neural networks), so in practice $\eta$ is just tuned by hand.&lt;/p&gt;

&lt;p&gt;Third, we only get convergence in the “neighborhood” sense (in that there is some point along the trajectory which is close to the stationary point). It is harder to prove “last-iterate” convergence results. In fact, one can even show that GD can go near a stationary point, spend a very long time near this point, but then bounce away later&lt;sup id=&quot;fnref:leegd&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:leegd&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Fourth, we get $\frac{1}{\sqrt{t}}$ error after $t$ iterations. The terminology to describe this error rate is not very consistent in the optimization literature, but one might call this a “sub-linear” rate of convergence.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Let us now take a lengthy detour into classical optimization. What if $L$ were smooth &lt;em&gt;and convex&lt;/em&gt;? As mentioned above, convex losses (as a function of the weights) are not common in deep learning; but it is instructive to understand how much convexity can buy us.&lt;/p&gt;

&lt;p&gt;Life is much simpler now, since we can expect GD to find a &lt;em&gt;global&lt;/em&gt; minimizer if $L$ is convex (i.e., not just a point where $\nabla L \approx 0$, but actually a point where $L \approx L_{\text{opt}}$).&lt;/p&gt;

&lt;p&gt;How do we show this? While smoothness shows that $L$ is upper bounded by a quadratic function, convexity implies that $L$ is also &lt;em&gt;lower bounded&lt;/em&gt; by tangents at every point. The picture looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/smoothconvex.png&quot; alt=&quot;Smoothness and convexity.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and mathematically, we have:&lt;/p&gt;

\[L(w) \geq L(u) + \langle \nabla L(u), w-u \rangle .\]

&lt;p&gt;This lets us control not just $\nabla L$ but $L$ itself. Formally, we obtain:&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GDConvex&quot;&gt;Theorem&lt;/strong&gt;
  If $L$ is $\beta$-smooth and convex, then GD with fixed step size converges to a minimizer.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;GDConvexProof&quot;&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $w^*$ be some minimizer, which achieves loss $L_{\text{opt}}$.&lt;/p&gt;

&lt;p&gt;(&lt;em&gt;Q. What if there is more than one minimizer? Good question!&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Set $\eta = \frac{1}{\beta}$, as before. We will bound the error in weight space as follows:&lt;/p&gt;

\[\begin{aligned}
  \lVert w_{i+1} - w^* \rVert^2 &amp;amp;= \lVert w_i - \frac{1}{\beta} \nabla L(w_i) - w^* \rVert^2 \\
  &amp;amp;= \lVert w_i - w^* \rVert^2 - \frac{2}{\beta} \langle \nabla L(w_i), w_i - w^* \rangle + \frac{1}{\beta^2} \lVert \nabla L(w_i) \rVert^2 . \\
  \end{aligned}\]

&lt;p&gt;From the smoothness proof above, we already showed that&lt;/p&gt;

\[\lVert \nabla L(w_i) \rVert^2 \leq 2 \beta \left(L(w_i) - L(w_{i+1})\right).\]

&lt;p&gt;Moreover, plugging in $u = w_i$ and $w = w^*$ in the convexity lower bound, we get:&lt;/p&gt;

\[\langle \nabla L(w_i), w^* - w_i \rangle \leq L_{\text{opt}} - L(w_i) .\]

&lt;p&gt;Therefore, we can bound both the rightmost terms in the weight error:&lt;/p&gt;

\[\begin{aligned}
  \lVert w_{i+1} - w^* \rVert^2 &amp;amp;\leq \lVert w_i - w^* \rVert^2 + \frac{2}{\beta} \left(L(w_i) - L(w_{i+1}) + L_{\text{opt}} - L(w_i) \right) \\
  &amp;amp;= \lVert w_i - w^* \rVert^2 - \frac{2}{\beta} (L_{i+1} - L_{\text{opt}}).
  \end{aligned}\]

&lt;p&gt;Therefore, we can invoke a similar argument as in the smoothness proof. One of two situations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Either $L_{i+1} = L_{\text{opt}}$, which means we have achieved a point with optimal loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Or, $L_{i+1} &amp;gt; L_{\text{opt}}$, which means GD decreases weight error.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore, we get convergence. In order to estimate the number of iterations, rearrange terms:&lt;/p&gt;

\[L_{i+1} - L_{\text{opt}} \leq \frac{\beta}{2} \left( \lVert w_i - w^* \rVert^2 - \lVert w_{i+1} - w^* \rVert^2 \right)\]

&lt;p&gt;and telescope $i$ from 0 to $t-1$ to get:&lt;/p&gt;

\[\sum_{i=0}^{t-1} L_i - t L_{\text{opt}} \leq \frac{\beta}{2} \left( \lVert w_0 - w^* \rVert^2 - \lVert w_t - w^* \rVert^2 \right)\]

&lt;p&gt;which gives:&lt;/p&gt;

\[\begin{aligned}
  L_t &amp;amp;\leq \frac{1}{t} \sum_{i=0}^{t-1} L_i \\
  &amp;amp;\leq L_{\text{opt}} + \frac{\beta}{2t} \lVert w_0 - w^* \rVert^2 .
  \end{aligned}\]

&lt;p class=&quot;proof&quot;&gt;Therefore, the optimality gap decreases as $1/t$ and to find an $\varepsilon$-approximate point (assuming we start somewhere reasonable), GD needs $O(\frac{\beta}{\varepsilon})$ iterations.&lt;/p&gt;

&lt;p&gt;Similar conclusions as above. Constant step size suffices for GD. Step size should be inversely proportional to smoothness constant. Convexity gives us a “last-iterate” bound, as well as parameter estimation guarantees.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Another aside: $L$ smooth and &lt;em&gt;strongly convex&lt;/em&gt;? Then $L$ is both lower and upper bounded by quadratics. Therefore, optimization is easy; exponential ($e^{-t}$) rate of convergence. &lt;em&gt;Fill this in&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So the hierarchy is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GD assuming smoothness: $\frac{1}{\sqrt{t}}$ rate&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GD assuming smoothness + convexity: $\frac{1}{t}$ rate&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Momentum accelerated GD: $\frac{1}{t^2}$ rate. We won’t prove this; see the paper by Nesterov. Remarkably, this is the &lt;em&gt;best possible&lt;/em&gt; one can do with first-order methods such as GD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GD assuming smoothness and strong convexity: $\exp(-t)$ rate .&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 class=&quot;label&quot; id=&quot;the-polyak-lojasiewicz-pl-condition&quot;&gt;The Polyak-Lojasiewicz (PL) condition&lt;/h3&gt;

&lt;p&gt;Above, we saw how leveraging smoothness, along with (strong) convexity, of the loss results in exponential convergence of GD. However (strong) convexity is not that relevant in the context of deep learning. This is because losses are very rarely convex in their parameters.&lt;/p&gt;

&lt;p&gt;However, there is a different characterization of functions (other than convexity) that also implies fast convergence rates of GD. This property was introduced by Polyak&lt;sup id=&quot;fnref:polyak&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:polyak&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; in 1963, but has somehow not been very widely publicized. It was re-introduced to the ML optimization literature by Karimi et al.&lt;sup id=&quot;fnref:karimi&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:karimi&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; and its relevance (particularly in the context of deep learning) is slowly becoming apparent.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;PLCondition&quot;&gt;Definition&lt;/strong&gt;
  A function $L$ (whose optimum is $L_{\text{opt}}$) is said to satisfy the Polyak-Lojasiewicz (PL) condition with parameter $\alpha$ if:&lt;/p&gt;

\[\lVert \nabla L(u) \rVert^2 \geq 2 \alpha (L(u) - L_{\text{opt}} )\]

&lt;p class=&quot;definition&quot;&gt;for all $u$ in its domain.&lt;/p&gt;

&lt;p&gt;The reason for the “2” sitting before $\alpha$ will become clear. Intuitively, the PL condition says that if $L(u) \gg  L_{\text{opt}}$ then $\lVert \nabla L(u) \rVert$ is also large. More precisely, the norm of the gradient at any point grows at least as the square root of the (functional) distance to the optimum.&lt;/p&gt;

&lt;p&gt;Notice that there is no requirement of convexity in the definition of the PL condition. For example, the function:&lt;/p&gt;

\[L(x) = x^2 + 3 \sin^2 x\]

&lt;p&gt;has a plot that looks like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/pl-example.png&quot; alt=&quot;Plot of $L(x)$&quot; /&gt;&lt;/p&gt;

&lt;p&gt;which is non-convex upon inspection, but nonetheless satisfies the PL condition with constant $\alpha = \frac{1}{32}$.&lt;/p&gt;

&lt;p&gt;(However, the converse is true. Strong convexity implies the PL condition, but PL is far more general. We return to this in Chapter 6.)&lt;/p&gt;

&lt;p&gt;We immediately get the following result.&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GDPL&quot;&gt;Theorem&lt;/strong&gt;
  If $L$ is $\beta$-smooth and satisfies the PL condition with parameter $\alpha$, then GD exponentially converges to the optimum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;
  Follows trivially. Let $\eta = \frac{1}{\beta}$. Then&lt;/p&gt;

\[w_{t+1} = w_t - \frac{1}{\beta} \nabla L(w_t) .\]

&lt;p&gt;Plug $w_{t+1} - w_t$ into the smoothness upper bound. We get:&lt;/p&gt;

\[L(w_{t+1}) \leq L(w_t) - \frac{1}{2\beta} \lVert \nabla L(w_t) \rVert^2 .\]

&lt;p&gt;But since $L$ satisfies PL, we get:&lt;/p&gt;

\[L(w_{t+1}) \leq L(w_t) - \frac{\alpha}{\beta} \left( L(w_{t+1}) - L(w_\text{opt}) \right).\]

&lt;p&gt;Simplifying notation, we get:&lt;/p&gt;

\[L_{t+1} - L_{\text{opt}} \leq \left(1 - \frac{\alpha}{\beta} \right) \left( L_{t} - L_{\text{opt}} \right) .\]

&lt;p class=&quot;proof&quot;&gt;which implies that GD converges at $\exp(-\frac{\alpha}{\beta}t)$ rate.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Why is the PL condition interesting? It has been shown that several neural net training problems satisfy PL.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Single neurons (with leaky ReLUs.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear neural networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear residual networks (with square weight matrices).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Others? Wide networks? &lt;strong&gt;Complete&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class=&quot;label&quot; id=&quot;stochastic-gradient-descent-sgd&quot;&gt;Stochastic gradient descent (SGD)&lt;/h2&gt;

&lt;p&gt;We have obtained a reasonable picture of how GD works, how many iterations it needs, etc. But how does the picture change with inexact (stochastic) gradients?&lt;/p&gt;

&lt;p&gt;This question is paramount in deep learning practice, since nobody really does full-batch GD. Datasets are massive, and since the loss is decomposable across all the data points, gradients of the loss require making a full sweep of the training dataset &lt;em&gt;for each iteration&lt;/em&gt;, which no one really has time.&lt;/p&gt;

&lt;p&gt;Even more so, it seems that stochastic gradients (instead of full gradients) may influences &lt;em&gt;generalization&lt;/em&gt; behavior. Anecdotally, it was observed that models trained by SGD typically improved over models trained with full-batch gradient descent. Therefore, there may be some hidden benefit of stochasticity.&lt;/p&gt;

&lt;p&gt;To explain this, there were a ton of papers discussing the distinction between “sharp” versus “flat” minima&lt;sup id=&quot;fnref:sharp&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sharp&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, and how the latter type of minima generalize better, and how minibatch methods (such as SGD) favor flat minima, and therefore SGD gives better solutions period. Folks generally went along with this explanation. However, since this initial flurry of papers this common belief has since been somewhat upended.&lt;/p&gt;

&lt;p&gt;First off, it is not really clear how “sharpness” or “flatness” should be formally defined. A paper by Dinh et al.&lt;sup id=&quot;fnref:dinh&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:dinh&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; showed that good generalization can be obtained even if the model corresponds to a very “sharp” minimum in the loss landscape (for most commonly accepted definitions of “sharp”). So even if SGD finds flatter minima, it is unclear whether such minima are somehow inherently better.&lt;/p&gt;

&lt;p&gt;A very recent paper by Geiping et al.&lt;sup id=&quot;fnref:gieping&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:gieping&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; in fact finds the opposite; performance by GD (with properly tuned hyperparameters and regularization) matches that of SGD. Theory is silent on this matter and I am not aware of any concrete separation-type between GD and SGD for neural networks.&lt;/p&gt;

&lt;p&gt;Still, independent of whether GD is theoretically better than SGD or not, it is instructive to analyze SGD (since everyone uses it.) Let us derive an analogous bound on the error rates of SGD.&lt;/p&gt;

&lt;p&gt;In SGD, our updates look like:&lt;/p&gt;

\[w_{i+1} = w_i - \eta_i g_i\]

&lt;p&gt;Here $g_i$ is a noisy version to the full gradient $\nabla L(w_i)$ (where the “noise” here is due to minibatch sampling). Due to stochasticity, $g_i$ (and therefore, $w_i$) are random variables, and we should analyze convergence in terms of their expected value.&lt;/p&gt;

&lt;p&gt;Intuitively, meaningful progress is possible only when the noise variance is not too large; this is achieved if the minibatch size is not too small.
The following assumptions are somewhat standard (although rigorously proving this takes some effort.)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Property 1: Unbiased gradients&lt;/em&gt;: We will assume that in expectation, $g_i$ is an unbiased estimate of the true gradient. In other words, if $\varepsilon_i = g_i - \nabla L(w_i)$, then&lt;/p&gt;

\[\mathbb{E}{\varepsilon_i} = 0 .\]

&lt;p&gt;&lt;em&gt;Property 2: Bounded gradients&lt;/em&gt;: We will assume that the gradients are uniformly bounded in magnitude by a constant:&lt;/p&gt;

\[\max_i \lVert g_i \rVert \leq G.\]

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;RemBounded&quot;&gt;Remark&lt;/strong&gt;
Property 1 is fine if we sample the terms in the gradient uniformly at random. Property 2 is hard to justify in practice. (In fact, for convex functions this is not even true!) But better proofs such as this&lt;sup id=&quot;fnref:bottou&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bottou&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; and this&lt;sup id=&quot;fnref:nguyen&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:nguyen&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; have shown that similar rates of convergence for SGD are possible even if we relax this assumption, so let’s just go with this for now and assume it can be fixed.&lt;/p&gt;

&lt;p&gt;Assuming the above two properties, we will prove:&lt;/p&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;SGDSmooth&quot;&gt;Theorem&lt;/strong&gt;
  If $L$ is $\beta$-smooth, then SGD converges (in expectation) to an $\varepsilon$-approximate critical point in $O\left(\frac{\beta}{\varepsilon^4}\right)$ steps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;
  Let’s run SGD with &lt;em&gt;fixed&lt;/em&gt; step-size $\eta$ for $t$ steps. From the smoothness upper bound, we get:&lt;/p&gt;

\[L(w_{t+1}) \leq L(w_t) + \langle \nabla L(w_t), w_{t+1} - w_t \rangle + \frac{\beta}{2} \lVert w_{t+1} - w_t \rVert^2 .\]

&lt;p&gt;But $w_{t+1} - w_t = - \eta g_t$. Plugging into the above bound and rearranging terms:&lt;/p&gt;

\[\eta \langle L(w_t), g_t \rangle \leq L(w_t) - L(w_{t+1}) + \frac{\beta}{2} \eta^2 \lVert g_t \rVert^2 .\]

&lt;p&gt;Take expectation on both sides. Property 1 and 2 give us:&lt;/p&gt;

\[\eta \mathbb{E} \lVert L(w_t) \rVert^2 \leq \mathbb{E} \left( L(w_t) - L(w_{t+1}) \right) + \frac{\beta}{2} \eta^2 G^2 .\]

&lt;p&gt;Telescope from 0 through $T$, and divide by $\eta T$. Then we get:&lt;/p&gt;

\[\min_{t &amp;lt; T} \mathbb{E} \lVert L(w_t) \rVert^2 \leq \frac{L_0 - L_T}{\eta T} + \frac{\beta \eta G^2}{2}.\]

&lt;p&gt;This is true for all $\eta$. In order to get the tightest upper bound and minimize the right hand side, we need to balance the two terms on the right. This is achieved if:&lt;/p&gt;

\[\eta = O(\frac{1}{\sqrt{T}}).\]

&lt;p&gt;Plugging this in, ignoring all other constants, we get:&lt;/p&gt;

\[\min_{t &amp;lt; T} \mathbb{E} \lVert L(w_t) \rVert^2 \lesssim \frac{1}{\sqrt{T}}, \quad \text{or} \quad  \mathbb{E} \lVert L(w_t) \rVert \lesssim \frac{1}{T^{1/4}}.\]

&lt;p class=&quot;proof&quot;&gt;This concludes the proof.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Hierarchy:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SGD assuming Lipschitz smoothness: $\frac{1}{t^{1/4}}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SGD assuming Lipschitz smoothness + convexity: $\frac{1}{\sqrt{t}}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other rates?&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;extensions&quot;&gt;Extensions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Nesterov momemtum&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;(COMPLETE)&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:adam&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;D. Kingma and J. Ba, &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;, 2014. &lt;a href=&quot;#fnref:adam&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:losslandscape&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;H. Li, Z. Xu, G. Taylor, C. Studer, T. Goldstein, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf&quot;&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:losslandscape&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zhang&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;Understanding deep learning requires rethinking generalization&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:zhang&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ji&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Z. Ji and M. Telgarsky, &lt;a href=&quot;https://arxiv.org/pdf/2006.06657.pdf&quot;&gt;Directional convergence and alignment in deep learning&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:ji&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:mjt&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Telgarsky, &lt;a href=&quot;https://mjt.cs.illinois.edu/dlt/&quot;&gt;Deep Learning Theory&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:mjt&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:oco&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;There is an entire literature on online optimization that uses this interpretation of gradient descent, but they call it the “Follow-the-leader” strategy. See &lt;a href=&quot;https://courses.cs.washington.edu/courses/cse599s/14sp/scribes/lecture3/lecture3.pdf&quot;&gt;here&lt;/a&gt; for an explanation. &lt;a href=&quot;#fnref:oco&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:leegd&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Du, C. Jin, J. Lee, M. Jordan, B. Poczos, A. Singh, &lt;a href=&quot;https://arxiv.org/abs/1705.10412&quot;&gt;Gradient Descent Can Take Exponential Time to Escape Saddle Points&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:leegd&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:polyak&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;B. Polyak, &lt;a href=&quot;http://www.mathnet.ru/links/b7971e9e07cc44b5d0e9fb6354f11988/zvmmf7813.pdf&quot;&gt;ГРАДИЕНТНЫЕ МЕТОДЫ МИНИМИЗАЦИИ ФУНКЦИОНАЛОВ (Gradient methods for minimizing functionals)&lt;/a&gt;, 1963. &lt;a href=&quot;#fnref:polyak&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:karimi&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;H. Karimi, J. Nutini, M. Schmidt, &lt;a href=&quot;https://arxiv.org/pdf/1608.04636.pdf&quot;&gt;Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Lojasiewicz Condition&lt;/a&gt;, 2016. &lt;a href=&quot;#fnref:karimi&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sharp&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;N. Keskar, D. Mudigere, J. Nocedal, P. Tang, &lt;a href=&quot;https://openreview.net/pdf?id=H1oyRlYgg&quot;&gt;On Large-Batch Training for Deep Learning&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:sharp&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dinh&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;L. Dinh, R. Pascanu, S. Bengio, Y. Bengio, &lt;a href=&quot;https://arxiv.org/pdf/1703.04933.pdf&quot;&gt;Sharp Minima Can Generalize For Deep Nets&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:dinh&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gieping&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;J. Geiping, M. Goldblum, P. Pope, M. Moeller, T. Goldstein, &lt;a href=&quot;https://arxiv.org/abs/2109.14119&quot;&gt;Stochastic Training is Not Necessary for Generalization&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:gieping&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bottou&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;L. Bottou, F. Curtis, J. Nocedal, &lt;a href=&quot;https://leon.bottou.org/publications/pdf/tr-optml-2016.pdf&quot;&gt;Optimization Methods for Large-Scale Machine Learning&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:bottou&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:nguyen&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;L. Nguyen, P. Ha Nguyen, M. van Dijk, P. Richtarik, K. Scheinberg, M. Takac, &lt;a href=&quot;https://arxiv.org/pdf/1802.03801.pdf&quot;&gt;SGD and Hogwild! Convergence Without the Bounded Gradients Assumption&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:nguyen&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 5 - Optimizing wide networks</title>
    <link href="http://localhost:4000/optimization02/"/>
    <updated>2022-01-14T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/optimization02</id>
    <content type="html">&lt;p&gt;In the previous chaper, we proved that (S)GD converges locally for training neural networks with smooth activations. (The case for ReLU networks is harder but still doable with additional algebraic heavy lifting.)&lt;/p&gt;

&lt;p&gt;However, in practice, somewhat puzzlingly we encounter very deep networks that are regularly optimized to &lt;em&gt;zero training loss&lt;/em&gt;, i.e., they exactly learn to interpolate the given training data and labels. Since loss functions are typically non-negative, this means that (S)GD have achieved convergence to the &lt;em&gt;global&lt;/em&gt; optimum.&lt;/p&gt;

&lt;p&gt;In this chapter, we ask the question:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;When and why does (S)GD give zero train loss?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;An answer to this question in full generality is not yet available. However, we will prove a somewhat surprising fact: for &lt;em&gt;very wide&lt;/em&gt; networks that are &lt;em&gt;randomly initialized&lt;/em&gt;, this is indeed the case. The road to proving this will give us several surprising insights, and connections to classical ML (such as kernel methods) will become clear along the way.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;local-versus-global-minima&quot;&gt;Local versus global minima&lt;/h2&gt;

&lt;p&gt;Let us bring up this picture from Xu et al.&lt;sup id=&quot;fnref:losslandscape&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:losslandscape&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; again:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/losslandscape.png&quot; alt=&quot;Loss landscapes of ResNet-56. Adding skips significantly influences ease of optimization, since the landscape is much better behaved.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The picture on the left shows the loss landscape of a feedforward convolutional network. The picture on the right shows the loss landscape of the same network with residual skip connections.&lt;/p&gt;

&lt;p&gt;Why would (S)GD navigate its way to the global minimum of this jagged-y, corrugated loss landscape? There could be one of many explanations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Explanation 1&lt;/em&gt;: We got lucky and initialized very well (close enough to the global optimum for GD to work). But if there is only one global minimum then “getting lucky” is an event that happens with exponentially small probability. So there has to be another reason.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Explanation 2&lt;/em&gt;: The skips (clearly) matter, so maybe most real-world networks in fact look like the ones on the right. There is a grain of truth here: in modern (very) deep networks, there exist all kinds of bells and whistles to “make things work”. Residual (skip) connections are common. So are other tweaks such as batch normalization, dropout, learning rate scheduling, etc etc. All&lt;sup id=&quot;fnref:resnet&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:resnet&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; of these&lt;sup id=&quot;fnref:batchnorm&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:batchnorm&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; tweaks&lt;sup id=&quot;fnref:explearn&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:explearn&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; significantly influence the nature of the optimization problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Explanation 3&lt;/em&gt;: Since modern deep networks are so &lt;strong&gt;heavily over-parameterized&lt;/strong&gt;, there may be tons of minima that exactly interpolate the data. In fact, with high probability, a random initialization lies “close” to a interpolating set of weights. Therefore, gradient descent starting from this location successfully trains to zero loss.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will pursue Explanation 3, and derive (mathematically) fairly precise justifications of the above claims. As we will argue in the next chapter, this is by no means the full picture. But the exercise is still going to be very useful.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;the-neural-tangent-kernel&quot;&gt;The Neural Tangent Kernel&lt;/h2&gt;

&lt;p&gt;Our goal will be to understand what level of over-parameterization enables gradient descent to train neural network models to zero loss. So if the network contains $p$ parameters, we would like to derive scaling laws of $p$ in terms of $n$ and $d$. Since the number of parameters is greater than the number of samples, proving generalization bounds will be challenging. But let’s set aside such troubling thoughts for now.&lt;/p&gt;

&lt;p&gt;When we think of modern neural networks and their size, we typically associate “growing” the size of networks in terms of their &lt;em&gt;depth&lt;/em&gt;. However, our techniques below will instead focus on the &lt;em&gt;width&lt;/em&gt; of the network (mirroring our theoretical treatment of network representation capacity). Many results below apply to deep networks, but the key controlling factor is the width (&lt;em&gt;note: actually, not even the width but other strange properties; more later.&lt;/em&gt;) The impact of depth will continue to remain elusive.&lt;/p&gt;

&lt;p&gt;Here’s a high level intuition of the path forward:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We know that if the loss landscape is convex, then the dynamics of training algorithms such as GD or SGD ensures that we always converge to a global optimum.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We know that linear/kernel models exhibit convex loss landscapes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We will &lt;em&gt;prove&lt;/em&gt; that &lt;em&gt;for wide enough networks&lt;/em&gt;, the loss landscape &lt;em&gt;looks like that of a kernel model&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So in essence, establishing the last statement triggers the chain of implications:&lt;/p&gt;

\[(3) \implies (2) \implies (1)\]

&lt;p&gt;and we are done.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some history. The above idea is called the &lt;em&gt;Neural Tangent Kernel&lt;/em&gt;, or the NTK, approach. The term itself first appeared in a landmark paper by Jacot et al.&lt;sup id=&quot;fnref:jacot&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:jacot&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, which first established the global convergence of NN training in the infinite-width limit. Roughly around the same time, several papers emerged that also provided global convergence guarantees in certain specialized families of neural network models. See here&lt;sup id=&quot;fnref:du2019&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:du2019&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, here&lt;sup id=&quot;fnref:allenzhu2019&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:allenzhu2019&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, and here&lt;sup id=&quot;fnref:lee2019&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:lee2019&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. All of them involved roughly the same chain of implications as described above, so now can be dubbed as “NTK-style” papers.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 class=&quot;label&quot; id=&quot;gradient-dynamics-for-linear-models&quot;&gt;Gradient dynamics for linear models&lt;/h3&gt;

&lt;p&gt;As a warmup, let’s first examine how GD over the squared-error loss learns the parameters of linear models. Most of this should be classical. For a given training dataset $(x_i, y_i)$, define the linear model using weights $w$, so that the predicted labels enjoy the form:&lt;/p&gt;

\[u_i = \langle x_i, w \rangle, \qquad u = Xw\]

&lt;p&gt;where $X$ is a matrix with the data points stacked row-wise. This leads to the familiar loss:&lt;/p&gt;

\[L(w) = \frac{1}{2} \sum_{i=1}^n \left( y_i - \langle x_i, w \rangle \right)^2 = \frac{1}{2} \lVert y - Xw \rVert^2 .\]

&lt;p&gt;We optimize this loss via GD:&lt;/p&gt;

\[w^+ = w - \eta \nabla L(w)\]

&lt;p&gt;where the gradient of the loss looks like:&lt;/p&gt;

\[\nabla L(w) = - \sum_{i=1}^n x_i (y_i - u_i) =  - X^T (y - u)\]

&lt;p&gt;and so far everything is good. Now, imagine that we execute gradient descent &lt;em&gt;with infinitesimally small step size&lt;/em&gt; $\eta \rightarrow 0$, which means that we can view the evolution of the weights as the output of the following ordinary differential equation (ODE):&lt;/p&gt;

\[\frac{dw}{dt} = - \nabla L(w) = X^T (y - u).\]

&lt;p&gt;This is sometimes called &lt;em&gt;Gradient Flow&lt;/em&gt; (GF), and standard GD is rightfully viewed as a finite-time discretization of this ODE. Moreover, due to linearity of the model, the evolution of the &lt;em&gt;output labels&lt;/em&gt; $u$ follows the ODE:&lt;/p&gt;

\[\begin{aligned}
\frac{du}{dt} &amp;amp;= X \frac{dw}{dt}, \\
\frac{du}{dt} &amp;amp;= XX^T (y-u) \\
&amp;amp;= H (y - u).
\end{aligned}\]

&lt;p&gt;Several remarks are pertinent at this point:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear ODEs of this nature can be solved in closed form. If $r = y-u$ is the “residual” then the equation becomes:&lt;/p&gt;

\[\frac{dr}{dt} = - H r\]

    &lt;p&gt;whose solution, informally, is the (matrix) exponential:&lt;/p&gt;

\[r(t) = \exp(-Ht) r(0)\]

    &lt;p&gt;which &lt;em&gt;immediately&lt;/em&gt; gives that if $H$ is &lt;em&gt;full-rank&lt;/em&gt; with $\lambda_{\text{min}} (H) &amp;gt;0$, then GD is extremely well-behaved; it provably converges at an exponential rate towards a set of weights with zero loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Following up from the first point: the matrix $H = X X^T$, which principally governs the dynamics of GD, is &lt;em&gt;constant with respect to time&lt;/em&gt;, and is entirely determined by the &lt;em&gt;geometry&lt;/em&gt; of the data. Configurations of data points which push $\lambda_{\text{min}} (H)$ as high as possible enable GD to converge quicker, and vice versa.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The data points themselves don’t matter very much; all that matters is the set of their &lt;em&gt;pairwise dot products&lt;/em&gt;:&lt;/p&gt;

\[H_{ij} = \langle x_i, x_j \rangle .\]

    &lt;p&gt;This immediately gives rise to an alternate way to introduce the well-known &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick&quot;&gt;kernel trick&lt;/a&gt;: simply replace $H$ by &lt;em&gt;any&lt;/em&gt; other easy-to-compute kernel function:&lt;/p&gt;

\[K_{ij} = \langle \phi(x_i), \phi(x_j) \rangle\]

    &lt;p&gt;where $\phi$ is some feature map. We now suddenly have acquired the superpower of being able to model non-linear features of the data. This derivation also shows that training kernel models is no more challenging than training linear models (provided $K$ is easy to write down.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 class=&quot;label&quot; id=&quot;gradient-dynamics-for-general-models&quot;&gt;Gradient dynamics for general models&lt;/h3&gt;

&lt;p&gt;Let us now derive the gradient dynamics for a general deep network $f(w)$. We mirror the above steps. For a given input $x_i$, the predicted label obeys the form:&lt;/p&gt;

\[u_i = f_{x_i}(w)\]

&lt;p&gt;where the subscript of $f$ here makes the dependence on the input features explicit. The squared-error loss becomes:&lt;/p&gt;

\[L(w) = \frac{1}{2} \sum_{j=1}^n \left( y_j - f_{x_j}(w) \right)^2\]

&lt;p&gt;and its gradient with respect to any one weight becomes:&lt;/p&gt;

\[\nabla L(w) \vert_{\text{one coordinate}} = - \sum_{j=1}^n \frac{\partial f_{x_j}(w)}{\partial w} \left( y_j - u_j \right) .\]

&lt;p&gt;Therefore, the dynamics of any one output label – which, in general, depends on all the weights – can be calculated by summing over the partial derivatives over all the weights:&lt;/p&gt;

\[\begin{aligned}
\frac{du_i}{dt} &amp;amp;= \sum_w \frac{\partial u_i}{\partial w} \frac{dw}{dt} \\
&amp;amp; = \sum_w \frac{\partial f_{x_i}(w)}{\partial w} \left( \sum_{j=1}^n \frac{\partial f_{x_j}(w)}{\partial w} \left( y_j - u_j \right) \right) \\
&amp;amp;= \sum_{j=1}^n \Big\lang \frac{\partial f_{x_i}(w)}{\partial w}, \frac{\partial f_{x_j}(w)}{\partial w} \Big\rang \left(y_j - u_j \right) \\
&amp;amp;= \sum_{j=1}^n H_{ij} (y_j - u_j) ,
\end{aligned}\]

&lt;p&gt;where in the last step we switched the orders of summation, and defined:&lt;/p&gt;

\[H_{ij} := \Big\lang \frac{\partial f_{x_i}(w)}{\partial w}, \frac{\partial f_{x_j}(w)}{\partial w} \Big\rang = \sum_w \frac{\partial f_{x_i}(w)}{\partial w}  \frac{\partial f_{x_j}(w)}{\partial w} .\]

&lt;p&gt;We have used angular brackets to denote dot products. In the case of an infinite number of parameters, the same relation holds. But we have to replace summations by expectations over a measure (and perform additional algebra book-keeping, so let’s just take it as true).&lt;/p&gt;

&lt;p&gt;Once again, we make several remarks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Observe that the dynamics is &lt;em&gt;very similar in appearance&lt;/em&gt; to that of a linear model! in vector form, we get the evolution of all $n$ labels as&lt;/p&gt;

\[\frac{du}{dt} = H_t (y - u)\]

    &lt;p&gt;where $H_t$ is an $n \times n$ matrix governing the dynamics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But! There is a crucial twist! The governing matrix $H_t$ is &lt;em&gt;no longer constant&lt;/em&gt;: it depends on the current set of weights $w$, and therefore is a function of time — hence the really pesky subscript $t$. This also means that the ODE is no longer linear; $H_t$ interacts with $u(t)$, and therefore the picture is far more complex.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One can check that $H_t$ is symmetric and positive semi-definite; therefore, we can view the above equation as the dynamics induced by a (time-varying) kernel mapping. Moreover, the corresponding feature map is nothing but:&lt;/p&gt;

\[\phi : x \mapsto \frac{\partial f_{x}(w)}{\partial w }\]

    &lt;p&gt;which can be viewed as the “tangent model” of $f$ at $w$. This is a long-winded explanation of the origin of the name “NTK” for the above analysis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 class=&quot;label&quot; id=&quot;wide-networks-exhibit-linear-model-dynamics&quot;&gt;Wide networks exhibit linear model dynamics&lt;/h3&gt;

&lt;p&gt;The above calculations give us a mechanism to understand how (and under what conditions) gradient dynamics of general networks resemble those of linear models. Basically, our strategy will be as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We will &lt;em&gt;randomly&lt;/em&gt; initialize weights at $t=0$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At $t=0$, we will prove that the corresponding NTK matrix, $H_0$, is full-rank and that its eigenvalues are bounded away from zero.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For large widths, we will show that $H_t \approx H_0$, i.e., the NTK matrix &lt;em&gt;stays approximately constant&lt;/em&gt;. In particular, the dynamics always remains full rank.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Combining $1+2+3$ gives the overall proof. This proof appeared in Du et al.&lt;sup id=&quot;fnref:du2019:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:du2019&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and the below derivations are adapted from this fantastic book &lt;sup id=&quot;fnref:arorabook&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:arorabook&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Concretely, we consider two-layer networks with $m$ hidden neurons with twice-differentiable activations $\psi$ with bounded first and second derivatives. This again means that ReLU doesn’t count, but other analogous proofs can be derived for ReLUs; see&lt;sup id=&quot;fnref:du2019:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:du2019&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;For ease of derivation, let us assume that the &lt;em&gt;second layer weights&lt;/em&gt; are fixed and equal to $\pm 1$ chosen equally at random, and that we only train the first layer. This assumption may appear strange, but (a) all proofs will go through if we train both layers, and (b) the first layer weights are really the harder ones in terms of theoretical analysis. (&lt;em&gt;Exercise&lt;/em&gt;: Show that if we flip things around and train only the second layer, then really we are fitting a linear model to the data.)&lt;/p&gt;

&lt;p&gt;Therefore, the model assumes the form:&lt;/p&gt;

\[f_x(w) = \frac{1}{\sqrt{m}} \sum_{r=1}^m a_r \psi(\langle w_r, x \rangle) .\]

&lt;p&gt;where $a_r = \pm 1$ and the scaling $\frac{1}{\sqrt{m}}$ is chosen to make the algebra below nice. (Somewhat curiously, this &lt;em&gt;exact&lt;/em&gt; scaling turns out to be crucial, and we will revisit this point later.)&lt;/p&gt;

&lt;p&gt;We initialize all weights $w_1(0), w_2(0), \ldots, w_r(0)$ according to a standard normal distribution. In neural network Since we are only using 2-layer feedforward networks, the gradient at time $t=0$ becomes:&lt;/p&gt;

\[\frac{\partial f_{x_i}(w(0))}{\partial w_r} = \frac{1}{\sqrt{m}} a_r x_i \psi'( \langle w_r(0), x_i \rangle )\]

&lt;p&gt;with respect to the weights of the $r^{th}$ neuron. As  per our above derivation, at time $t=0$, we get that the NTK has entries:&lt;/p&gt;

\[\begin{aligned}
[H(0)]_{ij} &amp;amp;= \Big\lang \frac{\partial f_{x_i}}{\partial w_r}, \frac{\partial f_{x_j}}{\partial w_r} \Big\rang \\
&amp;amp;= x_i^T x_j\Big[ \frac{1}{m} \sum_{r=1}^m a_r^2 \psi'( \langle w_r(0), x_i \rangle ) \psi'( \langle w_r(0), x_j \rangle ) \Big]
\end{aligned}\]

&lt;p&gt;There is quite a bit to parse here. The main point here is to note that each entry of $H(0)$ is the average of $m$ random variables whose expectation equals:&lt;/p&gt;

\[x_i^T x_j \mathbb{E}_{w \sim \mathcal{N}(0,I)} \psi'(x_i^T w) \psi'(x_j^T w) := H^*_{ij}.\]

&lt;p&gt;In other words, if we had infinitely many neurons in the hidden layer then the NTK at time $t=0$ would equal its expected value, given by the matrix $H^&lt;em&gt;$. (&lt;/em&gt;Exercise&lt;em&gt;: It is not hard to check that for $m = \Omega(n)$ and for data in general position, $H^&lt;/em&gt;$ is full rank; prove this.)&lt;/p&gt;

&lt;p&gt;Our first theoretical result will be a bound on the width of the network that ensures that $H(0)$ and $H^*$ are close.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;NTKInit&quot;&gt;Theorem&lt;/strong&gt;
  Fix $\varepsilon &amp;gt;0$. Then, with high probability we have&lt;/p&gt;

\[\lVert H(0) - H^* \rVert_2 \leq \varepsilon\]

&lt;p&gt;provided the hidden layer has at least&lt;/p&gt;

\[m \geq \tilde{O} \left( \frac{n^4}{\varepsilon^2} \right)\]

&lt;p class=&quot;theorem&quot;&gt;neurons.&lt;/p&gt;

&lt;p&gt;Our second theoretical result will be a width bound that ensures that $H(t)$ remains close to $H(0)$ throughout training.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;NTKDynamics&quot;&gt;Theorem&lt;/strong&gt;
  Suppose that $y_i = \pm 1$ and $u_i(\tau)$ remains bounded throughout training, i.e., for $0 \leq \tau &amp;lt; t$. Fix $\varepsilon &amp;gt;0$. Then, with high probability we have&lt;/p&gt;

\[\lVert H(0) - H^* \rVert_2 \leq \varepsilon\]

&lt;p&gt;provided the hidden layer has at least&lt;/p&gt;

\[m \geq \tilde{O} \left( \frac{n^6 t^2}{\varepsilon^2} \right)\]

&lt;p class=&quot;theorem&quot;&gt;neurons.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Before providing proofs of the above statements, let us make several more remarks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The above results show that the width requirement scales polynomially with the number of samples. (In fact, it is a rather high degree polynomial.) Subsequent works have tightened this dependence; these&lt;sup id=&quot;fnref:os20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:os20&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; papers&lt;sup id=&quot;fnref:sy19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sy19&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; were able to achieve a quadratic scaling of $m = \tilde{O}(n^2)$ hidden neurons for GD to provably succeed. As far as I know, the current best bound is sub-quadratic  ($O(n^{\frac{3}{2}})$), using similar arguments as above; see here&lt;sup id=&quot;fnref:efth&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:efth&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The above derivation is silent on the &lt;em&gt;dimension&lt;/em&gt; and the &lt;em&gt;geometry&lt;/em&gt; of the input data. If we assume additional structure on the data, we can improve the dependence to $O(nd)$; see our paper&lt;sup id=&quot;fnref:benefits&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:benefits&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;. However, the big-oh here hides several data-dependent constants that could become polynomially large themselves.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For $L$-layer networks, the best available bounds are rather weak; widths need to scale as $m = \text{poly}(n, L)$. See here&lt;sup id=&quot;fnref:allenzhu2019:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:allenzhu2019&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; and here&lt;sup id=&quot;fnref:zougu2020&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:zougu2020&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 class=&quot;label&quot; id=&quot;lazy-training&quot;&gt;Lazy training&lt;/h3&gt;

&lt;p&gt;** &lt;em&gt;(Complete)&lt;/em&gt; **&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;proofs&quot;&gt;Proofs&lt;/h2&gt;

&lt;p&gt;** &lt;em&gt;(COMPLETE)&lt;/em&gt; **.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:losslandscape&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;H. Li, Z. Xu, G. Taylor, C. Studer, T. Goldstein, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf&quot;&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:losslandscape&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:resnet&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;M. Hardt and T. Ma, &lt;a href=&quot;https://openreview.net/forum?id=ryxB0Rtxx&quot;&gt;Identity Matters in Deep Learning&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:resnet&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:batchnorm&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;H Daneshmand, A Joudaki, F Bach, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/26cd8ecadce0d4efd6cc8a8725cbd1f8-Paper.pdf&quot;&gt;Batch normalization orthogonalizes representations in deep random networks&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:batchnorm&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:explearn&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Z. Li and S. Arora, &lt;a href=&quot;https://arxiv.org/pdf/1910.07454.pdf&quot;&gt;An exponential learning rate schedule for deep learning&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:explearn&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:jacot&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;A. Jacot, F. Gabriel. C. Hongler, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf&quot;&gt;Neural Tangent Kernel: Convergence and Generalization in Neural Networks&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:jacot&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:du2019&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Du, X. Zhai, B. Poczos, A. Singh, &lt;a href=&quot;https://openreview.net/pdf?id=S1eK3i09YQ&quot;&gt;Gradient Descent Provably Optimizes Over-parameterized Neural Networks&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:du2019&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:du2019:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:du2019:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:allenzhu2019&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Z. Allen-Zhu, Y. Li, Z. Song, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2019/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf&quot;&gt;Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:allenzhu2019&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:allenzhu2019:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lee2019&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, J. Pennington, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf&quot;&gt;Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:lee2019&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:arorabook&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;R. Arora, S. Arora, J. Bruna. N. Cohen, S. Du. R. Ge, S. Gunasekar, C. Jin, J. Lee, T. Ma, B. Neyshabur, Z. Song, &lt;a href=&quot;http://simonshaoleidu.com/teaching/cs599tdl/DLbook.pdf&quot;&gt;Theory of Deep Learning&lt;/a&gt;,  2021. &lt;a href=&quot;#fnref:arorabook&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:os20&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Oymak, M. Soltanolkotabi, &lt;a href=&quot;http://proceedings.mlr.press/v97/oymak19a/oymak19a.pdf&quot;&gt;Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:os20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sy19&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Z. Song and X. Yang, &lt;a href=&quot;https://arxiv.org/pdf/1906.03593.pdf&quot;&gt;Over-parametrization for Learning and Generalization in Two-Layer Neural Networks&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:sy19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:efth&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Song, A. Ramezani-Kebrya, T. Pethick, A. Eftekhari, V. Cevher, &lt;a href=&quot;https://arxiv.org/pdf/2111.01875.pdf&quot;&gt;Subquadratic Overparameterization for Shallow Neural Networks&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:efth&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:benefits&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;T. Nguyen, R. Wong, C. Hegde, &lt;a href=&quot;https://arxiv.org/pdf/1911.11983.pdf&quot;&gt;Benefits of Jointly Training Autoencoders: An Improved Neural Tangent Kernel Analysis&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:benefits&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zougu2020&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;D. Zou, Y. Cao, D. Zhou, Q. Gu, &lt;a href=&quot;https://link.springer.com/article/10.1007/s10994-019-05839-6&quot;&gt;Gradient descent optimizes over-parameterized deep ReLU networks&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:zougu2020&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 6 - Beyond NTK</title>
    <link href="http://localhost:4000/optimization03/"/>
    <updated>2022-01-13T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/optimization03</id>
    <content type="html">&lt;p&gt;&lt;strong&gt;Under construction&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Is NTK the only explanation? Almost surely not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The crux of NTK: weights don’t move very much from their (random) inits&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Therefore, meaningful feature learning does not happen!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, we know (both from visualizations as well as controlled experiments) that neural networks &lt;em&gt;do&lt;/em&gt; learn features using GD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Hessian control&lt;/em&gt;: a theory that explains dynamics “far away” from the initialization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class=&quot;label&quot; id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;h2 class=&quot;label&quot; id=&quot;the-pl-condition&quot;&gt;The PL* condition&lt;/h2&gt;

&lt;h2 class=&quot;label&quot; id=&quot;hessian-control&quot;&gt;Hessian control&lt;/h2&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 7 - Implicit regularization</title>
    <link href="http://localhost:4000/generalization01/"/>
    <updated>2022-01-12T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/generalization01</id>
    <content type="html">&lt;p&gt;We spent the last few chapters understanding the &lt;em&gt;optimization&lt;/em&gt; aspect of deep learning. Namely, given a dataset and a family of network architectures, how well do standard training procedures (such as gradient descent or SGD) work in terms of fitting the data? As we witnessed, an answer to this question is far from complete, but a picture (involving tools from optimization theory and kernel-based learning) is beginning to emerge.&lt;/p&gt;

&lt;p&gt;However, training is not the end goal. Recall in the Introduction that we want to discover prediction functions that perform “well” on “most” possible data points. In other words, we not only want our model to fit a training dataset well, but also would like our model to exhibit &lt;em&gt;low generalization error&lt;/em&gt; when evaluated on hitherto unseen inputs.&lt;/p&gt;

&lt;p&gt;Over the next few lectures, we address the central question:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;When (and why) do networks trained using (S)GD generalize?  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we work through different aspects of this question, we will find that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;classical statistical learning theory fails to give useful (non-vacuous) generalization error bounds.&lt;/li&gt;
  &lt;li&gt;in fact, lots of other weirdness happens.&lt;/li&gt;
  &lt;li&gt;architecture, optimization, and generalization in deep learning interact in a non-trivial manner, and should be studied simultaneously.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us work backwards. In this chapter, we will see how the choice of training algorithm (which, in the case of deep learning, is almost always GD or SGD or some variant thereof) affects generalization.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;classical-models-margins-and-regularization&quot;&gt;Classical models, margins, and regularization&lt;/h2&gt;

&lt;p&gt;Statistical learning theory offers &lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_(mathematics)&quot;&gt;many&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory&quot;&gt;many&lt;/a&gt; lenses through which we can &lt;a href=&quot;https://en.wikipedia.org/wiki/Probably_approximately_correct_learning&quot;&gt;study&lt;/a&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Stability_(learning_theory)&quot;&gt;generalization&lt;/a&gt;. One approach (classical, and fairly intuitive) is to think in terms of &lt;em&gt;margins&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Let us be concrete. Say that we are doing binary classification using linear models.  We have a dataset $X = \lbrace (x_i,y_i)_{i=1}^n \subset \R^d \times \pm 1 \rbrace$. For simplicity let us ignore the bias and assume that everything is centered around the origin. The problem is to find a linear classifier that generalizes well. The following picture (in $d=2$) comes to mind.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fodl/assets/margins.png&quot; alt=&quot;Margins of linear classifiers.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Assume that the data is linearly separable. Then, we can write down the desired classifer as the solution to the following optimization problem (and in classical ML, we call such a model a &lt;em&gt;perceptron&lt;/em&gt;.)&lt;/p&gt;

\[\text{Find}~w,~\text{such that}~\text{sign}\lang w,x_i \rang = y_i~~\forall~i \in [n].\]

&lt;p&gt;or, equivalently,&lt;/p&gt;

\[\text{Find}~w,~\text{such that}~ y_i \lang w,x_i \rang \geq 0~~\forall~i \in [n].\]

&lt;p&gt;However, as is the case in the picture above, there could be several linear models that exactly interpolate the labels, which means that the above optimization problem may not have unique solutions. Which solution, then, should we pick?&lt;/p&gt;

&lt;p&gt;One way to resolve this issue is by the &lt;em&gt;Large Margin Hypothesis&lt;/em&gt;, which asserts that:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Larger margins --&amp;gt; better generalization.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Suppose we associate each linear classifier with the corresponding separating hyperplane (marked in red above). Since we want “most” points in the data space to be correctly classified, a reasonable choice is to find the hyperplane that is furthest away from either class. An easy calculation (shown, for example, &lt;a href=&quot;https://en.wikipedia.org/wiki/Support-vector_machine#Linear_SVM&quot;&gt;here&lt;/a&gt;) shows that for any hyperplane parameterized by $w$, the margin equals $\frac{2}{\lVert w \rVert}$. Therefore, maximizing the margin is the same as solving the the optimization problem:&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\min_w \frac{1}{2} \lVert w \rVert^2  \\
\text{s.t.~}&amp;amp; y_i \lang w, x_i \rang \geq 1,~\forall i \in [n] .
\end{aligned}\]

&lt;p&gt;The solution to this problem is called the &lt;em&gt;support vector machine&lt;/em&gt; (SVM). This is fine, as long as the data is linearly separable. If not, the constraint set is empty and the optimization problem is ill-posed. A remedy is to relax the (hard) constraints using Lagrange. We get the &lt;em&gt;soft-margin&lt;/em&gt; SVM as a solution to the (relaxed) problem:&lt;/p&gt;

\[\min_w \frac{1}{2} \lVert w \rVert^2 + C \sum_{i=1}^n l(y_i \lang w, x_i \rang)\]

&lt;p&gt;where $C &amp;gt; 0$ is some weighting parameter, and $l : \R \rightarrow \R$ is some (scalar) loss function that penalizes large negative inputs and leaves inputs bigger than 1 untouched. The $\exp$ loss ($l(z) = e^{-z}$), or the &lt;em&gt;Hinge&lt;/em&gt; loss ($l(z) = \max(0,1-z)$) are examples.&lt;/p&gt;

&lt;p&gt;Notice that the relaxed problem resembles the standard setup in statistical inference. There is a training loss function $L(w) = \sum_{i=1}^n l(y_i \lang w, x_i \rang)$ and a &lt;em&gt;regularizer&lt;/em&gt; $R(w) = \lVert w \rVert^2$, and we seek a model that tries to minimize both. The $\ell_2$-regularizer used above (in deep learning jargon) is sometimes called “weight decay” but others (such as the $\ell_1$-regularizer) are also popular.&lt;/p&gt;

&lt;p&gt;An alternative way to arrive at this standard setup is by invoking &lt;a href=&quot;https://en.wikipedia.org/wiki/Occam%27s_razor&quot;&gt;Occam’s Razor&lt;/a&gt;: among the many explanations to the data, we want the one that is of “minimum complexity”. This also motivates a formulation for linear regression which is the same except we use the least-squares loss. Setting $X \in \R^{n \times d}$ as the data matrix, we get:&lt;/p&gt;

\[\min_w \frac{1}{2} \lVert w \rVert^2 + \mu \lVert y - Xw \rVert^2\]

&lt;p&gt;which is sometimes called &lt;em&gt;Tikhonov regularization&lt;/em&gt;. Different choices of $\mu$ lead to different solutions: a large $\mu$ reduces the data fit error, while a small $\mu$ encourages lower-norm solutions.&lt;/p&gt;

&lt;p&gt;[There is a long history here which we won’t recount. See here&lt;sup id=&quot;fnref:esl&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:esl&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; for a nice introduction to the area, and here&lt;sup id=&quot;fnref:schapire&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:schapire&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; which establishes formal generalization bounds in terms of the margin.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;However&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In deep learning, anecdotally we seem to be doing just fine even without any regularizers. While regularization may help in some cases, several careful experiments&lt;sup id=&quot;fnref:zhang&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:zhang&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; show that regularization is neither necessary nor sufficient for good generalization. Why is that the case?&lt;/p&gt;

&lt;p&gt;To resolve this, a large body of recent work (which we introduce below) studies what I will call the &lt;em&gt;Algorithmic Bias Hypothesis&lt;/em&gt;. This hypothesis asserts that the &lt;em&gt;dynamics&lt;/em&gt; of the training procedure (specifically, gradient descent) &lt;em&gt;implicitly&lt;/em&gt; introduces a regularization effect. That is, the algorithm itself is biased towards the choice of solution towards minimum complexity solutions, and therefore the explicit imposition of a regularizer is unnecessary.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GDvsSGD&quot;&gt;Remark&lt;/strong&gt;
  The results we will prove below hold for both GD as well as SGD, so there is nothing inherently special about the &lt;em&gt;stochasticity&lt;/em&gt; of the training procedure. The literature is split on this issue, and I am not aware of compelling theoretical benefits of SGD beyond a per-iteration computational speedup. Indeed, even empirical evidence seems to suggest that stochastic training does not provide any particular benefits from the generalization perspective&lt;sup id=&quot;fnref:gieping&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:gieping&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;implicit-bias-of-gdsgd-for-linear-models&quot;&gt;Implicit bias of GD/SGD for linear models&lt;/h2&gt;

&lt;p&gt;Let’s stick with linear models for a bit longer, since the math will be instructive. We start with the setting of linear regression. We are given a dataset $X = \lbrace (x_i,y_i)_{i=1}^n \subset \R^d \times \R \rbrace$. We assume that the model is &lt;em&gt;over-parameterized&lt;/em&gt;, so the number of parameters is greater than the number of examples $(n &amp;lt; d)$. We would like to fit a linear model $u = \langle w, x \rangle$ that &lt;em&gt;exactly&lt;/em&gt; interpolates the training data. If we focus on the squared error as the loss function of choice, we seek a (global) minimizer of the loss function:&lt;/p&gt;

\[L(w) = \frac{1}{2}\lVert y - Xw \rVert^2 .\]

&lt;p&gt;But note that due to over-parameterization, the data matrix is short and wide, and there are many, many candidate models that achieve zero-loss (indeed, an infinity of them, assuming that the data is in general position.) This is where classical statistics would invoke Occam’s Razor, or margin theory, or such and start introducing regularizers.&lt;/p&gt;

&lt;p&gt;Let us blissfully ignore such suggestions, and instead, starting from $w_0 = 0$, &lt;em&gt;run gradient descent on the un-regularized loss&lt;/em&gt; $L(w)$. (To be precise, we will run gradient flow, i.e., with infinitesimally small step size – but similar conclusions can be derived for finite but small step sizes, or for stochastic minibatches, with extra algebra.) Therefore, at any time $t$, we have the gradient flow (GF) ordinary differential equation:&lt;/p&gt;

\[\frac{dw(t)}{dt} = - \nabla L(w(t)) .\]

&lt;p&gt;We have the following:&lt;/p&gt;

&lt;p class=&quot;lemma&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GFL2&quot;&gt;Lemma&lt;/strong&gt;
  Gradient flow over the squared error loss converges to the minimum $\ell_2$-norm interpolator:
  \(\begin{aligned}
  w^* = \arg \min_w \lVert w \rVert^2,~\text{s.t.}~y = Xw.
  \end{aligned}\)
  In other words, GF provides an implicit bias towards an $\ell_2$-regularized solution.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;GFL2Proof&quot;&gt;Proof&lt;/strong&gt;
  This fact is probably folklore, but a proof can be found in Zhang et al.&lt;sup id=&quot;fnref:zhang:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:zhang&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. We only need one key fact: because we are using the squared error loss, at any time $t$ the gradient looks like:&lt;/p&gt;

\[-\nabla L(w(t)) = X^T (y - Xw(t)) = \sum_{i=1}^n x_i e_i(t)\]

&lt;p&gt;which is always a vector in the &lt;em&gt;span&lt;/em&gt; of the data points. This means that if $w(0) = 0$, then the weights at any time $t$ continues to remain in this span. (Geometrically, the span of the data forms an $n$-dimensional subspace in the $d$-dimensional weight space, so the invariance being preserved here is that the path of gradient descent always lies within this subspace.)&lt;/p&gt;

&lt;p&gt;Now, &lt;em&gt;assume&lt;/em&gt; that GF converges to a global minimizer, $\bar{w}$. We need to be a bit careful and actually prove that global convergence happens. But we already know that GF/GD minimizes convex loss functions, so let us not repeat the calculations from Chapter 4. Therefore, $\bar{w} = X^T \alpha$ for some set of coefficients $\alpha$. But we also know that since $\bar{w}$ is a global minimizer, $X\bar{w} = y$, which means that&lt;/p&gt;

\[XX^T \alpha = y, \quad \text{or} \quad \alpha = (XX^T)^{-1} y.\]

&lt;p&gt;where the inverse exists since the data is in general position. Therefore,&lt;/p&gt;

&lt;p class=&quot;proof&quot;&gt;\(\bar{w} =  X^T (XX^T)^{-1} y\)
  which is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt;, or pseudo-inverse, of $X$ applied to $y$. From basic linear algebra we know that the pseudo-inverse gives the minimum-norm solution to the linear system $y = Xw$.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;flowlines&quot;&gt;Remark&lt;/strong&gt;
  The above proof is algebraic, but there is a simple geometric intuition. Gradient flowlines are &lt;a href=&quot;https://dsgissin.github.io/blog/assets/images/implicit_regularization/line_identity_param_grad_bw.png&quot;&gt;orthogonal&lt;/a&gt; to the subspace of all possible solutions. Therefore, if we start gradient flow from the origin, then the shortest path to the set of feasible solutions is the straight line segment orthogonal to this subspace, which means that the final answer is the interpolator with minimum $L_2$ norm.&lt;/p&gt;

&lt;p&gt;Great! For the squared error loss, the path of gradient descent always leads to the lowest-norm solution, regardless of the training dataset.&lt;/p&gt;

&lt;p&gt;Does this intuition carry over to more complex settings? The answer is &lt;em&gt;yes&lt;/em&gt;. Let us now describe a similar result (proved here&lt;sup id=&quot;fnref:soudry&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:soudry&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; and here&lt;sup id=&quot;fnref:jitelg&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:jitelg&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;) for the setting of linear classification. Given a linearly separable training dataset, we discover an interpolating classifier using gradient descent over the $\exp$ loss. (This result also holds for the logistic loss.) Somewhat surprisingly, we show that the limit of gradient descent (approximately) produces the min-norm interpolator, which is nothing but the (hard margin) SVM!&lt;/p&gt;

&lt;p&gt;The exact theorem statement needs some motivation. For that, let’s first examine the $\exp$ loss, which is somewhat of a strange beast. As defined above, this looks like:&lt;/p&gt;

\[L(w) = \sum_{i=1}^n \exp(- y_i \lang w, x_i \rang) .\]

&lt;p&gt;A bit of reflection will suggest that the infimum of $L$ is indeed 0, but ‘zero-loss’ is attained only when the norm of $w$ diverges to infinity. (&lt;strong&gt;Exercise&lt;/strong&gt;: Prove this.) Therefore, we will need to define ‘convergence’ a bit more carefully. The right way to think about it is to look at convergence &lt;em&gt;in direction&lt;/em&gt;; for linear classification, the norm of $w$ does not really matter — so we can examine the properties of $\lim w(t)/\lVert w(t) \rVert$ as $t \rightarrow \infty$. We obtain the following:&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;GFexp&quot;&gt;Theorem&lt;/strong&gt;
  Let $w^*$ be the hard-margin SVM solution, or the minimum $\ell_2$-norm interpolator:&lt;/p&gt;

\[\begin{aligned}
  w^* = \arg \min_w \lVert w \rVert^2,~\text{s.t.}~y_i \lang w, x_i \rang \geq 1~\forall i \in [n].
  \end{aligned}\]

&lt;p&gt;Then, gradient flow over the $\exp$ loss converges to:&lt;/p&gt;

\[w(t) \rightarrow w^* \log t + O(\log \log t).\]

&lt;p&gt;Moreover,&lt;/p&gt;

\[\frac{w(t)}{\lVert w(t) \rVert} \rightarrow  \frac{w^*}{\lVert w^* \rVert} .\]

&lt;p class=&quot;theorem&quot;&gt;In other words, GF directionally converges towards the $\ell_2$-max margin solution.&lt;/p&gt;

&lt;p class=&quot;proof&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GFL2Proof&quot;&gt;Proof sketch&lt;/strong&gt;
  (Complete.)&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;convergence&quot;&gt;Remark&lt;/strong&gt;
  Convergence happens, but at rate $\frac{1}{\log (t)}$, which is way slower than all the rates we showed in Chapter 4. (Prove this?)&lt;/p&gt;

&lt;p&gt;Therefore, we have seen that in linear models, gradient flow induces implicit $\ell_2$-bias for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the squared error loss, and&lt;/li&gt;
  &lt;li&gt;the $\exp$ loss (as well as the logistic loss&lt;sup id=&quot;fnref:soudry:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:soudry&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What about other choices of loss? Ji et al.&lt;sup id=&quot;fnref:ji2020&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ji2020&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; showed a similar directional convergence result for general &lt;em&gt;exponentially tailed&lt;/em&gt; loss functions, while clarifying that the limit direction may differ across different losses.&lt;/p&gt;

&lt;p&gt;More pertinently, what about other choices of &lt;em&gt;architecture&lt;/em&gt; (beyond linear models)? We examine this next.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;implicit-bias-in-multilinear-models&quot;&gt;Implicit bias in multilinear models&lt;/h2&gt;

&lt;p&gt;The picture becomes much more murky (and, frankly, really fascinating) when we move beyond linear models. As we will show below the architecture plays a fundamental role in the gradient dynamics, which further goes to show that &lt;em&gt;both&lt;/em&gt; representation and optimization method play a crucial role in inducing algorithmic bias.&lt;/p&gt;

&lt;p&gt;A linear model of the form $u = \lang w, x \rang$ can be viewed as a &lt;em&gt;single&lt;/em&gt; neuron with linear activation. Let us persist with linear activations for some more time, but graduate to &lt;em&gt;multiple layers of neurons&lt;/em&gt;. One such model is called a &lt;em&gt;two-layer diagonal linear network model&lt;/em&gt;:&lt;/p&gt;

\[u = \sum_{j=1}^d v_j u_j x_j ,\]

&lt;p&gt;where the weights $(u_j, v_j)$ are trainable. This model, of course, is merely a re-parameterization of a single neuron and can only express linear functions. However, the weights interact multiplicatively, and therefore the output is a &lt;em&gt;nonlinear function of the weights&lt;/em&gt;. This re-parameterization makes a world of difference in the context of gradient dynamics, and leads to a &lt;em&gt;very&lt;/em&gt; different form of algorithmic bias, as we will show below.&lt;/p&gt;

&lt;h3 class=&quot;label&quot; id=&quot;diagonal-linear-models&quot;&gt;Diagonal linear models&lt;/h3&gt;

&lt;p&gt;For simplicity, let’s just assume that the &lt;em&gt;weights are tied&lt;/em&gt; across layers, i.e., $u_i = v_i$ for $i \in [d]$. I don’t believe that this assumption is important; similar conclusions should hold even if no weight tying occurs.&lt;/p&gt;

&lt;p&gt;Then, the prediction for any data point $x \in \R^d$ becomes&lt;/p&gt;

\[y = \sum_{j=1}^d x_j u_j^2\]

&lt;p&gt;or if we stack up the training data then&lt;/p&gt;

\[y = X (u \circ u)\]

&lt;p&gt;where $\circ$ denotes the element-wise (Hadamard) product. This is called a “two-layer diagonal linear network”, and the extension to $L$-layers is analogous. Notice that by using this re-parameterization, we have not done very much in terms of expressiveness.&lt;/p&gt;

&lt;p&gt;(&lt;em&gt;Actually, the last statement is not very precise; square reparameterization only expresses linear models $y = Xw$, but with the added restriction that $w \geq 0$. But this can be fixed easily as follows: introduce 2 sets of variables $u$ and $v$, and write $y = X(u\circ u - v\circ v)$.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Let’s stick with the squared error loss:&lt;/p&gt;

\[L(u) = \frac{1}{2} \lVert y - X(u \circ u) \rVert^2\]

&lt;p&gt;and suppose we perform gradient flow over this loss. Since $L$ is a fourth-order polynomial (quartic) function of $u$, the ODE governing the gradient at any time instant is a bit more complicated:&lt;/p&gt;

\[\frac{du}{dt} = - \nabla_u L(u) = 2 u \circ X^T (y - X (u \circ u)) .\]

&lt;p&gt;Notice now that the gradient in fact is now a &lt;em&gt;cubic&lt;/em&gt; function of $u$. Suppose that we initialize $u = \alpha \mathbf{1}$, where $\mathbf{1}$ is the all-ones vector and $\alpha &amp;gt; 0$ is a scalar. (This is fine since in the end we will only be interested in models with positive coefficients.)&lt;/p&gt;

&lt;p&gt;Somewhat curiously, we can prove that if $\alpha$ is small enough then the algorithmic bias corresponding to gradient flow corresponds to $\ell_1$-regularization (i.e., we recover the familiar &lt;em&gt;basis pursuit&lt;/em&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;Lasso&lt;/a&gt; formulation. Formally, we get the following:&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;GFLasso&quot;&gt;Theorem&lt;/strong&gt;
  Let $w^*$ be the (non-negative) interpolator with minimum $\ell_1$-norm:&lt;/p&gt;

\[\begin{aligned}
  w^* = \arg \min_w \lVert w \rVert_1,~\text{s.t.}~y = Xw,~w \geq 0.
  \end{aligned}\]

&lt;p class=&quot;theorem&quot;&gt;Suppose that gradient flow for diagonal linear networks converges to an estimate $\bar{u}$. Then $\bar{w} = \bar{u} \circ \bar{u}$ converges to $w^*$ as $\alpha \rightarrow 0$.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;GFLassoProof&quot;&gt;Proof sketch&lt;/strong&gt;
  This proof is by Woodworth et al.&lt;sup id=&quot;fnref:woodworth&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:woodworth&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; Let us give high level intuition first. It will be helpful to understand two types of dynamics: one in the original (canonical) linear parameterization in terms of $w$, and the other in the square re-parameterization (in terms of $u$). By definition, we have $w_j = u_j^2$, and therefore&lt;/p&gt;

\[\frac{dw_j}{dt} = 2u_j \frac{du_j}{dt} .\]

&lt;p&gt;By plugging in the definition of gradient flow, we get:&lt;/p&gt;

\[\begin{aligned}
  \frac{dw}{dt} &amp;amp;= - 2 u(t) \circ \nabla_u L(u(t)) \\
  &amp;amp; =2 u(t) \circ 2 u(t) \circ X^T(y - X(u(t) \circ u(t))) \\
  &amp;amp; = 4 w(t) \circ X^T(y - Xw(t)).
  \end{aligned}\]

&lt;p&gt;This resembles gradient flow over the standard $\ell_2$-loss, &lt;em&gt;except&lt;/em&gt; that each coordinate is multiplied with $w_i(t)$. Intuitively, this induces a “rich-get-richer” phenomenon: a bigger coefficient in $w(t)$ is assigned a larger gradient update (relative to the other coefficients) and therefore “moves” towards its final destination faster.&lt;/p&gt;

&lt;p&gt;Gissin et al.&lt;sup id=&quot;fnref:gissin&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:gissin&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; call this &lt;em&gt;incremental learning&lt;/em&gt;: large coefficients are learned (approximately) in decreasing order of their magnitude. (See their paper/website for some terrific visualizations of the dynamics.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To rigorously argue that the bias indeed is the $\ell_1$-regularizer, let $e(t) = y - X w(t)$ denote the prediction error at any time instant. Then, as shown above, we have the ODE:&lt;/p&gt;

\[\frac{du}{dt} = 2 u(t) \circ X^T e(t) .\]

&lt;p&gt;Solving this equation coordinate-wise, we get the closed form solution:&lt;/p&gt;

\[u(t) = u(0) \circ \exp\left( 2 X^T \int_0^t e(s) ds \right) .\]

&lt;p&gt;Plugging in the initialization $u(0) = \alpha \mathbf{1}$ and $w(t) = u(t) \circ u(t)$, we get:&lt;/p&gt;

\[w(t) = \alpha^2 \mathbf{1} \circ \exp\left( 4 X^T \int_0^t e(s) ds \right).\]

&lt;p&gt;Therefore, &lt;em&gt;assuming&lt;/em&gt; the dynamics converges (technically we need to prove this, but let’s sweep that under the rug for now), we get that the limit of gradient flow gives:&lt;/p&gt;

\[\bar{w} = f(X^T \lambda)\]

&lt;p&gt;where $f$ is a coordinate-wise function such that $f(z):= \alpha^2 \exp(z)$ and $\lambda := 4 \int_0^\infty e(s) ds$. Since $\alpha &amp;gt; 0$, $f$ is bijective and therefore&lt;/p&gt;

\[X^T \lambda = f^{-1}(\bar{w}) = \log \frac{\bar{w}}{\alpha^2} .\]

&lt;p&gt;Now, &lt;em&gt;assume&lt;/em&gt; that the algorithmic bias of GF can be expressed by some unique regularizer $Q(w)$. (Again, we are making a major assumption here that the bias is expressible in the form of a well-defined function of $w$, but let’s do more rug+sweeping here.) We will show that $Q(w)$ approximately equals the $\ell_1$-norm of $w$. We get:&lt;/p&gt;

\[\bar{w} = \arg \min_w Q(w), \quad y = Xw.\]

&lt;p&gt;KKT optimality tells us that the optimum ($\bar{w}$) should satisfy:&lt;/p&gt;

\[y = X\bar{w}, \quad \nabla_w Q(\bar{w}) = X^T \lambda.\]

&lt;p&gt;for some vector $\lambda$. Therefore, we can reverse-engineer $Q$ by solving the differential equation:&lt;/p&gt;

\[\nabla_w Q(w) = \frac{1}{\alpha^2} \log w .\]

&lt;p&gt;This (again) has the closed form solution:&lt;/p&gt;

\[\begin{aligned}
  Q(w) &amp;amp;= \sum_{i=1}^d \int_{0}^{w_i} \log \frac{w_i}{\alpha^2} dw_i \\
  &amp;amp;= \sum_{i=1}^d w_i \log \frac{w_i}{\alpha^2} - w_i \\
  &amp;amp;= \sum_{i=1}^d 2w_i \log \frac{1}{\alpha} +  \sum_{i=1}^d (w_i \log w_i - w_i) .
  \end{aligned}\]

&lt;p&gt;Therefore, as $\alpha \rightarrow 0$, the first term starts to (significantly) dominate the second, and we can therefore write:&lt;/p&gt;

\[Q(w) \approx C_\alpha \sum_{i=1}^d w_i \propto \lVert w \rVert_1 ,\]

&lt;p class=&quot;proof&quot;&gt;due to the fact that $w$ is non-negative by definition.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;kernelvsrich&quot;&gt;Remark&lt;/strong&gt;
  The above proof shows (again) that the algorithmic bias of gradient descent &lt;em&gt;highly&lt;/em&gt; depends on the initialization; the fact that $\alpha \rightarrow 0$ was crucial in establishing $\ell_1$-bias. On the other hand, a &lt;em&gt;different&lt;/em&gt; initialization with $\alpha \rightarrow \infty$ leads to a completely different bias: GF automatically provides $\ell_2$-regularization.&lt;sup id=&quot;fnref:woodworth:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:woodworth&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; Woodworth et al. categorize these two distinct behaviors of gradient descent as respectively operating in the “rich” and “kernel” regimes.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;shape&quot;&gt;Remark&lt;/strong&gt;
  The fact that $u(0)$ was initialized with a constant vector (with magnitude $\alpha$) is unimportant; one can impose different forms of bias with varying choices (or “shapes”) of the initialization&lt;sup id=&quot;fnref:woodworth2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:woodworth2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;shape&quot;&gt;Remark&lt;/strong&gt;
  At its core, the net effect of square reparameterization is to alter the &lt;em&gt;optimization geometry&lt;/em&gt; of gradient descent. This can be connected to classical optimization theory in general Banach spaces, and indeed the dynamics of diagonal linear networks can be viewed as a form of &lt;em&gt;mirror descent&lt;/em&gt; (MD). See here&lt;sup id=&quot;fnref:optgeom&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:optgeom&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; and here&lt;sup id=&quot;fnref:mirror&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:mirror&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; for a more thorough treatment.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;noise&quot;&gt;Remark&lt;/strong&gt;
  All of the above assume that the limit of gradient flow admits an exact interpolation. In the presence of &lt;em&gt;noise&lt;/em&gt; in the labels the situation is a bit more subtle, and a more careful analysis of the ODEs is necessary; in some cases, for consistent generalization, &lt;em&gt;early stopping&lt;/em&gt; may be necessary. See our paper for details&lt;sup id=&quot;fnref:li&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:li&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;matrixfactorization&quot;&gt;Remark&lt;/strong&gt;
  (Two-layer dense nets and nuclear norm regularization) (&lt;strong&gt;complete&lt;/strong&gt;).&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;implicit-bias-of-gradient-descent-in-nonlinear-networks&quot;&gt;Implicit bias of gradient descent in nonlinear networks&lt;/h2&gt;

&lt;p&gt;As we saw above, changing the architecture fundamentally changes the gradient dynamics (and therefore the implicit regularization induced during training.) But our discussion above focused on networks with linear activations. How does the picture change with more standard networks (such as those with ReLU activations?)&lt;/p&gt;

&lt;p&gt;Not surprisingly, life becomes harder when dealing with nonlinearities. Yet, the tools above pave a way to understand algorithmic bias for such networks. Let’s start simple and work our way up.&lt;/p&gt;

&lt;h3 class=&quot;label&quot; id=&quot;single-neurons&quot;&gt;Single neurons&lt;/h3&gt;

&lt;p&gt;Consider a network consisting of a single neuron with nonlinear activation $\psi : \R \rightarrow \R$. The model becomes:&lt;/p&gt;

\[u = \psi(\langle w,x \rangle),\]

&lt;p&gt;and we will use the squared error loss to train it:&lt;/p&gt;

\[L(w) = \frac{1}{2} \lVert y - \psi(Xw) \rVert^2\]

&lt;p&gt;where $X$ is the data matrix. We train the above (single-neuron) network using gradient flow. The following proof is by Vardi and Shamir&lt;sup id=&quot;fnref:vardi3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:vardi3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong class=&quot;label&quot; id=&quot;GFNeuron&quot;&gt;Theorem&lt;/strong&gt;
  Suppose that $\psi : \R \rightarrow \R$ is a monotonic function. If gradient flow over $L(w)$ converges to zero loss, then the limit $\bar{w}$ automatically satisfies:&lt;/p&gt;

\[\begin{aligned}
  w^* = \arg \min_w \lVert w \rVert^2,~\text{s.t.}~y = \psi(Xw).
  \end{aligned}\]

&lt;p class=&quot;theorem&quot;&gt;In other words, GF provides an implicit bias towards an $\ell_2$-regularized solution.&lt;/p&gt;
&lt;p class=&quot;proof&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GFNeuronProof&quot;&gt;Proof&lt;/strong&gt;
  Reduce to linear case using monotonicity. &lt;strong&gt;(complete)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Activation functions such as leaky-ReLU, sigmoid etc do satisfy monotonicity. (For sigmoid, convergence to zero loss is a bit tricky because of non-convexity.) What about the regular ReLU? This is more difficult, and there does not appear to be a clean “norm”-based regularizer.&lt;/p&gt;

&lt;h3 class=&quot;label&quot; id=&quot;multiple-layers&quot;&gt;Multiple layers&lt;/h3&gt;

&lt;p&gt;Two-layer nets. &lt;strong&gt;complete&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:esl&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;T. Hastie, J. Friedman, R. Tibshirani, &lt;a href=&quot;https://hastie.su.domains/Papers/ESLII.pdf&quot;&gt;Elements of Statistical Learning&lt;/a&gt;. &lt;a href=&quot;#fnref:esl&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:schapire&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;R. Schapire, Y. Freund, P. Bartlett, W. Sun Lee, &lt;a href=&quot;https://faculty.cc.gatech.edu/~isbell/tutorials/boostingmargins.pdf&quot;&gt;Boosting the margin: A new explanation for the effectiveness of voting methods&lt;/a&gt;, 1997. &lt;a href=&quot;#fnref:schapire&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zhang&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;Understanding deep learning requires rethinking generalization&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:zhang&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:zhang:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gieping&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;J. Geiping, M. Goldblum, P. Pope, M. Moeller, T. Goldstein, &lt;a href=&quot;https://arxiv.org/abs/2109.14119&quot;&gt;Stochastic Training is Not Necessary for Generalization&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:gieping&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:soudry&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;D. Soudry, E. Hoffer, M. Spiegel Nacson, S. Gunasekar, N. Srebro, &lt;a href=&quot;https://arxiv.org/pdf/1710.10345.pdf&quot;&gt;The Implicit Bias of Gradient Descent on Separable Data&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:soudry&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:soudry:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:jitelg&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Z. Ji, M. Telgarsky, &lt;a href=&quot;https://arxiv.org/pdf/2006.06657.pdf&quot;&gt;Directional convergence and alignment in deep learning&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:jitelg&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ji2020&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Z. Ji, M. Dudik, R. Schapire, M. Telgarsky, &lt;a href=&quot;http://proceedings.mlr.press/v125/ji20a/ji20a.pdf&quot;&gt;Gradient descent follows the regularization path for general losses&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:ji2020&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:woodworth&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;B. Woodworth, S. Gunasekar, J. Lee, E. Moroshko, P. Savarse, I. Golan, D. Soudry, N. Srebro, &lt;a href=&quot;http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf&quot;&gt;Kernel and Rich Regimes in Overparametrized Models&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:woodworth&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:woodworth:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gissin&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;D. Gissin, S. Shalev-Shwartz, A. Daniely, &lt;a href=&quot;https://openreview.net/pdf?id=H1lj0nNFwB&quot;&gt;The Implicit Bias of Depth: How Incremental Learning Drives Generalization&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:gissin&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:woodworth2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Azulay, E. Moroshko, M. Naczon, N. Srebro, B. Woodworth, A. Globerson, D. Soudry, &lt;a href=&quot;http://proceedings.mlr.press/v139/azulay21a/azulay21a.pdf&quot;&gt;On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:woodworth2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:optgeom&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Gunasekar, J. Lee, D. Soudry, N. Srebro, &lt;a href=&quot;http://proceedings.mlr.press/v80/gunasekar18a/gunasekar18a.pdf&quot;&gt;Characterizing Implicit Bias in Terms of Optimization Geometry&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:optgeom&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:mirror&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Gunasekar, B. Woodworth, N. Srebro, &lt;a href=&quot;http://proceedings.mlr.press/v130/gunasekar21a/gunasekar21a.pdf&quot;&gt;Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:mirror&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:li&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;J. Li, T. Nguyen, C. Hegde, R. Wong, &lt;a href=&quot;https://arxiv.org/pdf/2108.05574.pdf&quot;&gt;Implicit Sparse Regularization: The Impact of Depth and Early Stopping&lt;/a&gt;, 2021. &lt;a href=&quot;#fnref:li&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vardi3&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;G. Vardi and O. Shamir, &lt;a href=&quot;https://arxiv.org/pdf/2012.05156.pdf&quot;&gt;Implicit Regularization in ReLU Networks with the Square Loss&lt;/a&gt;, 2020. &lt;a href=&quot;#fnref:vardi3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 8 - PAC learning primer and error bounds</title>
    <link href="http://localhost:4000/generalization02/"/>
    <updated>2022-01-11T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/generalization02</id>
    <content type="html">&lt;p&gt;OK, so implicit regularization (via the choice of training algorithm) may be part of the reason for generalization, but almost surely not the entire picture.&lt;/p&gt;

&lt;p&gt;It may be useful to revisit classical ML here. Very broadly, the conventional thinking has been:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Parsimony enables generalization.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The challenge is to precisely define what “parsimony” means here, since there are a myriad different ways of doing this. The bulk of work in generalization theory explores more and more refined complexity measures of ML models, but as we will see below, most existing classical approaches lead to &lt;em&gt;vacuous&lt;/em&gt; bounds for deep networks. Getting non-vacuous generalization guarantees is a major challenge.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;warmup-finite-hypothesis-classes&quot;&gt;Warmup: Finite hypothesis classes&lt;/h2&gt;

&lt;p&gt;Below, $R(f)$ is the population risk, $\hat{R}(f)$ is the empirical risk, and we would like $R(f) - \hat{R}(f)$ to be (a) small, and (b) decreasing as a function of sample size.
&lt;strong&gt;Complete.&lt;/strong&gt;&lt;/p&gt;

&lt;script&gt;
macros[&quot;\\f&quot;] = &quot;\\mathscr{F}&quot;
&lt;/script&gt;

&lt;p class=&quot;theorem&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;HoeffdingBound&quot;&gt;Theorem&lt;/strong&gt;
  Consider the setting of binary classification, and let $\f$ be a hypothesis class with finitely many elements. If $n \geq O\left( \frac{1}{\varepsilon^2} \log \frac{|\f|}{\delta}\right)$ then with probability at least $1-\delta$, we have that for every $f \in \f$, $|R(f) - \hat{R}(f)| \leq \varepsilon$.&lt;/p&gt;
&lt;p class=&quot;proof&quot;&gt;&lt;strong&gt;Proof&lt;/strong&gt;
  Hoeffding + union bound.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GenRemark1&quot;&gt;Remark&lt;/strong&gt;
  Observe that this holds for all $f \in \f$, not just the optimal predictor. Such a bound is called a &lt;em&gt;uniform convergence&lt;/em&gt; result.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GenRemark2&quot;&gt;Remark&lt;/strong&gt;
  This works irrespective of the choice of $\f$ or the data distribution, which could be a weakness of the technique. Even in incompatible cases (for example, the data is highly highly nonlinear, but we only use linear classifiers), one can use the above result that the generalization error is small. The lesson is that we need to control both train and generalization error.&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GenRemark1&quot;&gt;Remark&lt;/strong&gt;
  Rearranging terms, we get that the generalization error $\lesssim \frac{1}{\sqrt{n}}$, which is very typical. Classical theory tells us that this is a natural stopping point for optimization – this is the “noise floor” so we don’t have to optimize the train error below this level. (&lt;em&gt;Note: unfortunately, deep learning doesn’t obey this, and optimizing to zero is indeed beneficial; this is the “double descent” phenomenon.&lt;/em&gt;)&lt;/p&gt;

&lt;p class=&quot;remark&quot;&gt;&lt;strong class=&quot;label&quot; id=&quot;GenRemark1&quot;&gt;Remark&lt;/strong&gt;
  The scaling of the sample complexity looks like $n \approx \log |\f|$, which basically is the number of “bits” needed to encode any particular hypothesis.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;complexity-measures&quot;&gt;Complexity measures&lt;/h2&gt;

&lt;p&gt;Long list of ways to extend the above reasoning. The major drawback is that the above bound is meaningful only for hypothesis classes $\f$ with finite cardinality.  Alternatives:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Covering number&lt;/li&gt;
  &lt;li&gt;VC-dimension&lt;/li&gt;
  &lt;li&gt;Pseudo-dimension&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;agnostic-pac-learning&quot;&gt;Agnostic (PAC) learning&lt;/h3&gt;

&lt;h3 id=&quot;data-dependent-bounds&quot;&gt;Data-dependent bounds&lt;/h3&gt;

&lt;p&gt;Definition of Rademacher complexity, upper bounds for NN.&lt;/p&gt;

&lt;h3 id=&quot;pac-bayes-bounds&quot;&gt;PAC-Bayes bounds&lt;/h3&gt;

&lt;p&gt;Possibly first approach to produce “non-vacuous” bounds, at least for small networks. Key results: basic approach&lt;sup id=&quot;fnref:mcallester&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:mcallester&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, application to DL&lt;sup id=&quot;fnref:dzuigateroy18&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:dzuigateroy18&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;error-bounds-for-deep-neural-networks&quot;&gt;Error bounds for (deep) neural networks&lt;/h2&gt;

&lt;p&gt;Key results: here&lt;sup id=&quot;fnref:bartlett97&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bartlett97&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, here&lt;sup id=&quot;fnref:bartlett98&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bartlett98&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, here&lt;sup id=&quot;fnref:bartlett19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bartlett19&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;possible-roadblocks&quot;&gt;Possible roadblocks?&lt;/h2&gt;

&lt;p&gt;All of the above bounds lead to generalization bounds of the form:&lt;/p&gt;

\[\text{Test error} \leq \text{train error}~+~O \left(\frac{\text{complexity measure}}{\sqrt{n}}\right),\]

&lt;p&gt;and progress has been focused on defining better and better complexity measures. However, two issues with this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Usually, the complexity measure in the numerator is far too large anyway, leading to “vacuous” bounds. For example, in &lt;sup id=&quot;fnref:bartlett19:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bartlett19&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; it reduces to $\text{depth} \times \text{width}$, which is too large in the overparameterized setting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This also (seemingly) means that error bounds should decrease with dataset size, for a fixed model class. Not the case :( .&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A recent result provides evidence&lt;sup id=&quot;fnref:nagarajan19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:nagarajan19&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; to show that &lt;em&gt;any&lt;/em&gt; result that uses uniform convergence may suffer from this kind of looseness. We likely need alternate techniques, which we will do next.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:mcallester&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;D. Mcallester, &lt;a href=&quot;https://link.springer.com/article/10.1023/A:1007618624809&quot;&gt;Some PAC-Bayesian Theorems&lt;/a&gt;, 1999. &lt;a href=&quot;#fnref:mcallester&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dzuigateroy18&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;G. K. Dziugaite, D. Roy, &lt;a href=&quot;https://arxiv.org/pdf/1703.11008.pdf&quot;&gt;Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data&lt;/a&gt;, 2017. &lt;a href=&quot;#fnref:dzuigateroy18&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bartlett97&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;P. Bartlett, &lt;a href=&quot;https://proceedings.neurips.cc/paper/1996/file/fb2fcd534b0ff3bbed73cc51df620323-Paper.pdf&quot;&gt;For Valid Generalization the Size of the Weights is More Important than the Size of the Network&lt;/a&gt;, 1997. &lt;a href=&quot;#fnref:bartlett97&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bartlett98&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;P. Bartlett, V. Maiorov, R. Meir, &lt;a href=&quot;https://proceedings.neurips.cc/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf&quot;&gt;Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks&lt;/a&gt;, 1998. &lt;a href=&quot;#fnref:bartlett98&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bartlett19&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;P. Bartlett, N. Harvey, C. Liaw, A. Mehrabian,  &lt;a href=&quot;https://www.jmlr.org/papers/volume20/17-612/17-612.pdf&quot;&gt;Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:bartlett19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt; &lt;a href=&quot;#fnref:bartlett19:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:nagarajan19&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;V. Nagarajan, Z. Kolter, &lt;a href=&quot;https://arxiv.org/pdf/1902.04742.pdf&quot;&gt;Uniform convergence may be unable to explain generalization in deep learning&lt;/a&gt;, 2019. &lt;a href=&quot;#fnref:nagarajan19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
    <title>Chapter 9 - Generalization bounds via stability</title>
    <link href="http://localhost:4000/generalization03/"/>
    <updated>2022-01-10T00:00:00-05:00</updated>
    <id>http://localhost:4000/fodl/generalization03</id>
    <content type="html">&lt;p&gt;In the last two lectures, we have investigated generalization through the lens of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;margin-based regularization&lt;/li&gt;
  &lt;li&gt;model parsimony&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each had their shortfalls, especially in the context of deep learning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regularization very much depends on model architecture and/or training procedure. Implicit bias of GD shows up mysteriously in several places.&lt;/li&gt;
  &lt;li&gt;Model parsimony is hard to measure; interacts with the data distribution in hard-to-quantify ways; and anyway in the last chapter we saw that uniform convergence seems to be an insufficient tool for getting non-trivial bounds, especially in the over-parameterized setting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us now briefly a &lt;em&gt;third&lt;/em&gt; lens to understand generalization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Algorithmic stability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our central hypothesis in this chapter is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stable learning implies generalization.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The high level idea is that if the model produced by a training algorithm is not sensitive to any single training data point, then the model generalizes. This idea has been around for some time (and in fact, methods such as bagging&lt;sup id=&quot;fnref:breiman&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:breiman&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; explicitly were developed in order to make classifiers stable to individual points.)&lt;/p&gt;

&lt;p&gt;(Notice that the definition of stability involves specifying the training procedure; once again, the role of the training algorithm is more clearly illuminated here.)&lt;/p&gt;

&lt;p&gt;The plus side is that we &lt;em&gt;won’t&lt;/em&gt; need to appeal to uniform convergence, so over-parameterization is not a concern. The minus side is that &lt;em&gt;convexity&lt;/em&gt; of the loss seems to be the key tool here, which makes application to deep learning difficult.&lt;/p&gt;

&lt;p&gt;On the other hand, our tour of optimization (Chapter 4) has given us tools to deal with (some) non-convex losses. All these will be prove to be fruitful.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Stability as a tool for generalization dates back to an influential paper by Bousquet and Elisseef&lt;sup id=&quot;fnref:bousquet02&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bousquet02&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, who introduced the notion of &lt;em&gt;uniform stability&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Further refined in the context of empirical risk minimization by Shalev-Schwartz et al.&lt;sup id=&quot;fnref:ermstability&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ermstability&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;algorithmic-stability-of-sgd-for-empirical-risk-minimization&quot;&gt;Algorithmic stability of (S)GD for empirical risk minimization&lt;/h2&gt;

&lt;h3 id=&quot;beyond-convexity&quot;&gt;Beyond convexity&lt;/h3&gt;

&lt;p&gt;Stability under smoothness + PL-condition. Key paper: here&lt;sup id=&quot;fnref:plstability&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:plstability&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 class=&quot;label&quot; id=&quot;connections-to-differential-privacy&quot;&gt;Connections to differential privacy&lt;/h2&gt;

&lt;p&gt;Fascinating parallels between&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;generalization via uniform stability&lt;/li&gt;
  &lt;li&gt;differential privacy (DP)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;which has a different origin story, dating back to several papers by Dwork and co-authors &lt;sup id=&quot;fnref:dwork1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:dwork1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:dwork2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:dwork2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:dwork3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:dwork3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In DP the goal is to protect “data leakage” – nothing about the user’s identity (as far as possible) should be revealed from the model parameters. But the &lt;em&gt;method&lt;/em&gt; to achieve is the same as the ones we discussed above: make sure the model does not depend on any one data point too much.&lt;/p&gt;

&lt;p&gt;Upshot: DP implies uniform stability (and DP-style algorithm design gives a way to control generalization.)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Complete&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:breiman&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;L. Breiman, &lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/bagging.pdf&quot;&gt;Bagging Predictors&lt;/a&gt;, 1994. &lt;a href=&quot;#fnref:breiman&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bousquet02&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;O. Bousquet and A. Elisseef, &lt;a href=&quot;https://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf&quot;&gt;Stability and Generalization&lt;/a&gt;, 2002. &lt;a href=&quot;#fnref:bousquet02&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ermstability&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;S. Shalev-Schwartz, O. Shamir, K. Sridharan, N. Srebro, &lt;a href=&quot;https://jmlr.csail.mit.edu/papers/volume11/shalev-shwartz10a/shalev-shwartz10a.pdf&quot;&gt;Learnability, Stability and Uniform Convergence&lt;/a&gt;, 2010. &lt;a href=&quot;#fnref:ermstability&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:plstability&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Z. Charles and D. Papailiopoulos, &lt;a href=&quot;http://proceedings.mlr.press/v80/charles18a/charles18a.pdf&quot;&gt;Stability and Generalization of Learning Algorithms that Converge to Global Optima&lt;/a&gt;, 2018. &lt;a href=&quot;#fnref:plstability&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dwork1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Dwork, &lt;a href=&quot;https://link.springer.com/chapter/10.1007/11787006_1&quot;&gt;Differential Privacy&lt;/a&gt;, 2006. &lt;a href=&quot;#fnref:dwork1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dwork2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Dwork, F. McSherry, K. Nissim, A. Smith, &lt;a href=&quot;https://link.springer.com/chapter/10.1007/11681878_14&quot;&gt;Calibrating Noise to Sensitivity in Private Data Analysis&lt;/a&gt;, 2006. &lt;a href=&quot;#fnref:dwork2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dwork3&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;C. Dwork and A. Roth, &lt;a href=&quot;https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf&quot;&gt;The Algorithmic Foundations of Differential Privacy&lt;/a&gt;, 2014. &lt;a href=&quot;#fnref:dwork3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
</feed>
