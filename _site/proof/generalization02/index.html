<!DOCTYPE html>
<html lang="en-us">
  <head>
    <!-- Basic meta elements -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta http-equiv="content-type"    content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- Canonical link to help search engines -->
    <link rel="canonical"     href="http://localhost:4000/fodl/generalization02/" />
    <link rel="shortcut icon" type="image/x-icon" href="/fodl/static/favicon.ico" />
    <link rel="alternate"     type="application/rss+xml" title="RSS" href="/fodl/atom.xml" />
    <link rel="stylesheet"    href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i|Playfair+Display+SC&amp;display=swap" />
    <link rel="stylesheet"    href="/fodl/static/style.css" />

    <title>Chapter 8 - PAC learning primer and error bounds</title>

    <!-- Dublin Core metadata for Zotero -->
    <meta name="DC.title"   content="Chapter 8 - PAC learning primer and error bounds" />
    <meta name="DC.creator" content="" />
    <meta name="DC.date"    content="" />
    <meta name="DC.source"  content="Foundations of Deep Learning" />

    <!-- Open Graph metadata -->
    <meta property="og:type"           content="article" />
    <meta property="og:title"          content="Chapter 8 - PAC learning primer and error bounds" />
    <meta property="og:article:author" content="" />
    <meta property="og:description"    content="Quicklinks" />
    <meta property="og:url"            content="http://localhost:4000/fodl/generalization02/" />
    <meta property="og:image"          content="http://localhost:4000/fodl/static/open-graph-logo.png" />
    <meta property="og:image:width"    content="200" />
    <meta property="og:image:height"   content="200" />

    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
      integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous" />
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
      integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
    <!-- KaTeX auto-render extention: https://katex.org/docs/autorender.html -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js"
      integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      var macros = {
          "\\P": "\\mathbb{P}"
      }, delimiters = [
          { left: "$$",  right: "$$",  display: true}, // Technically, this does not work. See below.
          { left: "\\[", right: "\\]", display: true},
          { left: "$",   right: "$",   display: false},
          { left: "\\(", right: "\\)", display: false}
      ];

      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, { delimiters: delimiters, macros: macros });

        // Currently GitHub pages overrides math-engine: mathjax in _config, hence $$..$$ are converted to to scripts tags.
        // See: https://github.com/github/pages-gem/pull/545
        document.querySelectorAll("script[type='math/tex']").forEach(function(el) {
          el.outerHTML = katex.renderToString(el.textContent, { displayMode: false, macros: macros });
        });

        document.querySelectorAll("script[type='math/tex; mode=display']").forEach(function(el) {
          el.outerHTML = katex.renderToString(el.textContent.replace(/%.*/g, ''), { displayMode: true, macros: macros });
        });
      });
    </script>
    <style>.katex { font-size: 1.15em; }</style>
    <!-- KaTeX copy-tex extention: https://github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex -->
    <link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css" />
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.js"
      integrity="sha384-XhWAe6BtVcvEdS3FFKT7Mcft4HJjPqMQvi5V4YhzH9Qxw497jC13TupOEvjoIPy7" crossorigin="anonymous"></script>
  </head>

  <body class="theme-base-">
    
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" />

    <!-- Toggleable sidebar -->
    <div class="sidebar" id="sidebar">
      <div class="sidebar-item">
        <p>Quicklinks</p>
      </div>

      <nav class="sidebar-nav">
        <a class="sidebar-nav-item" href="/fodl/">Home</a>
        <a class="sidebar-nav-item" href="/fodl/manual/">Manual</a>
        <a class="sidebar-nav-item" href="/fodl/search/">Search</a>

        <!-- The code below is used for manually entered links -->
        <a href="https://github.com/mahrud/proof"
          class="sidebar-nav-item" target="_blank">GitHub Repository</a>
      </nav>

      <div class="sidebar-item">
        <p>Distributed under an MIT license.</p>
      </div>
    </div>

    <!--
      Wrap is the content to shift when toggling the sidebar.
      We wrap the content to avoid any CSS collisions with our real content.
    -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/fodl/">Foundations of Deep Learning</a><br />
            <small>NYU Tandon, Spring 2022</small>
          </h3>
        </div>
      </div>

      <div class="container content" id="main">
<!-- ########################################################################################### -->
<div class="page">
  <h1 class="page-title">Chapter 8 - PAC learning primer and error bounds</h1>
  <p>OK, so implicit regularization (via the choice of training algorithm) may be part of the reason for generalization, but almost surely not the entire picture.</p>

<p>It may be useful to revisit classical ML here. Very broadly, the conventional thinking has been:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parsimony enables generalization.
</code></pre></div></div>

<p>The challenge is to precisely define what “parsimony” means here, since there are a myriad different ways of doing this. The bulk of work in generalization theory explores more and more refined complexity measures of ML models, but as we will see below, most existing classical approaches lead to <em>vacuous</em> bounds for deep networks. Getting non-vacuous generalization guarantees is a major challenge.</p>

<h2 class="label" id="warmup-finite-hypothesis-classes">Warmup: Finite hypothesis classes</h2>

<p>Below, $R(f)$ is the population risk, $\hat{R}(f)$ is the empirical risk, and we would like $R(f) - \hat{R}(f)$ to be (a) small, and (b) decreasing as a function of sample size.
<strong>Complete.</strong></p>

<script>
macros["\\f"] = "\\mathscr{F}"
</script>

<p class="theorem"><strong class="label" id="HoeffdingBound">Theorem</strong>
  Consider the setting of binary classification, and let $\f$ be a hypothesis class with finitely many elements. If $n \geq O\left( \frac{1}{\varepsilon^2} \log \frac{|\f|}{\delta}\right)$ then with probability at least $1-\delta$, we have that for every $f \in \f$, $|R(f) - \hat{R}(f)| \leq \varepsilon$.</p>
<p class="proof"><strong>Proof</strong>
  Hoeffding + union bound.</p>

<p class="remark"><strong class="label" id="GenRemark1">Remark</strong>
  Observe that this holds for all $f \in \f$, not just the optimal predictor. Such a bound is called a <em>uniform convergence</em> result.</p>

<p class="remark"><strong class="label" id="GenRemark2">Remark</strong>
  This works irrespective of the choice of $\f$ or the data distribution, which could be a weakness of the technique. Even in incompatible cases (for example, the data is highly highly nonlinear, but we only use linear classifiers), one can use the above result that the generalization error is small. The lesson is that we need to control both train and generalization error.</p>

<p class="remark"><strong class="label" id="GenRemark1">Remark</strong>
  Rearranging terms, we get that the generalization error $\lesssim \frac{1}{\sqrt{n}}$, which is very typical. Classical theory tells us that this is a natural stopping point for optimization – this is the “noise floor” so we don’t have to optimize the train error below this level. (<em>Note: unfortunately, deep learning doesn’t obey this, and optimizing to zero is indeed beneficial; this is the “double descent” phenomenon.</em>)</p>

<p class="remark"><strong class="label" id="GenRemark1">Remark</strong>
  The scaling of the sample complexity looks like $n \approx \log |\f|$, which basically is the number of “bits” needed to encode any particular hypothesis.</p>

<h2 class="label" id="complexity-measures">Complexity measures</h2>

<p>Long list of ways to extend the above reasoning. The major drawback is that the above bound is meaningful only for hypothesis classes $\f$ with finite cardinality.  Alternatives:</p>

<ul>
  <li>Covering number</li>
  <li>VC-dimension</li>
  <li>Pseudo-dimension</li>
</ul>

<h3 id="agnostic-pac-learning">Agnostic (PAC) learning</h3>

<h3 id="data-dependent-bounds">Data-dependent bounds</h3>

<p>Definition of Rademacher complexity, upper bounds for NN.</p>

<h3 id="pac-bayes-bounds">PAC-Bayes bounds</h3>

<p>Possibly first approach to produce “non-vacuous” bounds, at least for small networks. Key results: basic approach<sup id="fnref:mcallester" role="doc-noteref"><a href="#fn:mcallester" class="footnote" rel="footnote">1</a></sup>, application to DL<sup id="fnref:dzuigateroy18" role="doc-noteref"><a href="#fn:dzuigateroy18" class="footnote" rel="footnote">2</a></sup>.</p>

<h2 class="label" id="error-bounds-for-deep-neural-networks">Error bounds for (deep) neural networks</h2>

<p>Key results: here<sup id="fnref:bartlett97" role="doc-noteref"><a href="#fn:bartlett97" class="footnote" rel="footnote">3</a></sup>, here<sup id="fnref:bartlett98" role="doc-noteref"><a href="#fn:bartlett98" class="footnote" rel="footnote">4</a></sup>, here<sup id="fnref:bartlett19" role="doc-noteref"><a href="#fn:bartlett19" class="footnote" rel="footnote">5</a></sup>.</p>

<h2 class="label" id="possible-roadblocks">Possible roadblocks?</h2>

<p>All of the above bounds lead to generalization bounds of the form:</p>

\[\text{Test error} \leq \text{train error}~+~O \left(\frac{\text{complexity measure}}{\sqrt{n}}\right),\]

<p>and progress has been focused on defining better and better complexity measures. However, two issues with this:</p>

<ul>
  <li>
    <p>Usually, the complexity measure in the numerator is far too large anyway, leading to “vacuous” bounds. For example, in <sup id="fnref:bartlett19:1" role="doc-noteref"><a href="#fn:bartlett19" class="footnote" rel="footnote">5</a></sup> it reduces to $\text{depth} \times \text{width}$, which is too large in the overparameterized setting.</p>
  </li>
  <li>
    <p>This also (seemingly) means that error bounds should decrease with dataset size, for a fixed model class. Not the case :( .</p>
  </li>
</ul>

<p>A recent result provides evidence<sup id="fnref:nagarajan19" role="doc-noteref"><a href="#fn:nagarajan19" class="footnote" rel="footnote">6</a></sup> to show that <em>any</em> result that uses uniform convergence may suffer from this kind of looseness. We likely need alternate techniques, which we will do next.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:mcallester" role="doc-endnote">

      <p>D. Mcallester, <a href="https://link.springer.com/article/10.1023/A:1007618624809">Some PAC-Bayesian Theorems</a>, 1999. <a href="#fnref:mcallester" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
    </li>
    <li id="fn:dzuigateroy18" role="doc-endnote">

      <p>G. K. Dziugaite, D. Roy, <a href="https://arxiv.org/pdf/1703.11008.pdf">Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data</a>, 2017. <a href="#fnref:dzuigateroy18" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
    </li>
    <li id="fn:bartlett97" role="doc-endnote">

      <p>P. Bartlett, <a href="https://proceedings.neurips.cc/paper/1996/file/fb2fcd534b0ff3bbed73cc51df620323-Paper.pdf">For Valid Generalization the Size of the Weights is More Important than the Size of the Network</a>, 1997. <a href="#fnref:bartlett97" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
    </li>
    <li id="fn:bartlett98" role="doc-endnote">

      <p>P. Bartlett, V. Maiorov, R. Meir, <a href="https://proceedings.neurips.cc/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf">Almost Linear VC Dimension Bounds for Piecewise Polynomial Networks</a>, 1998. <a href="#fnref:bartlett98" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
    </li>
    <li id="fn:bartlett19" role="doc-endnote">

      <p>P. Bartlett, N. Harvey, C. Liaw, A. Mehrabian,  <a href="https://www.jmlr.org/papers/volume20/17-612/17-612.pdf">Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks</a>, 2019. <a href="#fnref:bartlett19" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;</a> <a href="#fnref:bartlett19:1" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;<sup>2</sup></a></p>
    </li>
    <li id="fn:nagarajan19" role="doc-endnote">

      <p>V. Nagarajan, Z. Kolter, <a href="https://arxiv.org/pdf/1902.04742.pdf">Uniform convergence may be unable to explain generalization in deep learning</a>, 2019. <a href="#fnref:nagarajan19" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
    </li>
  </ol>
</div>

</div>

<!-- ########################################################################################### -->
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
    // Highlight search Query
    var url = window.location.href;
    if (url.lastIndexOf("?q=") > 0) {
      // get the index of the parameter, add three (to account for length)
      var stringloc = url.lastIndexOf("?q=") + 3;
      // get the substring (query) and decode
      var searchquery = decodeURIComponent(url.substr(stringloc));
      // regex matches at beginning of line, end of line or word boundary, useful for poetry
      var regex = new RegExp("(?:^|\\b)(" + searchquery + ")(?:$|\\b)", "gim");
      // get, add mark and then set content
      var content = document.getElementById("main").innerHTML;
      document.getElementById("main").innerHTML = content.replace(regex, "<mark>$1</mark>");
    }

    // Toggle sidebar
    (function(document) {
      var toggle = document.querySelector('.sidebar-toggle');
      var sidebar = document.querySelector('#sidebar');
      var checkbox = document.querySelector('#sidebar-checkbox');

      document.addEventListener('click', function(e) {
        var target = e.target;

        if(!checkbox.checked ||
           !sidebar.contains(target) ||
           (target === checkbox || target === toggle)) return;

        checkbox.checked = false;
      }, false);
    })(document);
    </script>

  </body>
</html>
